[
{
	"uri": "https://ipfscluster.io/news/",
	"title": "News",
	"tags": [],
	"description": "",
	"content": " News This section contains updates, release notes and blog posts. Subscribe to RSS/Atom feed or visit it regularly to get to know all the news from IPFS Cluster:\n"
},
{
	"uri": "https://ipfscluster.io/documentation/deployment/architecture/",
	"title": "Architecture overview",
	"tags": [],
	"description": "",
	"content": " Architecture overview Before jumping in to install and run IPFS Cluster, it is important to clarify some basic concepts.\nThe IPFS Cluster software consists of three binary files:\n ipfs-cluster-service runs a Cluster peer (similar to ipfs daemon) using a configuration file and by storing some information on disk. ipfs-cluster-ctl is used to communicate with a Cluster peer and perform actions such as pinning IPFS CIDs to the Cluster. ipfs-cluster-follow runs a follower Cluster peer. It can be used as a substitute for ipfs-cluster-service for this specific usecase and mixes some service and ctl features.  The Cluster peer communicates with the IPFS daemon using the HTTP API (localhost:5001). Therefore, the IPFS daemon must be launched and running separately.\nUsually, ipfs-cluster-ctl is used on the same machine or server on which ipfs-cluster-service is running. For example, ipfs-cluster-ctl pin add \u0026lt;hash\u0026gt; will instruct the local Cluster peer to submit a pin to the Cluster. The different peers in the Cluster will then proceed to ask their local IPFS daemons to pin that content. The number of pins across the Cluster will depend on the replication factor set for each pin (the default is set in the ipfs-cluster-service configuration file).\nThe Cluster swarm IPFS Cluster is a fully distributed application. ipfs-cluster-service runs a Cluster peer and all peers are equal. Cluster peers form an separate, isolated libp2p [private] network, which uses the cluster_secret (a 32-bit hex-encoded passphrase present in the configuration of every peer).\nThis network does not interact with the main IPFS network, nor with other private IPFS networks and is solely used so that Cluster peers can communicate and operate. The network uses a number of blocks also used by IPFS (DHT, PubSub, Bitswap\u0026hellip;) but, unlike IPFS, does not enjoy public bootstrappers.\nThis means that Cluster peers will normally need their own bootstrappers (it can be any peer in the Cluster), although sometimes they can rely on mDNS discovery.\nThis also means that Cluster peers operate separately from IPFS with regards to NAT hole punching, ports etc.\nThe shared state: consensus All peers in the Cluster maintain a global pinset. Making every peer maintain the same view of the pinset regardless of concurrent pinning operations and on a distributed application layout requires coordination given by what is called a consensus component. Cluster supports two implementations:\n A CRDT-based approach, based on Conflict-Free Replicated Datatypes A Raft-based approach, based on a popular log-based consensus algorithm  The relevant details and trade-offs between them are outlined in the Consensus Components section. The choice (which must be performed during initialization and cannot be easily changed), heavily affects the procedures for adding, removing and handling failures in Cluster peers. tl;dr: we recommend using the CRDT-consensus component.\nThe shared state can be inspected with ipfs-cluster-ctl pin ls and is the only piece of information present locally in every peer. Pin status (status) information, or peers information (peers ls) for other than the peer that is running the command, must be obtained at runtime from their respective peers and assembled together.\n"
},
{
	"uri": "https://ipfscluster.io/documentation/reference/configuration/",
	"title": "Configuration",
	"tags": [],
	"description": "",
	"content": " Configuration reference All IPFS Cluster configurations and persistent data can be found, by default, at the ~/.ipfs-cluster folder. For more information about the persistent data in this folder, see the Data, backups and recovery section.\nipfs-cluster-service -c \u0026lt;path\u0026gt; sets the location of the configuration folder. This is also controlled by the IPFS_CLUSTER_PATH environment variable. The ipfs-cluster-service program uses two main configuration files:\n service.json, containing the cluster peer configuration, usually identical in all cluster peers. identity.json, containing the unique identity used by each peer.  identity.json The identity.json file is auto-generated during ipfs-cluster-service init. It includes a base64-encoded private key and the public peer ID associated to it. This peer ID identifies the peer in the Cluster. You can see an example here.\nThis file is not overwritten when re-running ipfs-cluster-service -f init. If you wish to generate a new one, you will need to delete it first.\nThe identity fields can be overwritten using the CLUSTER_ID and CLUSTER_PRIVATEKEY environment values.\nManual identity generation When automating a deployment or creating configurations for several peers, it might be handy to generate peer IDs and private keys manually beforehand.\nYou can obtain a valid peer ID and its associated private key in the format expected by the configuration using ipfs-key as follows:\nipfs-key -type ed25519 | base64 -w 0  service.json The service.json file holds all the configurable options for the cluster peer and its different components. The configuration file is divided in sections. Each section represents a component. Each item inside the section represents an implementation of that component and contains specific options. A default service.json file with sensible values is created when running ipfs-cluster-service init.\nImportant: The cluster section of the configuration stores a 32 byte hex-encoded secret which secures communication among all cluster peers. The secret must be shared by all cluster peers. Using an empty secret has security implications (see the Security section). If present, the `CLUSTER_SECRET` environment value is used when running `ipfs-cluster-service init` to set the cluster `secret` value. As an example, this is a default service.json configuration file.\nThe file looks like:\n{ \u0026quot;source\u0026quot;: \u0026quot;url\u0026quot; // a single source field may appear for remote configurations \u0026quot;cluster\u0026quot;: {...}, \u0026quot;consensus\u0026quot;: { // either crdt or raft \u0026quot;crdt\u0026quot;: {...}, \u0026quot;raft\u0026quot;: {...}, }, \u0026quot;api\u0026quot;: { \u0026quot;ipfsproxy\u0026quot;: {...}, \u0026quot;restapi\u0026quot;: {...} }, \u0026quot;ipfs_connector\u0026quot;: { \u0026quot;ipfshttp\u0026quot;: {...} }, \u0026quot;pin_tracker\u0026quot;: { \u0026quot;stateless\u0026quot;: {...} }, \u0026quot;monitor\u0026quot;: { \u0026quot;pubsubmon\u0026quot;: {...} }, \u0026quot;allocator\u0026quot;: { \u0026quot;balanced\u0026quot;: {...} }, \u0026quot;informer\u0026quot;: { \u0026quot;disk\u0026quot;: {...}, \u0026quot;tags\u0026quot;: {...}, \u0026quot;pinqueue\u0026quot;: {...}, }, \u0026quot;observations\u0026quot;: { \u0026quot;metrics\u0026quot;: {...}, \u0026quot;tracing\u0026quot;: {...} }, \u0026quot;datastore\u0026quot;: { // either pebble, badger3, badger or leveldb \u0026quot;pebble\u0026quot;: {...}, \u0026quot;badger3\u0026quot;: {...}, \u0026quot;badger\u0026quot;: {...}, \u0026quot;leveldb\u0026quot;: {...}, } }  The different sections and subsections are documented in detail below.\nUsing environment variables to overwrite configuration values All the options in the configuration file can be can be overridden by setting environment variables. i.e. CLUSTER_SECRET will overwrite the secret value; CLUSTER_LEAVEONSHUTDOWN will overwrite the leave_on_shutdown value; CLUSTER_RESTAPI_CORSALLOWEDORIGINS will overwrite the restapi.cors_allowed_origins value.\nIn general the environment variable takes the form CLUSTER_\u0026lt;COMPONENTNAME\u0026gt;_KEYWITHOUTUNDERSCORES=value. Environment variables will be applied to the resultant configuration file when generating it with ipfs-cluster-service init.\nRemote configurations Since version 0.11.0, the service.json may be initialized with a single source field containing a URL that points to a standard service.json file. This configuration is read on every start of the peer.\nA remote service.json can be used to point all peers the same configuration file stored in the same location. It is also possible to use an URL pointing to an file provided through IPFS.\nThe cluster main section The main cluster section of the configuration file configures the core component.\nThe replication_factor_min and replication_factor_max control the pinning defaults when these options, which can be set on a per-pin basis, are left unset. Cluster always tries to allocate up to replication_factor_max peers to every item. However, if it is not possible to reach that number, pin operations will succeed as long as replication_factor_min can be fulfilled. Once the allocations are set, Cluster does not automatically change them (i.e. to increase them). However, a new Pin operation for the same CID will try again to fulfill replication_factor_max while respecting the already existing allocations.\nThe leave_on_shutdown option allows a peer to remove itself from the peerset when shutting down cleanly. It is most relevant when using raft. This means that, for any subsequent starts, the peer will need to be bootstrapped in order to re-join the Cluster.\n   Key Default Description     peername \u0026quot;\u0026lt;hostname\u0026gt;\u0026quot; A human name for this peer.   secret \u0026quot;\u0026lt;randomly generated\u0026gt;\u0026quot; The Cluster secret (must be the same in all peers).   leave_on_shutdown false The peer will remove itself from the cluster peerset on shutdown.   listen_multiaddress [\u0026quot;/ip4/0.0.0.0/tcp/9096\u0026quot;, \u0026quot;/ip4/0.0.0.0/udp/9096/quic\u0026quot;] The peers Cluster-RPC listening endpoints.   connection_manager {  A connection manager configuration object.   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;high_water 400 The maximum number of connections this peer will have. If it, connections will be closed until the low_water value is reached.   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;low_water 100 The libp2p host will try to keep at least this many connections to other peers.   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;grace_period \u0026quot;2m0s\u0026quot; New connections will not be dropped for at least this period.   }     dial_peer_timeout \u0026quot;3s\u0026quot; How long to wait when dialing a cluster peer before giving up.   state_sync_interval \u0026quot;10m0s\u0026quot; Interval between automatic triggers of StateSync.   pin_recover_interval \u0026quot;1h0m0s\u0026quot; Interval between automatic triggers of RecoverAllLocal. This will automatically re-try pin and unpin operations that failed.   replication_factor_min -1 Specifies the default minimum number of peers that should be pinning an item. -1 == all.   replication_factor_max -1 Specifies the default maximum number of peers that should be pinning an item. -1 == all.   monitor_ping_interval \u0026quot;15s\u0026quot; Interval for sending a ping (used to detect downtimes).   peer_watch_interval \u0026quot;5s\u0026quot; Interval for checking the current cluster peerset and detect if this peer was removed from the cluster (and shut-down).   mdns_interval \u0026quot;10s\u0026quot; Setting it to \u0026quot;0\u0026quot; disables mDNS. Setting to a larger value enables mDNS but no longer controls anything.   enable_relay_hop true Let the cluster peer acts as relay for other peers that are not reachable directly.   pin_only_on_trusted_peers false The cluster peer will only allocate pins to trusted peers (as configured)   disable_repinning true Do not automatically re-pin all items allocated to a peer that becomes unhealthy (down).   follower_mode false Peers in follower mode provide useful error messages when trying to perform actions like pinning.   peer_addresses [] Full peer multiadresses for peers to connect to on boot (similar to manually added entries to the peerstore file.   pin_only_on_trusted_peers false Limits the possible allocations given to a pin to those in the trusted_peers list    Manual secret generation You can obtain a 32-bit hex encoded random string with:\nexport CLUSTER_SECRET=$(od -vN 32 -An -tx1 /dev/urandom | tr -d ' \\n')  The consensus section The consensus contains a single configuration object for the chosen implementations of the consensus component (either crdt or raft, but not both).\ncrdt Including the CRDT section enables cluster to use a crdt-based distributed key value store for the cluster state (pinset).\nBatched commits are enabled in this section by setting batching.max_batch_size and batching.max_batch_age to a value greater than 0 (the default). These two settings control when a batch is committed, either by reaching a maximum number of pin/unpin operations, or by reaching a maximum age.\nAn additional batching.max_queue_size option provides the ability to perform backpressure on Pin/Unpin requests. When more than max_queue_size pin/unpins are waiting to be included in a batch, the operations will fail. If this happens, it is means cluster cannot commit batches as fast as pins are arriving. Thus, max_queue_size should be increase (to accommodate bursts), or the max_batch_size increased (to perform less commits and hopefully handle the requests faster).\nNote that the underlying CRDT library will auto-commit when batch deltas reach 1MB of size.\n   Key Default Description     cluster_name \u0026quot;ipfs-cluster\u0026quot; An arbitrary name. It becomes the pubsub topic to which all peers in the cluster subscribe to, so it must be the same for all.   trusted_peers [] The default set of trusted peers. See Trust in CRDT Mode for more information. Can be set to [ \u0026quot;*\u0026quot; ] to trust all peers.   batching {  Batching settings when submitting pins to the CRDT layer. Both max_batch_size and max_batch_age need to be greater than 0 for batching to be enabled.   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;max_batch_size 0 The maximum number of pin/unpin operations to include in a batch before committing it.   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;max_batch_age \u0026quot;0s\u0026quot; The maximum time an uncommitted batch waits before it is committed.   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;max_queue_size 1000 The maximum number of pin/unpin operations that are waiting to be included in a batch.   }     peerset_metric \u0026quot;ping\u0026quot; The name of the monitor metric to determine the current pinset.   rebroadcast_interval \u0026quot;1m0s\u0026quot; How often to republish the current heads when no other pubsub message has been seen. Reducing this will allow new peers to learn about the current state sooner.   repair_interval \u0026quot;1h0m0s\u0026quot; How often to check if the crdt-datastore is marked as dirty, and trigger a re-processing of the DAG in that case.    raft    Key Default Description     init_peerset [] An array of peer IDs specifying the initial peerset when no raft state exists.   wait_for_leader_timeout \u0026quot;15s\u0026quot; How long to wait for a Raft leader to be elected before throwing an error.   network_timeout \u0026quot;10s\u0026quot; How long before Raft protocol network operations timeout.   commit_retries 1 How many times to retry committing an entry to the Raft log on failure.   commit_retry_delay \u0026quot;200ms\u0026quot; How long to wait before commit retries.   backups_rotate 6 How many backup copies on the state to keep when it\u0026rsquo;s cleaned up.   heartbeat_timeout \u0026quot;1s\u0026quot; See https://godoc.org/github.com/hashicorp/raft#Config .   election_timeout \u0026quot;1s\u0026quot; See https://godoc.org/github.com/hashicorp/raft#Config .   commit_timeout \u0026quot;50ms\u0026quot; See https://godoc.org/github.com/hashicorp/raft#Config .   max_append_entries 64 See https://godoc.org/github.com/hashicorp/raft#Config .   trailing_logs 10240 See https://godoc.org/github.com/hashicorp/raft#Config .   snapshot_interval \u0026quot;2m0s\u0026quot; See https://godoc.org/github.com/hashicorp/raft#Config .   snapshot_threshold 8192 See https://godoc.org/github.com/hashicorp/raft#Config .   leader_lease_timeout \u0026quot;500ms\u0026quot; See https://godoc.org/github.com/hashicorp/raft#Config .    Raft stores and maintains the peerset internally but the cluster configuration offers the option to manually provide the peerset for the first start of a peer using the init_peerset key in the raft section of the configuration. For example:\n\u0026quot;init_peerset\u0026quot;: [ \u0026quot;QmPQD6NmQkpWPR1ioXdB3oDy8xJVYNGN9JcRVScLAqxkLk\u0026quot;, \u0026quot;QmcDV6Tfrc4WTTGQEdatXkpyFLresZZSMD8DgrEhvZTtYY\u0026quot;, \u0026quot;QmWXkDxTf17MBUs41caHVXWJaz1SSAD79FVLbBYMTQSesw\u0026quot;, \u0026quot;QmWAeBjoGph92ktdDb5iciveKuAX3kQbFpr5wLWnyjtGjb\u0026quot; ]  This will allow you to start a Cluster from scratch with already fixed peerset.\nThe api section The api section contains configurations for the implementations of the API component, which are meant to provide endpoints for the interaction with Cluster. Removing any of these sections will disable the component. For example, removing the ipfsproxy section from the configuration will disable the proxy endpoint on the running peer.\nipfsproxy This component provides the IPFS Proxy Endpoint. This is an API which mimics the IPFS daemon. Some requests (pin, unpin, add) are hijacked and handled by Cluster. Others are simply forwarded to the IPFS daemon specified by node_multiaddress. The component is by default configured to mimic CORS headers configurations as present in the IPFS daemon. For that it triggers accessory requests to them (like CORS preflights).\n   Key Default Description     node_multiaddress \u0026quot;/ip4/127.0.0.1/tcp/5001\u0026quot; The listen address of the IPFS daemon API.   listen_multiaddress \u0026quot;/ip4/127.0.0.1/tcp/9095\u0026quot; The proxy endpoint listening address.   log_file \u0026quot;\u0026quot; A file to write request log files (Apache Combined Format). Otherwise they are written to the Cluster log under the ipfsproxylog facility.   node_https false Use HTTPS to talk to the IPFS API endpoint (experimental).   read_timeout \u0026quot;0s\u0026quot; Parameters for https://godoc.org/net/http#Server . Note setting this value might break adding to cluster, if the timeout is shorter than the time it takes to add something to the cluster.   read_header_timeout \u0026quot;30s\u0026quot; Parameters for https://godoc.org/net/http#Server .   write_timeout \u0026quot;0s\u0026quot; Parameters for https://godoc.org/net/http#Server . Note setting this value might break adding to cluster, if the timeout is shorter than the time it takes to add something to the cluster.   idle_timeout \u0026quot;30s\u0026quot; Parameters for https://godoc.org/net/http#Server .   extract_headers_extra [] If additional headers need to be extracted from the IPFS daemon and used in hijacked requests responses, they can be added here.   extract_headers_path \u0026quot;/api/v0/version\u0026quot; When extracting headers, a request to this path in the IPFS API is made.   extract_headers_ttl \u0026quot;5m\u0026quot; The extracted headers from extract_headers_path have a TTL. They will be remembered and only refreshed after the TTL.    restapi This is the component which provides the REST API implementation to interact with Cluster.\n   Key Default Description     http_listen_multiaddress \u0026quot;/ip4/127.0.0.1/tcp/9094\u0026quot; The API HTTP listen endpoint. Set empty to disable the HTTP endpoint.   ssl_cert_file \u0026quot;\u0026quot; Path to an x509 certificate file. Enables SSL on the HTTP endpoint. Unless an absolute path, relative to config folder.   ssl_key_file \u0026quot;\u0026quot; Path to a SSL private key file. Enables SSL on the HTTP endpoint. Unless an absolute path, relative to config folder.   read_timeout \u0026quot;0s\u0026quot; Parameters for https://godoc.org/net/http#Server . Note setting this value might break adding to cluster, if the timeout is shorter than the time it takes to add something to the cluster.   read_header_timeout \u0026quot;30s\u0026quot; Parameters for https://godoc.org/net/http#Server .   write_timeout \u0026quot;0s\u0026quot; Parameters for https://godoc.org/net/http#Server . Note setting this value might break adding to cluster, if the timeout is shorter than the time it takes to add something to the cluster.   idle_timeout \u0026quot;30s\u0026quot; Parameters for https://godoc.org/net/http#Server .   libp2p_listen_multiaddress \u0026quot;\u0026quot; A listen multiaddress for the alternative libp2p host. See below.   id \u0026quot;\u0026quot; A peer ID for the alternative libp2p host (must match private_key). See below.   private_key \u0026quot;\u0026quot; A private key for the alternative libp2p host (must match id). See below.   basic_auth_credentials null An object mapping \u0026quot;username\u0026quot; to \u0026quot;password\u0026quot;. It enables Basic Authentication for the API. Should be used with SSL-enabled or libp2p-endpoints.   headers null A key: [values] map of headers the API endpoint should return with each response to GET, POST, DELETE requests. i.e. \u0026quot;headers\u0026quot;: {\u0026quot;header_name\u0026quot;: [ \u0026quot;v1\u0026quot;, \u0026quot;v2\u0026quot; ] }. Do not place CORS headers here, as they are fully handled by the options below.   http_log_file \u0026quot;\u0026quot; A file to write API log files (Apache Combined Format). Otherwise they are written to the Cluster log under the restapilog facility.   cors_allowed_origins [\u0026quot;*\u0026quot;] CORS Configuration: values for Access-Control-Allow-Origin.   cors_allowed_methods [\u0026quot;GET\u0026quot;] CORS Configuration: values for Access-Control-Allow-Methods.   cors_allowed_headers [] CORS Configuration: values for Access-Control-Allow-Headers.   cors_exposed_headers [\u0026quot;Content-Type\u0026quot;, \u0026quot;X-Stream-Output\u0026quot;, \u0026quot;X-Chunked-Output\u0026quot;, \u0026quot;X-Content-Length\u0026quot;] CORS Configuration: values for Access-Control-Expose-Headers.   cors_allow_credentials true CORS Configuration: value for Access-Control-Allow-Credentials.   cors_max_age \u0026quot;0s\u0026quot; CORS Configuration: value for Access-Control-Max-Age.    The REST API component automatically, and additionally, can expose the HTTP API as a libp2p service on the main libp2p cluster Host (which listens on port 9096) (this happens by default on Raft clusters). Exposing the HTTP API as a libp2p service allows users to benefit from the channel encryption provided by libp2p. Alternatively, the API supports specifying a fully separate libp2p Host by providing id, private_key and libp2p_listen_multiaddress. When using a separate Host, it is not necessary for an API consumer to know the cluster secret. Both the HTTP and the libp2p endpoints are supported by the API Client and by ipfs-cluster-ctl.\npinsvcapi This is the component which provides the Pinning Services API implementation to interact with Cluster. It shares most of the code with the restapi, thus it has the same options.\n   Key Default Description     http_listen_multiaddress \u0026quot;/ip4/127.0.0.1/tcp/9094\u0026quot; The Pinning SVC API HTTP listen endpoint. Set empty to disable the HTTP endpoint.   ssl_cert_file \u0026quot;\u0026quot; Path to an x509 certificate file. Enables SSL on the HTTP endpoint. Unless an absolute path, relative to config folder.   ssl_key_file \u0026quot;\u0026quot; Path to a SSL private key file. Enables SSL on the HTTP endpoint. Unless an absolute path, relative to config folder.   read_timeout \u0026quot;0s\u0026quot; Parameters for https://godoc.org/net/http#Server . Note setting this value might break adding to cluster, if the timeout is shorter than the time it takes to add something to the cluster.   read_header_timeout \u0026quot;30s\u0026quot; Parameters for https://godoc.org/net/http#Server .   write_timeout \u0026quot;0s\u0026quot; Parameters for https://godoc.org/net/http#Server . Note setting this value might break adding to cluster, if the timeout is shorter than the time it takes to add something to the cluster.   idle_timeout \u0026quot;30s\u0026quot; Parameters for https://godoc.org/net/http#Server .   libp2p_listen_multiaddress \u0026quot;\u0026quot; A listen multiaddress for the alternative libp2p host. See notes in restapi.   id \u0026quot;\u0026quot; A peer ID for the alternative libp2p host (must match private_key). See notes in restapi.   private_key \u0026quot;\u0026quot; A private key for the alternative libp2p host (must match id). See notes in restapi.   basic_auth_credentials null An object mapping \u0026quot;username\u0026quot; to \u0026quot;password\u0026quot;. It enables Basic Authentication for the API. Should be used with SSL-enabled or libp2p-endpoints.   headers null A key: [values] map of headers the API endpoint should return with each response to GET, POST, DELETE requests. i.e. \u0026quot;headers\u0026quot;: {\u0026quot;header_name\u0026quot;: [ \u0026quot;v1\u0026quot;, \u0026quot;v2\u0026quot; ] }. Do not place CORS headers here, as they are fully handled by the options below.   http_log_file \u0026quot;\u0026quot; A file to write API log files (Apache Combined Format). Otherwise they are written to the Cluster log under the restapilog facility.   cors_allowed_origins [\u0026quot;*\u0026quot;] CORS Configuration: values for Access-Control-Allow-Origin.   cors_allowed_methods [\u0026quot;GET\u0026quot;] CORS Configuration: values for Access-Control-Allow-Methods.   cors_allowed_headers [] CORS Configuration: values for Access-Control-Allow-Headers.   cors_exposed_headers [\u0026quot;Content-Type\u0026quot;, \u0026quot;X-Stream-Output\u0026quot;, \u0026quot;X-Chunked-Output\u0026quot;, \u0026quot;X-Content-Length\u0026quot;] CORS Configuration: values for Access-Control-Expose-Headers.   cors_allow_credentials true CORS Configuration: value for Access-Control-Allow-Credentials.   cors_max_age \u0026quot;0s\u0026quot; CORS Configuration: value for Access-Control-Max-Age.    The ipfs_connector section The ipfs_connector section contains configurations for the implementations of the IPFS Connector component, which are meant to provide a way for the Cluster peer to interact with an IPFS daemon.\nipfshttp This is the default and only IPFS Connector implementation. It provides a gateway to the IPFS daemon API and an IPFS HTTP Proxy.\n   Key Default Description     listen_multiaddress \u0026quot;/ip4/127.0.0.1/tcp/9095\u0026quot; IPFS Proxy listen multiaddress.   node_multiaddress \u0026quot;/ip4/127.0.0.1/tcp/5001\u0026quot; The IPFS daemon HTTP API endpoint. This is the daemon that the peer uses to pin content.   connect_swarms_delay \u0026quot;30s\u0026quot; On start, the Cluster Peer will run ipfs swarm connect to the IPFS daemons of others peers. This sets the delay after starting up.   ipfs_request_timeout \u0026quot;5m0s\u0026quot; Specifies a timeout on general requests to the IPFS daemon for requests without a specific timeout option.   repogc_timeout \u0026quot;24h\u0026quot; Specifies a timeout on /repo/gc operations.   pin_timeout \u0026quot;2m0s\u0026quot; Specifies the timeout for pin/add which starts from the last block received for the item being pinned. Thus items which are being pinned slowly will not be cancelled even if they take more than 24h.   unpin_timeout \u0026quot;3h0m0s\u0026quot; Specifies the timeout for pin/rm requests to the IPFS daemon.   unpin_disable false Prevents the connector from unpinning anything (even if the Cluster does).   informer_trigger_interval 0 Force a broadcast of all peer metrics after the number of pin requests indicated by this value.    The pin_tracker section The pin_tracker section contains configurations for the implementations of the Pin Tracker component, which are meant to ensure that the content in IPFS matches the allocations as decided by IPFS Cluster.\nstateless The stateless tracker implements a pintracker which relies on ipfs and the shared state, only keeping track in-memory of ongoing operations.\n   Key Default Description     max_pin_queue_size 1000000 How many pin or unpin requests can be queued waiting to be pinned before we error them directly. Re-queing will be attempted on the next \u0026ldquo;state sync\u0026rdquo; as defined by state_sync_interval   concurrent_pins 10 How many parallel pin or unpin requests we make to IPFS.   priority_pin_max_age \u0026quot;24h0m0s\u0026quot; If a pin becomes this old and has failed to pin, retries will be deprioritized in the face of newer pin requests.   priority_pin_max_retries 5 If a pin has surpassed this number of pinning attempts, retries will be deprioritized in the face of newer pin requests.    The monitor section The monitor section contains configurations for the implementations of the Peer Monitor component, which are meant to distribute and collects monitoring information (informer metrics, pings) to and from other peers, and trigger alerts.\npubsubmon The pubsubmon implementation collects and broadcasts metrics using libp2p\u0026rsquo;s pubsub. This will provide a more efficient and scalable approach for metric distribution.\n   Key Default Description     check_interval \u0026quot;15s\u0026quot; The interval between checks making sure that no metrics are expired for any peers in the peerset. If an expired metric is detected, an alert is triggered. This may trigger repinning of items.    The informer section The informer section contains configuration for Informers. Informers fetch the metrics which are used to allocate content to the different peers.\ndisk The disk informer collects disk-related metrics at intervals.\n   Key Default Description     metric_ttl \u0026quot;30s\u0026quot; Time-to-Live for metrics provided by this informer. This will trigger a new metric reading at TTL/2 intervals.   metric_type \u0026quot;freespace\u0026quot; freespace or reposize. The informer will report the free space in the ipfs daemon repository (StorageMax-RepoSize) or the RepoSize.    tags The tags informer issues metrics based on user-provided tags. These \u0026ldquo;metrics\u0026rdquo; are just used to inform other peers of the tags associated to each peer. These tags are useful for the balanced allocator below, as they can be part of the allocate_by option. One metric is issued for every defined tag.\n   Key Default Description     metric_ttl \u0026quot;30s\u0026quot; Time-to-Live for metrics provided by this informer. This will trigger a new metric reading at TTL/2 intervals.   tags {\u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;} A simple \u0026ldquo;tag_name: tag_value\u0026rdquo; object to specify the tags associated to this peer.    pinqueue The pinqueue informer collects the number of pins in the pintracker\u0026rsquo;s pinning queue. It can be part of the allocate_by option in the balanced allocator to deprioritize pinning on peers with big queues. The weight_bucket_size option specifies by what amount the actual number of queued items should be divided. i.e If two peers have 53 and 58 items queued and weight_bucket_size is 1, then the peer with 58 items queued will be deprioritized by the allocator over the peer with 53 items. However if weight_bucket_size is 10, both peers will have the same weight (5), and thus prioritization will depend on other metrics (i.e. freespace).\n   Key Default Description     metric_ttl \u0026quot;30s\u0026quot; Time-to-Live for metrics provided by this informer. This will trigger a new metric reading at TTL/2 intervals.   weight_bucket_size 100000 The allocator will use the actual pin queue size divided by this value when comparing pinqueue metrics.    numpin The numpin informer uses the total number of pins as metric, which collects at intervals.\n   Key Default Description     metric_ttl \u0026quot;30s\u0026quot; Time-to-Live for metrics provided by this informer. This will trigger a new metric reading at TTL/2 intervals.    The allocator section The allocator is used to configure allocators. Allocators control how pins are assigned to peers in the cluster.\nbalanced The balanced allocator selects which peers to allocate pins to (when replication factor is larger than 0) by using the different metrics received from the peers to group and create a balanced distribution of every pin among those groups.\nFor example: Allocate by [\u0026quot;tag:group\u0026quot;, \u0026quot;freespace\u0026quot;], means that the allocator will divide all the peers based on the value of tag-metric \u0026ldquo;group\u0026rdquo; that they have first. Then it will order the peers in each group by their \u0026ldquo;freespace\u0026rdquo; metric value. When deciding which peers should a pin be allocated to, it will select the peer with most free-space from the group with most overall free-space. Then it will forcefully select the peer with most free-space from a second group (if it exists), as it is trying to balance allocations among existing groups.\nThis can be extended to subgroups: Assuming a cluster made of 6 peers, 2 per region (per a \u0026ldquo;region\u0026rdquo; tag), and one per availability zone (per an \u0026ldquo;az\u0026rdquo; tag), configuring the allocator with [\u0026quot;tag:region\u0026quot;, \u0026quot;tag:az\u0026quot;, \u0026quot;freespace\u0026quot;] will ensure that a pin with replication factor = 3 lands in the 3 different regions, in availability zone with most available aggregated space and in the peer in that zone with most available space.\n   Key Default Description     allocate_by [\u0026quot;tag:group\u0026quot;, \u0026quot;freespace\u0026quot;] Specifies by which informer metrics each pin should be allocated.    The observations section The observations section contains configuration for application distributed tracing and metrics collection.\nmetrics The metrics component configures the OpenCensus metrics endpoint for scraping of metrics by Prometheus.\n   Key Default Description     enable_stats false Whether metrics should be enabled.   prometheus_endpoint /ip4/127.0.0.1/tcp/8888 Publish collected metrics to endpoint for scraping by Prometheus.   reporting_interval \u0026quot;2s\u0026quot; How often to report on collected metrics.    The cluster peer exports the following cluster-specific metric, along with standard Go metrics:\n   Name Description     pins Total number of cluster pins   pins_pin_queued Current number of pins queued for pinning   pins_pinning Current number of pins currently pinning   pins_pin_error Current number of pins in pin_error state   pins_ipfs_pins Current number of pins in the local IPFS daemon   pins_pin_add Total number of pin requests made to IPFS   pins_pin_add_errors Total number of errors in pin requests made to IPFS   blocks_put Total number of block/put requests made to IPFS (i.e. when adding via cluster)   blocks_added_size Total size added to IPFS in bytes (when adding via cluster)   blocks_added Total number of blocks written to IPFS (when adding via cluster)   blocks_put_errors Total number of block/put requests errors   informer_disk The metric value weight issued by the disk informer (usually corresponds to free-space in bytes)    tracing The tracing component configures the Jaeger tracing client for use by OpenCensus.\n   Key Default Description     enable_tracing false Whether tracing should be enabled.   jaeger_agent_endpoint /ip4/0.0.0.0/udp/6831 Multiaddress to send traces to.   sampling_prob 0.3 How often to be sampling traces.   service_name cluster-daemon Service name that will be associated with cluster traces.    The datastore section The datastore section contains configuration for the storage backend. It can contain either a pebble, badger3, badger or a leveldb section.\npebble The pebble is the default datastore backend. It uses Pebble from CockroachDB. Pebble is best suited to most scenarios due to its conservative memory usage, short ready-time upon restarts and efficient disk footprint.\n   Key Default Description     pebble_options {...} Some Pebble specific options initialized to their defaults, including per level configuration.    badger3 The badger3 component configures the BadgerDB backend based on version 3. Badger3 can be very fast but configuration tuning is more difficult than Pebble, and very large repositories will need several minutes to be ready on start.\n   Key Default Description     gc_discard_ratio 0.2 See RunValueLogGC documentation.   gc_interval \u0026quot;15m0s\u0026quot; How often to run Badger GC cycles. A cycle is made of several rounds, which repeat until no space can be freed. Setting this to \u0026quot;0s\u0026quot; disables GC cycles.   gc_sleep \u0026quot;10s\u0026quot; How long to wait between GC rounds in the same GC cycle. Setting this to \u0026quot;0s\u0026quot; causes a single round to be run instead.   badger3_options {...} Some BadgerDBv3 specific options initialized to optimized defaults.    badger The badger component configures the BadgerDB backend based on version 1.6.2. We recommend new setups to use Pebble or Badger3. Badger is old, unmaintained and suffers from a number of issues, including large disk-space footprint.\n   Key Default Description     gc_discard_ratio 0.2 See RunValueLogGC documentation.   gc_interval \u0026quot;15m0s\u0026quot; How often to run Badger GC cycles. A cycle is made of several rounds, which repeat until no space can be freed. Setting this to \u0026quot;0s\u0026quot; disables GC cycles.   gc_sleep \u0026quot;10s\u0026quot; How long to wait between GC rounds in the same GC cycle. Setting this to \u0026quot;0s\u0026quot; causes a single round to be run instead.   badger_options {...} Some BadgerDB specific options initialized to optimized defaults (per IPFS recommendations, see below). Setting table_loading_mode and value_log_loading_mode to 0 should help in memory constrained platforms (Raspberry Pis etc. with \u0026lt;1GB RAM)    The adjustments performed on top of the default badger options by default can be seen in the badger configuration initialization code.\nleveldb The leveldb component configures the LevelDB backend which is used to store things when the CRDT consensus is enabled. We discourage using leveldb.\n   Key Default Description     leveldb_options {...} Some LevelDB specific options initialized to their defaults.    "
},
{
	"uri": "https://ipfscluster.io/documentation/",
	"title": "Documentation",
	"tags": [],
	"description": "",
	"content": " Documentation Updated to version 1.0.6 (see the Changelog). Welcome to IPFS Cluster documentation. The different sections of the documentation will explain how to setup, start, and operate a Cluster. Operating a production IPFS Cluster can be a daunting task if you are not familiar with concepts around IPFS and peer-2-peer networking (libp2p in particular). We aim to provide comprehensive documentation and guides but we are always open for improvements: documentation issues can be submitted to the ipfs-cluster-website repository.\nIf you need help, head over to the Support section.\nAre you an IPFS Cluster user? Let us know about your setup by contributing to the IPFS Cluster user registry. "
},
{
	"uri": "https://ipfscluster.io/documentation/quickstart/",
	"title": "Test Cluster Quickstart",
	"tags": [],
	"description": "",
	"content": " Test Cluster Quickstart This will help you setup a testing, local instance of IPFS Cluster using Docker and Docker Compose. The objective is that you get a quick preview of what is to run an IPFS Cluster and how you can interact with it. To successfully follow these instructions you need to be familiar with Docker and with running commands from the command line (including checking out a git-repository).\nWe will be starting a 3-peer cluster (along with IPFS daemons) using a docker compose template. Once the cluster is up and running, we will be interacting with one of the peers using ipfs-cluster-ctl.\n0. Install Docker and Docker Compose  Docker Docker Compose  1. Download ipfs-cluster-ctl Download and uncompress the latest version ipfs-cluster-ctl for your platform from dist.ipfs.io into a folder of your choice.\nipfs-cluster-ctl is the command-line client to the the IPFS Cluster daemon which we will use to inspect the cluster, add and pin content.\n2. Download the docker-compose.yml file Download the docker-compose.yml and place it in the same directory as ipfs-cluster-ctl.\n3. Start up the cluster From the folder in which you downloaded both files, run:\n$ docker-compose up  Wait until all the containers are running. You may see some errors because the cluster peers start too fast, before IPFs is ready, but they are harmless.\nSELinux Users If the services fail to start because of \u0026ldquo;Permission denied\u0026rdquo; errors, you may need to do the following:\n Look for AVC denials and grant the ipfs processes the needed SELinux permissions. sudo chmod -R 1000:100 compose. The services seem to use the root user when creating the compose directory. Add :z to the end of the volumes in the docker-compose.yml file. See Configuring SELinux Volumes in Docker for more information.  4. Play with the cluster You should now have a 3-peer IPFS Cluster running! Use ipfs-cluster-ctl on a different terminal (same folder) to interact with it:\n./ipfs-cluster-ctl peers ls # show information about the peers in the cluster ./ipfs-cluster-ctl add somefile # add a file to the cluster ./ipfs-cluster-ctl pin add /ipns/ipfscluster.io # pin the cluster website ./ipfs-cluster-ctl status \u0026lt;cid\u0026gt; # use the CID shown above to see the status in every peer ./ipfs-cluster-ctl pin ls \u0026lt;cid\u0026gt; # inspect the pin information  You can learn more about managing the pinset in the pinning guide.\nWhen you are done, you can run docker-compose kill.\n"
},
{
	"uri": "https://ipfscluster.io/documentation/guides/pinning/",
	"title": "Adding and pinning",
	"tags": [],
	"description": "",
	"content": " Adding and pinning Cluster usage mostly consists of adding and removing pins. This is usually performed by using the ipfs-cluster-ctl utility or talking to one of the Cluster APIs.\nYou can get help and usage information for all ipfs-cluster-ctl commands with ipfs-cluster-ctl --help and ipfs-cluster-ctl \u0026lt;command\u0026gt; --help When working with a large number of pins, it is important to keep an eye on the state of the pinset, whether every pin is getting correctly pinned an allocated. This section provides in-depth explanations on how pinning works and the different operations that a cluster peer can perform to simplify and maintain the cluster pinsets.\nFor clarity, we use ipfs-cluster-ctl commands, but every one of them is using an HTTP REST API endpoint from the cluster peer, so all commands can be performed directly against the API.  Adding files Pinning CIDs Replication factors The pinning process pin ls vs status Filtering results Recovering Automatic syncing and recovering  Adding files ipfs-cluster-ctl add myfile.txt  The ipfs-cluster-ctl add command is very similar to the ipfs add command and shares most of the same options (such as those that define chunking, the DAG type or which CID-version to use).\nHowever, where the ipfs add command only adds to the local IPFS daemon, the ipfs-cluster-ctl add command will add to several Cluster peers at the same time. How many it adds depends on the replication factors you set as command flags pin or the defaults in the configuration file.\nThis means that when the add process is finished, your file will have been fully added to several IPFS daemons (and not necessarily the local one). For example:\n$ ipfs-cluster-ctl add pinning.md added QmarNBnreCx4YtT4ETXxQ4dn2xQpcTGd2PaVM4b2UuyGku pinning.md $ ipfs-cluster-ctl pin ls QmarNBnreCx4YtT4ETXxQ4dn2xQpcTGd2PaVM4b2UuyGku # check pin data QmarNBnreCx4YtT4ETXxQ4dn2xQpcTGd2PaVM4b2UuyGku | | PIN | Repl. Factor: -1 | Allocations: [everywhere] | Recursive $ ipfs-cluster-ctl status QmarNBnreCx4YtT4ETXxQ4dn2xQpcTGd2PaVM4b2UuyGku # request status from every peer QmarNBnreCx4YtT4ETXxQ4dn2xQpcTGd2PaVM4b2UuyGku : \u0026gt; cluster0 : PINNED | 2019-07-26T12:25:18.23191214+02:00 \u0026gt; cluster1 : PINNED | 2019-07-26T10:25:18.234842017Z \u0026gt; cluster2 : PINNED | 2019-07-26T10:25:18.212836746Z \u0026gt; cluster3 : PINNED | 2019-07-26T10:25:18.238415569Z \u0026gt; cluserr4 : PINNED | 2019-07-26T10:25:24.508614677Z  The process of adding this way is slower than adding to a local IPFS daemon because all the blocks are sent to their remote locations in parallel. As an alternative, the --local flag can be provided to the add command. In this case, the content will be added to the local IPFS daemon of the peer receiving the request, and then pinned normally. This will make the add calls take less time, but the content will not be yet fully replicated when they return.\nAdding with --local and no additional options will always include the local peer among the allocations for the content (regardless of free space etc.). This can be worked around using the --allocations flag to provide different allocations manually, if needed.\nAnother feature in the add command that is not available on IPFS is the possibility of importing CAR files (similar to ipfs dag import). In order to import a CAR file you can do ipfs-cluster-ctl add --format car myfile.car. CAR files should have a single root, which is the CID that becomes pinned after import. IPFS Cluster does not perform checks to verify that the CAR files are complete and contain all the blocks etc.\nPinning CIDs ipfs-cluster-ctl pin add \u0026lt;cid/ipfs-path\u0026gt;  In many cases, you know what content from the IPFS network you want to add to your Cluster. The ipfs-cluster-ctl pin add operation is similar to the ipfs pin add one, but allows to set Cluster-specific flags, such the replication factors or the name associated to a pin. For example:\n$ ipfs-cluster-ctl pin add --name cluster-website --replication 3 /ipns/ipfscluster.io QmXvQLhK2heNz65fWRabTfbzXwYfaBgEBuTdUJNzp69Xjx : \u0026gt; cluster2 : PINNED | 2021-12-07T15:24:58Z | Attempts: 0 | Priority: false \u0026gt; cluster3 : PINNING | 2021-12-07T15:24:58Z | Attempts: 0 | Priority: true \u0026gt; cluster0 : PINNING | 2021-12-07T15:24:58Z | Attempts: 0 | Priority: true \u0026gt; Qmabc123 : REMOTE | 2021-12-07T16:25:07.48933158+01:00 | Attempts: 0 | Priority: false \u0026gt; Qmabc456 : REMOTE | 2021-12-07T16:25:07.4893358+01:00 | Attempts: 0 | Priority: false $ ipfs-cluster-ctl pin ls QmXvQLhK2heNz65fWRabTfbzXwYfaBgEBuTdUJNzp69Xjx QmXvQLhK2heNz65fWRabTfbzXwYfaBgEBuTdUJNzp69Xjx | cluster-website | PIN | Repl. Factor: 3--3 | Allocations: [12D3KooWGbmjg3MDUYFosLNPbE1jKkv5fzKHD7wyGDa1P95iKMjF QmYY1ggjoew5eFrvkenTR3F4uWqtkBkmgfJk8g9Qqcwy51] | Recursive | Metadata: no | Exp: âˆž | Added: 2021-12-07 16:24:58  As we see, the pin started pinning in two places (replication = 3). When we check the pin object, we see both the peer IDs it was allocated to and that it is called cluster-website. We also see whether it has any metadata attached, an expiry date and date it was last added.\nPins can be removed at any time with ipfs-cluster-ctl pin rm.\nReplication factors Every pin submitted to IPFS Cluster carries two replication options:\n replication_factor_min (--replication-min flag in ipfs-cluster-ctl) replication_factor_max (--replication-max flag in ipfs-cluster-ctl)  The cluster configuration sets the default values that apply when the option is not set.\nThe replication_factor_min value specifies the minimal number of copies that the pin should have in the cluster. If automatic repinning is enabled and the cluster detects that the peers that should be pinning an item are not available, and that the item is under-replicated (the number of peers pinning it is below replication_factor_min), it will re-allocate the item to new peers. Pinning will fail directly when there are not enough peers to pin something up to replication_factor_min.\nThe replication_factor_max value indicates how many peers should be allocated to the pin. On pin submission, the cluster will try to allocate that many peers, but not fail if it cannot find so many, as long as it finds more than replication_factor_min. Repinnings of an item will try to increase allocations to replication_factor_max, however automatic repinnings of an item, when enable, will not affect pins that are between the two thresholds.\nThe recommendation is to use thresholds with some leeway (usually 2-3, or 3-5) when disable_repinning is set to false. In this case, without leeway, a cluster peer going down for a few seconds could trigger repinnings and result in an unbalanced cluster, even if the peer comes up fine later and still holds the content (at which point it will be unpinned because it is no longer allocated to it).\nThe pinning process Cluster-pinning and unpinning are at the core of the cluster operation and involve multiple internal components but have two main stages:\n The Cluster-pinning stage: the pin is correctly persisted by cluster and broadcasted to every other peer. The IPFS-pinning stage: every peer allocated to the pin must ensure the IPFS daemon gets it pinned.  The stages and the results they produce are actually inspectable with the different options for pin add:\n ipfs-cluster-ctl pin add ... waits 1 second by default and reports status resulting from the ongoing IPFS-pinning process. ipfs-cluster-ctl pin add --no-status ... does not wait and reports pin ls information resulting from the Cluster-pinning process. ipfs-cluster-ctl pin add --wait ... waits until the IPFS-pinning process is complete in at least 1 peer.  The Cluster-pinning stage We consider a pin add operation has been successful when the cluster-pinning stage is finished. This means the pin has been ingested by Cluster and that things are underway to tell IPFS to pin the content. If IPFS fails to pin the content, Cluster will know, report about it and try to handle the situation. The cluster-pinning stage is relatively fast, but the ipfs-pinning stage can take much longer depending on the amount of things being pinned and the sizes involved. Therefore the second stage happens asynchronously once the cluster-pinning stage is completed.\nThe process can be summarized as a follows:\n A pin request arrives including certain options. The allocation process (see below) chooses which peers should be pinning the content, based on replication factors and metrics. These and other things result in a pin object which is committed and broadcasted to everyone (the how depends on the consensus component).  The allocation process is in charge of producing the final allocations for a pin (list of cluster peer ids), that are attached to the pin object. It strives to select the best places to pin things. The process takes several inputs:\n The pin min and max replication factors and user-set allocations. The metrics broadcasted by all the peers in the cluster. The configuration of the allocator (allocate_by). The existing allocations if the pin already existed.  In general, the process consists in building a priority list of which peers should be pinning a CID and selecting up to replication_factor_max from it. The order in this list depends on the \u0026ldquo;allocator\u0026rdquo; component and the metrics that it is configured to use. In order for a peer in the cluster to be considered as an allocation, all the necessary metrics need to be recent (not expired).\nFor example, if allocate_by is set to [ \u0026quot;freespace\u0026quot; ], the peers are ordered by the value of this metric and the ones with most freespace are selected as allocations. If allocate_by is set to [ \u0026quot;tag:region\u0026quot;, \u0026quot;freespace\u0026quot; ], then we will group peers by their \u0026ldquo;region\u0026rdquo; and then sort them by their \u0026ldquo;freespace\u0026rdquo;, and then sort the regions by their aggregate free-space. The first peer will be the peer with most free-space in the region with most aggregated free-space. The second peer, however, will come from the second region with most free-space and so on. This results in pins being allocated to peers in different regions. In general the process can be seen as putting peers into buckets of different sizes, which have additional buckets inside, the selection process works by selecting peers from as many different buckets as possible.\nThere are some exceptions to how the allocations are determined: if a pin already exists and has allocations, they are not changed unless those allocated peers have become unavailable causing the min_replication_factor limit to be crossed. Similarly, if a pin is already allocated to some peers and we are changing the replication factor, the previously allocated peers are respected. If a user has specified manual allocations when pinning, those peers are given priority regardless of their metric values, as long as they are available (otherwise they are ignored).\nThe IPFS-pinning stage Once the Cluster-pinning stage is completed, each peer is notified of a new item in the pinset. If the peer is among the allocations for that item, it will proceed to ask ipfs to pin it:\n Peer starts \u0026ldquo;tracking\u0026rdquo; CID If allocated to it, it queues it for pinning and when it turns comes in the queue an \u0026ldquo;ipfs pin add\u0026rdquo; operation is triggered The tracking process waits for the \u0026ldquo;ipfs pin add\u0026rdquo; operation to succeed. The pinning operation may time out based on when was the last block received by ipfs, not based on the total amount of time spent pinning. When the pinning completes, the item is considered pinned. If an error happens while pinning, the item goes into error state and will be eventually retried by the cluster, increasing its attempt count.  The pinning process has two different queues which take the available pinning slots. The first one is a \u0026ldquo;priority\u0026rdquo; one for new items and items that have not failed to pin many times. The other is used for the rest of items. This allows that pins that cannot complete for whatever reason do not stand in the way and use pinning slots for new pins.\nCRDT-Batching When CRDT-batching is enabled, pins will be batched in the local peer receiving the pin requests and submitted all together to the network in a single batch. This happens only when the batch reaches its maximum age, or when it gets full (both things controlled in the configuration).\nIf a peer is restarted before a batch has been broadcasted, these pins will be lost. Thus we recommend stopping requests and waiting for the batch max_age before restarting peers.\npin ls vs status It is very important to distinguish between ipfs-cluster-ctl pin ls and ipfs-cluster-ctl status. Both endpoints provide a list of CIDs pinned in the cluster, but they do it in very different ways:\n pin ls shows shows information from the cluster shared state or global pinset which is fully available in every peer. It shows which are the allocations and how the pin has been configured. For example:  QmXvQLhK2heNz65fWRabTfbzXwYfaBgEBuTdUJNzp69Xjx | cluster-website | PIN | Repl. Factor: 2--3 | Allocations: [12D3KooWGbmjg3MDUYFosLNPbE1jKkv5fzKHD7wyGDa1P95iKMjF QmYY1ggjoew5eFrvkenTR3F4uWqtkBkmgfJk8g9Qqcwy51] | Recursive | Metadata: no | Exp: âˆž | Added: 2021-12-07 16:24:58   status. however, requests information about the status of each pin on each cluster peer allocated to it, including whether that CID is PINNED on IPFS, or still PINNING, or errored for some reason:  bafybeiary2ibmljf3l466qzk5hud3rnlk7zped37oik64zlfh22sa5nrg4 | cluster-website: \u0026gt; cluster2 : PINNED | 2021-12-07T15:24:58Z | Attempts: 0 | Priority: false \u0026gt; cluster3 : PINNED | 2021-12-07T15:24:58Z | Attempts: 0 | Priority: false \u0026gt; cluster0 : PINNED | 2021-12-07T15:24:58Z | Attempts: 0 | Priority: false \u0026gt; QmYAajUVaFMw7EyUsZqwDhbNmCsP8L7VDRLuXNkEw6DCC1 : REMOTE | 2021-12-07T16:40:08.122100484+01:00 | Attempts: 0 | Priority: false \u0026gt; QmaHvxFk6DoNsRHqe2a7UJH66AjGDKPG2HCBxr25YYop32 : REMOTE | 2021-12-07T16:40:08.122100484+01:00 | Attempts: 0 | Priority: false  In order to show this information, the status request must contact every of the peers allocated to the pin (only the peers pinning something can tell on what state that operation is). Thus, the status request can be very expensive on clusters with many peers, but provides very useful information on the state of pin. Both commands take an optional cid to limit the results to a single item. The status results include information to inspect how many attempts to pin something have occurred and whether the last attempt happened via the priority pinning queue or not. Finally, the status request supports a --local flag to just report status from the local peer.\nFiltering results The status commands supports filtering to display only pins which are in a given situation (in at least one of the peers). The following filters are supported:\n cluster_error: pins for which we cannot obtain status information (i.e. the cluster peer is down) pin_error: pins that failed to pin (due to an ipfs problem or a timeout) unpin_error: pins that failed to unpin (due to an ipfs problem or a timeout) error: pins in pin_error, unpin_error or cluster_error unexpected_unpinned: pins that are not pinned by ipfs, yet they should be pinned and are not pin_queued or pinning right now. pinned: pins were correctly pinned pinning: pins that are currently being pinned by ipfs unpinning: pins that are currently being unpinned by ipfs remote: pins that are allocated to other cluster peers (remote means: not handled by this peer). pin_queued: pins that are waiting to start pinning (usually because ipfs is already pinning a bunch of other things) unpin_queued: pins that are waiting to start unpinning (usually because something else is being unpinned) queued: pins in pin_queued or unpin_queued states.  It is sometimes useful to combine this option with the --local one. For example:\nipfs-cluster-ctl status --local --filter pinning,queued  will only display CIDs which are still pinning or queued (waiting to start pinning) in the local peer.\nipfs-cluster-ctl status --filter error  will display status information for CIDs which are in error state for some reason (the error message contains more information).\nipfs-cluster-ctl status --help provides more information on usage and options Recovering Sometimes an item is pinned in the Cluster but it actually fails to pin on the allocated IPFS daemons because of different reasons:\n The IPFS daemon is down or not responding The pin operation times out or errors It is manually removed from IPFS.  In these cases, the items will show a status of PIN_ERROR (equivalently, also UNPIN_ERROR when removing) or UNEXPECTEDLY_UNPINNED. This is not a cluster issue and it usually indicates a problem with IPFS (content is not available etc.).\nIn such cases, the ipfs-cluster-ctl recover can be used to retrigger a pin or unpin operation against the allocated ipfs daemons as needed, once the problems have been fixed. As explained below, recover operations are regularly triggered by every cluster peer automatically anyways. Note that pins can also be re-added with pin add, obtaining a similar effect. The main difference is that recover happens in sync (waits until done), while pin add returns immediately.\nipfs-cluster-ctl recover --help provides more information on usage and options. Automatic unpinning and recovering Cluster peers run unpin operations for expired items and recover operations automatically, in intervals defined in the configuration:\n state_sync_interval how often to check for expired items and potentially trigger unpin requests. pin_recover_interval controls the interval to trigger recover operations for all pins in error state.  "
},
{
	"uri": "https://ipfscluster.io/download/",
	"title": "Download",
	"tags": [],
	"description": "",
	"content": " Download We offer ipfs-cluster-service, ipfs-cluster-ctl and ipfs-cluster-follow through several options. For information on how to setup and run IPFS Cluster:\n Read the documentation Check the command help (--help) (ipfs-cluster-ctl, ipfs-cluster-service), ipfs-cluster-follow Get support  Binary distribution IPFS Cluster provides pre-built binaries for several platforms on the IPFS Distributions page:\n ipfs-cluster-service ipfs-cluster-ctl ipfs-cluster-follow  You can download these binaries, make them executable and run them directly. They include all the necessary dependencies.\nThe prebuilt-binaries are only updated on new releases (with occasional release candidates). These releases aim to provide a stable distribution of IPFS Cluster.\nDocker We have automatic docker builds (https://hub.docker.com/r/ipfs/ipfs-cluster/) to create a minimal container that runs ipfs-cluster-service by default. You can get it with:\ndocker pull ipfs/ipfs-cluster:\\\u0026lt;tag\\\u0026gt;  where \u0026lt;tag\u0026gt; is either latest or a tagged version of cluster (i.e. v0.11.0). The latest build is built from master.\nSee the Docker documentation section for more details. Installing from source The following requirements apply to the installation from source:\n Go 1.12+ Git  In order to build and install IPFS Cluster follow these steps:\ngit clone https://github.com/ipfs-cluster/ipfs-cluster.git cd ipfs-cluster make install  After the dependencies have been downloaded, ipfs-cluster-service, ipfs-cluster-ctl and ipfs-cluster-follow will be installed to your $GOPATH/bin.\nIf you would rather have them built locally, use make build.\nBuilding the docker image Run\u0026hellip;\ndocker build . -t ipfs-cluster  \u0026hellip;in the repository root.\n"
},
{
	"uri": "https://ipfscluster.io/documentation/collaborative/setup/",
	"title": "Hosting a collaborative cluster",
	"tags": [],
	"description": "",
	"content": " Collaborative clusters setup In order to create your own collaborative cluster that other people can subscribe to you will need to:\n Setup your regular production deployment of IPFS Cluster in CRDT mode (these will be your \u0026ldquo;trusted peers\u0026rdquo;). Distribute a configuration template so that follower peers can easily join the cluster. Let users take advantage of ipfs-cluster-follow.  Trusted peers setup Collaborative clusters must use CRDT mode. The first step in setting a collaborative cluster is to deploy a regular CRDT cluster with one or more peers.\nFollow the instructions in the Production deployment section, particularly those related to the CRDT mode bootstrapping. The simplest is to run a single-peer cluster, although you may choose to run two or more to have some redundancy.\nA summarized version of the instructions for a single peer with default configuration would amount to the following:\n# Start your ipfs daemon $ ipfs-cluster-service init --consensus crdt $ ipfs-cluster-service daemon # Write down: # - The generated cluster secret (will need to be re-used in other peers) # - The peer ID (this will be a \u0026quot;trusted peer\u0026quot;) # - The multiaddress on which it will be reachable by other peers (usually /ip4/public_ip/tcp/9096/p2p/peer_id  Once you have your base cluster configured and running, you will need to make sure that the trusted_peers array in the crdt configuration section is set to the peer IDs in your base cluster. Otherwise (if set to the default *), anyone might be able to modify the pinset and this may be something you don\u0026rsquo;t want.\nReview the resulting configuration in your cluster peers:\n All trusted peers in your setup should probably have the same configuration (unless they are running on machines with different requirements) trusted_peers should be set to the list of peer IDs in the original cluster that are under your control (or someone\u0026rsquo;s trusted control). You should have generated a cluster secret. It will be ok to distribute this secret later. Depending on your Cluster setup, who you plan to join the cluster, and the level of trust on those follower peers, you can set replication_factor_min/max. For the general usecase, we recommend leaving at -1 (everything pinned everywhere). The main usecase of collaborative clusters is to ensure wide distribution and replication of content. You can modify the crdt/cluster_name value to your liking, but remember to inform your followers about its value.  In principle, followers can use exactly the same configuration as your trusted peers, but we recommend tailoring a specific follower configuration as explained in the next section.\nDistributing a configuration template Any follower peer can start a IPFS cluster peer that will join your collaborative clusters with these pieces of information:\n The list of trusted_peers in the Cluster. The full, reachable multiaddress of at least one of those peers. The cluster secret. If changed, the value of crdt/cluster_name  It is however better when you can distribute a configuration template which has all the options set right for your cluster. Ultimately, you will want follower peers to run using ipfs-cluster-follow, rather than ipfs-cluster-service.\nCreating a configuration template for followers Follower peers can technically use the same configuration as trusted peers but we recommend considering a couple of modifications. The following apply to a copy of your service.json file that you will distribute to your followers:\n Set peer_addresses to the addresses of your trusted peers. These must be reachable whenever any follower peer starts, so ensure there is connectivity to your cluster. Consider removing any configurations in the api section (restapi, ipfsproxy): follower peers should not be told how their peers APIs should look like. Misconfiguring the APIs might open unwanted security holes. ipfs-cluster-follow overrides any api configuration by creating a secure, local-only endpoint. Reset connection_manager/high_water and low_water to sensible defaults if you modified them for your trusted peers configuration. Set follower_mode to true: while non-trusted peers cannot do anything to the cluster pinset, they can still modify their own view of it, which may be very confusing. This setting (which ipfs-cluster-follow activates automatically) ensures useful error messages are returned when trying to perform write actions. If you are running multiple collaborative clusters, or expect your users to do so, consider modifying the addresses defined in listen_multiaddress by changing the default ports to something else, hopefully unused. You can use 0 as well, so that peers choose a random free port during start, but this will cause that peers change ports on every re-start (how important that is depends on your setup).  After all these changes, you will have a service.json file that is ready to be distributed to followers. Test it first:\n ipfs-cluster-service -c someFolder init Replace the generated default service.json with the follower configuration you created. Run ipfs-cluster-service -c someFolder daemon and make sure a peer starts and joins the main cluster (ipfs-cluster-ctl peers ls on one of the trusted peers should show it).  Distributing the template Once you have a configuration template to provide to your followers, you can either host it on a webserver and make it accessible through HTTP(s), or, more interestingly, add it to IPFS (and to the cluster you created) and make it accessible through every IPFS daemon\u0026rsquo;s gateway:\nipfs-cluster-ctl add follower_service.json --name follower-config  Once the configuration is on IPFS, any follower peer can be configured from it by reading it via the local IPFS gateway.\nFor example, when using ipfs-cluster-service:\nipfs-cluster-service init http://127.0.0.1:8080/ipfs/Qm....  We, however, recommend using ipfs-cluster-follow, instead:\nipfs-cluster-follow myCluster init http://127.0.0.1:8080/ipfs/Qm...  If you have a domain name you can control, we recommend using it to set a DNSLINK TXT record pointing to the hash of the configuration /ipfs/Qm.... This way your users can use http://127.0.0.1:8080/ipns/my.domain.com instead, and you can update the configuration by updating the dnslink value. Configurations are always read during the start of the peer (not during initialization).\nipfs-cluster-follow The ipfs-cluster-follow command is specially crafted to make it super easy to join collaborative clusters. We recommend that follower peers are always run using this command rather than ipfs-cluster-service.\nipfs-cluster-follow does a number of things to improve user experience when joining collaborative clusters:\n It allows to initialize and run multiple peers by separating configuration folders for each based on a given cluster name. It is streamlined to use IPFS-hosted configuration templates, translating my.domain.com to http://127.0.0.1:8080/ipns/my.domain.com during initialization. It can initialize and run the peer in a single command. It runs, by default, a local HTTP API endpoint on a local socket, preventing conflicts for multiple API endpoints listening on the same ports and ensuring the follower peers API cannot be accessed from the outside. Any API configuration provided in the template is discarded. It sets follower_mode automatically along with a number of other small changes to the config for a better user experience.  You can find more about ipfs-cluster-follow here and in the joining a collaborative cluster section.\n"
},
{
	"uri": "https://ipfscluster.io/documentation/deployment/",
	"title": "Production deployment",
	"tags": [],
	"description": "",
	"content": " Production deployment This section is meant for administrators looking to setup IPFS and IPFS Cluster on a production environment. Administrators are expected to be familiar with IPFS and with the deployment of production applications (including reading the logs, being able to verify if ports are open, if connectivity exists between peers or if process is running).\nAdditionally, running IPFS Cluster in production requires:\n Basic understanding of the Cluster application architecture and how it interacts with IPFS Adjusting ipfs and ipfs-cluster-service configurations to the environment and the requirements of the cluster, as well as ensuring things have connectivity as needed (firewall ports etc.). Starting the ipfs-cluster-service daemons and verifying that they can connect and sync from each others. Optionally automating the deployment and lifecycle of cluster peers.  These topics are explained in the sections below:\n Architecture overview   Download and setup   Bootstrapping the Cluster   Deployment automations   "
},
{
	"uri": "https://ipfscluster.io/documentation/reference/api/",
	"title": "REST API",
	"tags": [],
	"description": "",
	"content": " REST API Reference IPFS Cluster peers include an API component which provides HTTP-based access to the peer\u0026rsquo;s functionality. The API attempts to be REST-ful in form and behaviour. It is enabled by default, but it can be disabled by removing its section from the service.json configuration file.\nWe do not maintain ad-hoc API documentation, as it gets easily out of date or, at worst, is innaccurate or buggy. Instead, we provide an easy way to find how to do what you need to do by using the ipfs-cluster-ctl command.\nRunning ipfs-cluster-ctl --enc=json --debug \u0026lt;command\u0026gt; will print information about the endpoint, the query options, the request body and raw responses for that command. Use it on a test cluster!\nipfs-cluster-ctl is an HTTP API client to the REST API endpoint with full feature-parity that always works with the HTTP API as offered by a cluster peer on the same version. Anything that ipfs-cluster-ctl can do is supported by the REST API. The command flags usually control different request options.\nAs additional resources:\n All the available API endpoints and their parameters and object formats are supported and documented by the Go API Client. The API source code is here (the routes method is a good place to start). There are two Javascript client libraries: js-cluster-client (old) and NFT.storage\u0026rsquo;s cluster client (new). The request body for the /add endpoint is a bit special, but it works just like the IPFS one. See the section below.  The above should be enough to find out about the existing endpoints, their methods and current supported options.\nAuthentication The REST API supports both Basic and JWT token authentication:\n Basic Authentication credentials are stored in service.json configuration file (basic_auth_credentials). JWT token authentication works by sending requests with an Authorization: Bearer \u0026lt;JWT-token\u0026gt; header. The access token can be obtained by querying the POST /token endpoint. In order to obtain an access token, the user needs to be part of basic_auth_credentials and have an associated password. The JWT token is tied to the user requesting it and signed using their password. The only way to revoke JWT tokens right now is to change or remove the original Basic Auth credentials, which need an ipfs-cluster-service restart.  The /add/ endpoint The /add endpoint can be use to upload content to IPFS via the Cluster API. The Cluster peer is in charge of building or extracting the IPLD DAG, which is sent block by block to the cluster peers where it should be pinned, which in turn perform block/put calls to the IPFS daemon they are connected to. At the end of the process, a Cluster-Pin happens and with it the pinning operation arrives to the IPFS daemons which should already have all the needed blocks.\nThere are considerations to take into account here:\n Adding content via IPFS Cluster is slower because it replicates to all pinning locations at the same time that it adds. The local=true query parameter will instruct the cluster peer receiving the request to ingest all blocks locally. This makes adding way faster and the expense of a slower Cluster-pinning: the pinning nodes will have to use IPFS to receive the blocks when they are instructed to pin. IPFS garbage collection should be disabled while adding. Because blocks are block/put individually, if a GC operation happens while and adding operation is underway, and before the blocks have been pinned, they would be deleted.  Currently IPFS Cluster supports adding with two DAG-formats (?format= query parameter):\n By default it uses the unixfs format. In this mode, the request body is expected to be a multipart just like described in /api/v0/add documentation. The /add endpoint supports the same optional parameters as IPFS does and produces exactly the same DAG as go-ipfs when adding files. In UnixFS, files uploaded in the request are chunked and a DAG is built replicating the desired folder layout. This is done by the cluster peer. Alternatively, the /add endpoint also accepts a CAR file with ?format=car format. In this case, the CAR file already includes the blocks that need to be added to IPFS and Cluster does not do any further processing (similarly to ipfs dag import). At the moment, the /add endpoint will process only a single CAR file and this file must have only one root (the one that will be pinned). CAR files allow adding arbitrary IPLD-DAGs through the Cluster API.  Using the /add endpoint with Nginx in front as a reverse proxy may cause problems. Make sure to add ?stream-channels=false to every Add request to avoid them.\nThe problems manifest themselves as \"connection reset by peer while reading upstream\" errors in the logs. They are caused by read after write on the HTTP request/response cycle: Nginx refuses any application that has started sending the response body to read further from the request body (see bug report). IPFS and IPFS Cluster send object updates while adding files, therefore triggering the situation, which is otherwise legal per HTTP specs. The issue depends on Nginx internal buffering and may appear very sporadically or not at all, but it exists. List of endpoints As a final tip, this table provides a quick summary of methods available.\n   Method Endpoint Comment     GET /id Cluster peer information   GET /version Cluster version   GET /peers Cluster peers. Streaming endpoint.   DELETE /peers/{peerID} Remove a peer   POST /add Add content to the cluster. Streaming endpoint. See notes above   GET /allocations List of pins and their allocations (pinset). Streaming endpoint.   GET /allocations/{cid} Show a single pin and its allocations (from the pinset)   GET /pins Local status of all tracked CIDs. Streaming endpoint.   GET /pins/{cid} Local status of single CID   POST /pins/{cid} Pin a CID   POST /pins/{ipfs\\|ipns\\|ipld}/\u0026lt;path\u0026gt; Pin using an IPFS path   DELETE /pins/{cid} Unpin a CID   DELETE /pins/{ipfs\\|ipns\\|ipld}/\u0026lt;path\u0026gt; Unpin using an IPFS path   POST /pins/{cid}/recover Recover a CID   POST /pins/recover Recover all pins in the receiving Cluster peer   GET /monitor/metrics Get a list of metric types known to the peer   GET /monitor/metrics/{metric} Get a list of current metrics seen by this peer   GET /health/alerts Display a list of alerts (metric expiration events)   GET /health/graph Get connection graph   GET /health/alerts Get connection graph   POST /ipfs/gc Perform GC in the IPFS nodes   POST /token Generate a new JWT token for current user    "
},
{
	"uri": "https://ipfscluster.io/documentation/reference/pinsvc_api/",
	"title": "Pinning Service API",
	"tags": [],
	"description": "",
	"content": " IPFS Pinning Service API IPFS Cluster peers include an API component which implements the IPFS Pinning Service API.\nThis API component is experimental and it attempts to have full compliance with the Spec, but it is important to keep in mind a number of caveats:\n IPFS Cluster does not support tracking the same CID multiple times (i.e. once per user). That means that pinning the same CID multiple times will always result in the same \u0026ldquo;request ID\u0026rdquo;. In fact, the request ID is the CID. IPFS Cluster stores the pinset in an unordered key-value store where the keys are CIDs. That means that pagination and filtering features of this API have partial support: pagination is not supported. Anything else will degrade badly on very large pinsets: for example, filtering by before will need to list everything in the pinset and select only the things that match, which can be a very expensive operation. The Pinning Service API specifies that pin lists must be returned as a single JSON-slice. Cluster will build the response in-memory until it can be sent to the user, which can cause large memory bubbles when the list of pins is very large (Pinning Service API does not support streaming as we do it in the REST API).  Other than that, adding, getting and deleting pins should be well-supported operations with little overhead versus the REST API, and with very similar semantics.\nAuthentication Per the Spec, the Pinning Service API must support JWT token authentication. JWT tokens can be generated by querying the POST /token endpoint. The authorization flow is shared with the REST API. This means that Basic Authentication is supported as well, and that any user using JWT tokens must have an entry and a password associated to it in the basic_auth_credentials setting for the pinsvc API section:\n$ curl -X POST -u \u0026quot;myuser:mypassword\u0026quot; 'http://clusterip:9097/token'  This returns the following JSON: {\u0026quot;token\u0026quot;:\u0026quot;xxx...\u0026quot;}. The secret access token is what you need to add your pinning service in Kubo:\nipfs pin remote service add mycluster \u0026quot;http://clusterip:9097\u0026quot; \u0026quot;xxx...\u0026quot;  Or via WebUI:\nThe JWT token is tied to the user requesting it and signed using their password. The only way to revoke JWT tokens right now is to change or remove the original Basic Auth credentials, which need an ipfs-cluster-service restart.\n"
},
{
	"uri": "https://ipfscluster.io/documentation/collaborative/",
	"title": "Collaborative Clusters",
	"tags": [],
	"description": "",
	"content": " Collaborative Clusters This section documents the collaborative clusters feature in IPFS Cluster. Collaborative clusters allow individual, untrusted peers to join an IPFS Cluster as \u0026ldquo;followers\u0026rdquo; (without permission to modify or edit the pinset). In the subsections we detail how to setup and join these clusters:\n Hosting a collaborative cluster   Joining a collaborative cluster   "
},
{
	"uri": "https://ipfscluster.io/documentation/guides/consensus/",
	"title": "Consensus components",
	"tags": [],
	"description": "",
	"content": " Consensus components IPFS Cluster peers can be started using different choices for the implementations of some components. The most important one is the \u0026ldquo;consensus\u0026rdquo; one. The \u0026ldquo;consensus component\u0026rdquo; is in charge of:\n Managing the global cluster pinset by receiving modifications from other peers and publishing them. Managing the persistent storage of pinset-related data on disk. Achieving strong eventual consistency between all peers: all peers should converge to the same pinset. Managing the Cluster peerset: performing the necessary operation to add or remove peers from the Cluster. Setting peer trust: defining which peers are trusted to perform actions and access local RPC endpoints.  IPFS Cluster offers two \u0026ldquo;consensus component\u0026rdquo; options and the users are forced to make a choice when initializing a cluster peer by providing either --consensus crdt or --consensus raft to the init command.\nFor offline cluster pinset management check the Data, backups and recovery section.\nCRDT crdt is the default implementation of the Cluster\u0026rsquo;s \u0026ldquo;consensus component\u0026rdquo; based on an ipfs-powered distributed key-value store. It:\n Publishes updates to the pinset via libp2p-pubsub (GossipSub), locates and exchange data via ipfs-lite (dht+bitswap). Stores all persistent data on a local datastore in the .ipfs-cluster/ folder (pebble, badger3, badger or leveldb folder, depending on selected option). Uses Merkle-CRDTs to obtain eventual consistency using go-ds-crdt. These are append-only, immutable Merkle-DAGs. They cannot be compacted on normal conditions and new peers must discover and traverse them from the root, which might be a slow operation if the DAG is very deep. Does not need to perform any peerset management. Every peer for which we received \u0026ldquo;pings\u0026rdquo; via pubsub is considered a member of the Cluster until their last metric expires. Trusts peers as defined in the trusted_peers configuration option: only those peers can modify the pinset in the local peer and can access \u0026ldquo;trusted\u0026rdquo; RPC endpoints. Can optionally batch many pin/unpin operations on a single update, thus allowing scaling pin ingestion capabilities.  We successfully use crdt on very large clusters and it is regularly maintained and heavily battle-tested.\nRaft The Raft implementation has not received much attention for a long time and is legacy now. It probably does not behave well with very large pinsets in terms of memory consumption. raft is an implementation of the Cluster\u0026rsquo;s \u0026ldquo;consensus component\u0026rdquo; based on Raft consensus. It:\n Publishes updates by connecting and sending them directly to every Cluster peer. Stores all persistent data on a local BoltDB store and on regular snapshots in the .ipfs-cluster/raft folder. Uses Raft-consensus implementation (hashicorp/raft with a libp2p network transport) to obtain eventual consistency and protection of network partitions. Peerset views can be outdated in Raft, but they can never diverge in ways that need reconciliation. Raft-clusters elect a leader which is in charge of committing every entry to the log. For it to be valid, more than half of the peers in the cluster must acknowledge each operation. The append-only log can be consolidated and compacted into a snapshot which can be sent to new peers. Performs peerset management by making peerset changes (adding and removing peers) a commit operation in the Raft log, thus subjected to the limitations of them: an elected leader and more than half of the peers online. Trusts all peers. Any peer can request joining a Raft-based cluster and any peer can access RPC endpoints of others (as long as they know the Cluster secret).  Choosing a consensus component Choose CRDT when:\n You expect your cluster to work well with peers easily coming and going You plan to have follower peers without permissions to modify the pinset You do not have a fixed peer(s) for bootstrapping or you need to take advantage of mDNS autodiscovery The cluster needs to accommodate regular and heavy bursts of pinning/unpinning operations (batching support helps).  Choose Raft when:\n You think the above warning about Raft consensus being legacy does not apply to you. Your cluster peerset is stable (always the same peers, always running) and not updated frequently You cannot tolerate temporary partitions that result in divergent states You don\u0026rsquo;t need any of the things CRDT mode provides Your cluster will be small in terms of pins  CRDT vs Raft comparison    CRDT Raft     GossipSub broadcast Explicit connection to everyone + acknowledgments   Trusted-peer support All peers trusted   \u0026ldquo;Follower peers\u0026rdquo; and \u0026ldquo;Publisher peers\u0026rdquo; support All peers are publishers   Peers can come and go freely \u0026gt;50% must be online at all times or nothing works. Errors logged when someone is not online.   State size always grow State size reduced after deletions   Cluster state Compaction only possible by taken a full cluster offline Automatic compaction of the state   Potentially slow first-sync Faster first-sync by sending full snapshot   Works with very large number of peers Works with a small number of peers   Based on IPFS-tech (bitswap, dht, pubsub) Based on hashicorp-tech (raft)   Strong Eventual Consistency: Pinsets can diverge until they are consolidated Consensus: Pinsets can be outdated but never diverge   Fast pin ingestion with batching support Slow pin ingestion   Pin committed != Pin arrived to most peers Pin committed == pin arrived to most peers   Maximum tested size: 60M pins 100k pins    "
},
{
	"uri": "https://ipfscluster.io/documentation/guides/datastore/",
	"title": "Datastore backends",
	"tags": [],
	"description": "",
	"content": " Datastore backends IPFS Cluster supports several datastore backend options. The backend is a key-value store for all the persistent storage needed by a cluster peer. This includes the pinset information and also the CRDT-DAG blockstore etc.\nDatastores are highly configurable, and configuration has an impact on performance, memory footprint and disk footprint. Different datastores will likely behave differently based on whether the underlying medium are SSDs or spinning disks. Depending on constraints, hardware used etc. it may be better to use one or another, but we recommend the newer Pebble or Badger3 and discourage using Badger or LevelDB.\nConfiguration settings for each of the backends can be found in the Configuration reference.\nPebble Pebble is a high performant backend from Cochroachdb, used by default in Cluster:\n Proven to work well on very large pinsets. Best disk-usage compared to the rest. No need to trigger GC cycles for space reclaim. Performance and memory usage seems on par with Badger3, and behaves better than Badger on both counts. Behaves correctly with default settings but we bump them up a bit. 0-delay startup times, even with very large amounts of data. Options support compression (we chose to leave it enabled by default). The Pebble project is officially alive and maintained. Pebble only runs on 64-bit architectures. One key difference with Badger3 is that Pebble stores keys and values together and any lookup for a key will also read the values, while Badger3 can store keys and values separately (i.e. keys only in the index, which can be loaded onto memory when small enough).  Badger3 Badger is based on the v3 series of the library:\n Badger3 has tons of improvements over Badger. Significantly better disk-footprint. GC cycles work better. Still higher footprint than Pebble though. Works well with default settings for large clusters. Reduced memory usage (because can use more conservative settings to start with). Startup times are faster than Badger but still slow for large pinsets. Badger3 works with 32-biut architectures and is the default for those.  Badger Badger is based on the v1.6.2 version of the library:\n Heavily battle tested, it was used for our largest cluster deployments. Bad disk-footprint behaviour: needs regular GC cycles but they don\u0026rsquo;t quite achieve to reduce disk usage as much as it would be possible. Baseline performance and memory usage. Needs configuration tuning for large clusters (see deployment guide). Startup times become very slow once the pinset grows (can take minutes to open the datastore). Legacy and unmaintained.  LevelDB LevelDB is based on a LevelDB implementation as used by Kubo:\n This backend is known to misbehave with very large number of items. Lightweight, needs little tuning and its ok for smaller clusters. Disk-footprint is better than Badger. It was added as an alternative to Badger, but usage not recommended given Pebble and Badger3 options. Do not use.  "
},
{
	"uri": "https://ipfscluster.io/documentation/deployment/setup/",
	"title": "Download and setup",
	"tags": [],
	"description": "",
	"content": " Download and installation In order to run an IPFS Cluster peer and perform actions on the Cluster, you will need to obtain the ipfs-cluster-service and ipfs-cluster-ctl binaries. The former runs the Cluster peer. The latter allows to interact with it:\n Visit the download page for instructions on the different ways to obtain ipfs-cluster-service and ipfs-cluster-ctl. Place the binaries in a place where they can be run unattended by an ipfs system user (usually /usr/local/bin). IPFS Cluster should be installed and run along ipfs (go-ipfs). Consider configuring your systems to start ipfs and ipfs-cluster-service automatically (beware to check that you need to ensure your cluster is fully operational and peers discover each other beforehand). Some sample Systemd service files are available here: ipfs-cluster-service, ipfs.  Initialization To create and generate a default configuration file, a unique identity for your peer and an empty peerstore file, run:\n$ ipfs-cluster-service init  This assumes that the ipfs-cluster-service command is installed in one of the folders in your $PATH.\nIf all went well, after running this command there will be three different files in $HOME/.ipfs-cluster:\n service.json contains a default peer configuration. Usually, all peers in a Cluster should have exactly the same configuration. identity.json contains the peer private key and ID. These are unique to each Cluster peer. peerstore is an empty file used to store the peer addresses of other peers so that this peer knows where to contact them.  The peer will be initialized using the default crdt \u0026ldquo;consensus\u0026rdquo;. This is the recommended option for most setups. See Consensus Components for more information.\nThe default and recommended datastore backend (optional --datastore flag) is pebble. See the Datastore backends guide for more information.\nThe new service.json file generated by ipfs-cluster-service init will have a randomly generated secret value in the cluster section. For a Cluster to work, this value should be the same in all cluster peers. This is usually a source of pitfalls since initializing default configurations everywhere results in different random secrets.\nIf present, the CLUSTER_SECRET environment value is used when running ipfs-cluster-service init to set the cluster secret value. Remote configuration ipfs-cluster-service can be initialized to use a remote configuration file accessible on an HTTP(s) location which is read to obtain the running configuration every time the peer is launched. This is useful to initialize all peers with the same configuration and provide seamless upgrades to it.\nA good trick is to use IPFS to store the actual configuration and, for example, call init with a gateway url as follows:\n$ ipfs-cluster-service init http://localhost:8080/ipns/config.mydomain.com  (a DNSLink TXT record needs to be configured for the example above to work. A regular URL can be used too).\nDo not host configurations publicly unless it is OK to expose the Cluster secret. This is only OK in crdt-based clusters which have configured trusted_peers to other than *. Trusted peers The crdt section of the service.json file includes a single * value for the trusted_peers array. By default, peers running on crdt-mode trusts all other peers. In raft mode, all peers trust all other peers and this option does not exist.\nRead more about trusted peers in the Security and Ports guide.\nThe peerstore file The peerstore file will be maintained by the running Cluster peer and will be used to store known-peer addresses. However, you can also pre-fill this file (one line per multiaddress) to help this peer connect to others during its first start. Here is an example:\n/dns4/cluster1.domain/tcp/9096/ipfs/QmcQ5XvrSQ4DouNkQyQtEoLczbMr6D9bSenGy6WQUCQUBt /dns4/cluster2.domain/tcp/9096/ipfs/QmdFBMf9HMDH3eCWrc1U11YCPenC3Uvy9mZQ2BedTyKTDf /ip4/192.168.1.10/tcp/9096/ipfs/QmSGCzHkz8gC9fNndMtaCZdf9RFtwtbTEEsGo4zkVfcykD  Alternatively, you can also use the peer_addresses configuration value to provide addresses for other peers.\nPorts By default, Cluster uses:\n 9096/tcp as the cluster swarm endpoint which should be open and diallable by other cluster peers. 9094/tcp as the HTTP API endpoint, when enabled 9095/tcp as the Proxy API endpoint, when enabled 9097/tcp as the IPFS Pinning API endpoint, when enabled 8888/tcp as the Prometheus metrics endpoint, when enabled. 6831/tcp as the Jaeger agent endpoint for traces, when enabled.  A full description of the ports and endpoints is available in the Security guide.\nSettings for production The default IPFS and Cluster settings are conservative and work for simple setups out of the box. There are however, a number of options that can be optimized with regards to:\n Large pinsets Large number of peers Networks with very high or lower latencies  Additionally to the settings mentioned here, the configuration reference contains detailed information for every configuration section, with extended descriptions of what each value means.\nIPFS Configuration IPFS daemons can be optimized for production. The options are documented in the official repository:\n config documentation Experimental features documentation  Server profile for cloud deployments Initialize ipfs using the server profile: ipfs init --profile=server or ipfs config profile apply server if the configuration already exists.\nPay attention to AddrFilters and NoAnnounce options. They should be pre-filled to sensible values with the server configuration profile, but depending on the type of network you are running on, you may want to modify them.\nIPFS Datastore settings Unlike a previous recommendation, we have found that the flatfs datastore performs better than badger or very large repositories on modern hardware, and gives less headaches (i.e. does not need several minutes to be ready).\nsync should be set to false in the configuration (big performance impact otherwise), and the backing Filesystem should probably be XFS or ZFS (faster when working with folder with large number of files in them). IPFS puts huge pressure on disk by performing random reads, specially when providing popular content.\nFlatfs can be improved by setting the Sharding function to /repo/flatfs/shard/v1/next-to-last/3 (next-to-last/2 is the default). This should only be done for multi-terabyte repositories.\nUpdating the sharding function can be done by initializing from a configuration template or by setting it in config and the datastore_spec and removing the blocks/ folder. It should be done during the first setup of the IPFS node, although small datastores can be converted using ipfs-ds-convert.\nIncreasing Datastore.BloomFilterSize should be considered in most cases, according to the expected IPFS repository size: 1048576 (1MB) is a good value to start (more info here).\nDo not forget to set Datastore.StorageMax to a value according to the disk you want to dedicate for the ipfs repo. This will affect how cluster calculates how much free space there is in every peer.\nConnection manager settings Increase the Swarm.ConnMgr.HighWater (maximum number of connections) and reduce GracePeriod to 20s. It can be as high as your machine would take (10000 is a good value for large machines). Adjust Swarm.ConnMgr.LowWater to about a 25% of the HighWater value.\nExperimental DHT providing Large datastore will have a lot to provide, so enabling AcceleratedDHTClient is a good thing.\nFile descriptor limit The IPFS_FD_MAX environment variable controls the FD ulimit value that go-ipfs sets for itself. Depending on your Highwater value, you may want to increase it to 8192 or more.\nGarbage collection We recommend to keep automatic garbage collection on IPFS disabled when using IPFS Cluster to add content as the GC process may delete content as it is being added. Alternatively, it is possible to add content directly to IPFS (this will lock the GC process in the mean time), and only use the Cluster to pin it once added.\nBitswap optimizations It is also very important to adjust Bitswap internal configuration when nodes have lots of traffic. Multiplying the defaults by 100 is not unhread of in big machines. However, you would have to find the right balance, as this will make IPFS consume much more memory when it is busy bitswapping. Example for machine with 128GB of RAM:\n \u0026quot;Internal\u0026quot;: { \u0026quot;Bitswap\u0026quot;: { \u0026quot;EngineBlockstoreWorkerCount\u0026quot;: 2500, \u0026quot;EngineTaskWorkerCount\u0026quot;: 500, \u0026quot;MaxOutstandingBytesPerPeer\u0026quot;: 1048576, \u0026quot;TaskWorkerCount\u0026quot;: 500 } }  Multiply by 2 these values if your machines can handle it.\nPeerings The peering can be used to ensure IPFS peers in the cluster stay always connected.\nIPFS Cluster configuration First, it is important to ensure that the ipfs-cluster-service daemon can operate with ample \u0026ldquo;file descriptor count\u0026rdquo; limits. This can be done by adding LimitNOFILE=infinity to systemd service unit files (in the [Service] section), or by ensuring ulimits are correctly set. The number of file descriptors used by the daemon is proportional to:\n The number of Pebble\u0026rsquo;s open files and write-ahead log files (Pebble\u0026rsquo;s configuration) The number of items being pinned in parallel (pintracker configuration) The number of ongoing add requests.  Additionally, the service.json configuration file contains a few options which should be tweaked according to your environment, capacity and requirements.\ncluster section Cluster regularly performs full-pinset sweeps to make sure, for example, that pins in error state are retried, or that expired pins are unpinned. On very large pinsets, these operations are costly. Thus we recommend increasing the following intervals:\n cluster.pin_recover_interval: In order to trigger pin recovery operations, this will trigger a ipfs pin ls --type=recursive calls and list all items in the cluster pinset. For clusters with multimillion pinsets, this should be set to at least 2+ hours, or as long as you can tolerate. cluster.state_sync_interval: This specifies how often the cluster will check for expired pins and trigger unpin operations, which requires visiting every item in the pinset. For clusters with multimillion pinsets, this should be set to at least 2+hours, or as long as you can tolerate.  For large clusters, we recommend leaving repinning disabled (cluster.disable_repinning). As it is implemented now, repinning can trigger re-allocation of content when no more heartbeats (pings) are received from a node. On multi-million pinsets, this can be an expensive operation (specially if the node can recover afterwards). When repinning is enabled, cluster.monitor_ping_interval and monitor.*.check_interval dictacte how long cluster takes to realize a peer is not responding (and potentially trigger re-pins). If you enable repinning, we recommend using replication_factor_min and replication_factor_max to allow some leeway: i.e. a 2\u0026frasl;3 will allow one peer to be down without re-allocating the content assigned to it somewhere else.\nconsensus section We recommend using crdt consensus only, for raft is mostly unmaintained and won\u0026rsquo;t scale.\nReducing the crdt.rebroadcast_interval (default 1m) to a few seconds should make new peers start downloading the state faster, and badly connected peers should have more options to receive bits of information, at the expense of increased pubsub chatter in the network. It can be reduced to 10 seconds for clusters under 10 peers, but in general we recommend to leave it at 1 minute.\nMost importantly, the crdt.batching configuration allows to increase CRDT throughput by orders of magnitude by batching multiple pin updates in a single delta (at the cost of delays). For example, you can let clusters only commit new pins every minute, or when there are 500 in a batch as follows:\n \u0026quot;batching\u0026quot;: { \u0026quot;max_batch_size\u0026quot;: 500, \u0026quot;max_batch_age\u0026quot;: \u0026quot;1m\u0026quot;, \u0026quot;max_queue_size\u0026quot;: 50000 },  You can edit the crdt.cluster_name, as long as it is the same for all peers.\npin_tracker section The stateless pin tracker handles two pinning queues: a priority one and a \u0026ldquo;normal\u0026rdquo; one. When processing the queues, pins queued in the priority queue always have preference. All new pins go to the priority queue and stay there until they become old or have been retried too many times:\n \u0026quot;pin_tracker\u0026quot;: { \u0026quot;stateless\u0026quot;: { \u0026quot;max_pin_queue_size\u0026quot;: 200000000, \u0026quot;concurrent_pins\u0026quot;: 10, \u0026quot;priority_pin_max_age\u0026quot; : \u0026quot;24h\u0026quot;, \u0026quot;priority_pin_max_retries\u0026quot; : 3 } },  The right value for concurrent_pins depends on the size of the pins and the performance of IPFS to both fetch and write them to disk. We have observed that values between 10-20 tend to work better than larger ones (which may cause too much contention).\nipfs_connector section Pin requests performed by cluster time out based on the last blocked fetched by IPFS (not on the total length of the pin requests). Therefore the pin_timeout setting can be set very low: 20 seconds will ask cluster to give up a pin if no block can be fetched for 20 seconds. Lower pin timeouts let cluster churn through pinning queues faster. Pin errors will be retried later.\nThe ipfs_request_timeout is set to a conservative 5m value by default. However, on pinsets with millions of items this might be too low and increasing it is necessary.\napi section Adjust the api.restapi/pinsvcapi/ipfsproxy network timeouts depending on your API usage. This may protect against misuse of the API or DDoS attacks. Note that there are usually client-side timeouts that can be modified too if you control the clients.\nEach of the APIs can be disabled by just removing their configuration from the api section.\ninformer section On big clusters with many pins/peers/storage size, you can increase the metric_ttl for all informers to 5 or 10 minutes, as there is no need to have extremely fresh freespace metrics etc. This means that peers will be producing a stable set of allocations for new pins as long as the metrics are not refreshed, which allows the rest of the peers to \u0026ldquo;rest\u0026rdquo;, giving some space to deal with other tasks like pin recovery.\nIf you want to geo-distribute your pins, setup the tags allocator with a region tag and the right value for every peer.\nThe pinqueue allocator should be set with a high weight_bucket_size adjusted to how big it is acceptable for a pinning queue to grow before we start pinning somewhere else. The default weight_bucket_size is 100k, which is a very high number. This means that all peers under 100k items in the queue are considered equal with regards to this metric, allowing the freespace metric to be the deciding factor for pinning allocations (when things are so configured in the balanced allocator section).\nallocator section The balanced allocator can help distribute pins and load effectively. We recommend to set allocate_by to something like:\n[ \u0026quot;tag:region\u0026quot;, \u0026quot;pinqueue\u0026quot;, \u0026quot;freespace\u0026quot; ]  Given a replication factor of 3, and a cluster with peers in 3 regions correctly configured, this will place each pin replica in 1 of the regions, choosing the peer with most free-space in that region among those with the lowest queue weight (\u0026ldquo;pin queue size / weight_bucket_size\u0026rdquo;). You can also add additional tag metrics (i.e [ \u0026quot;tag:region\u0026quot;, \u0026quot;tag:availability_zone\u0026quot;, ...]). See the pinning guide for more information on how pinning process happens.\ndatastore section Our default configuration for pebble (our default backend), should be tuned to support very large cluster deployments with very large number of pins. We have modified some of the Pebble defaults that we consider a bit conservative (even though they probably work just fine for 95% of cases). Our choices with some additional comments are documented here. All Pebble options are documented at https://pkg.go.dev/github.com/cockroachdb/pebble#Options.\nBadger3 is an alternative, particularly for platforms not supporting Pebble. The defaults should be mostly fine too, but it is worth tuning and understanding our defaults and the meaning of the options (https://pkg.go.dev/github.com/dgraph-io/badger/v3#Options).\nWe heavily discourage using badger and leveldb for anything at this point, but we used to recommend the following Bager settings for a large machine with 128GB of RAM:\n\u0026quot;badger\u0026quot;: { \u0026quot;gc_discard_ratio\u0026quot;: 0.2, \u0026quot;gc_interval\u0026quot;: \u0026quot;15m0s\u0026quot;, \u0026quot;gc_sleep\u0026quot;: \u0026quot;10s\u0026quot;, \u0026quot;badger_options\u0026quot;: { \u0026quot;dir\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;value_dir\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;sync_writes\u0026quot;: false, \u0026quot;table_loading_mode\u0026quot;: 2, \u0026quot;value_log_loading_mode\u0026quot;: 0, \u0026quot;num_versions_to_keep\u0026quot;: 1, \u0026quot;max_table_size\u0026quot;: 268435456, \u0026quot;level_size_multiplier\u0026quot;: 10, \u0026quot;max_levels\u0026quot;: 7, \u0026quot;value_threshold\u0026quot;: 512, \u0026quot;num_memtables\u0026quot;: 10, \u0026quot;num_level_zero_tables\u0026quot;: 10, \u0026quot;num_level_zero_tables_stall\u0026quot;: 20, \u0026quot;level_one_size\u0026quot;: 268435456, \u0026quot;value_log_file_size\u0026quot;: 1073741823, \u0026quot;value_log_max_entries\u0026quot;: 1000000, \u0026quot;num_compactors\u0026quot;: 2, \u0026quot;compact_l_0_on_close\u0026quot;: true, \u0026quot;read_only\u0026quot;: false, \u0026quot;truncate\u0026quot;: false } }  "
},
{
	"uri": "https://ipfscluster.io/documentation/reference/proxy/",
	"title": "IPFS Proxy",
	"tags": [],
	"description": "",
	"content": " IPFS Proxy The IPFS Proxy is an endpoint which presents the IPFS HTTP API in the following way:\n Some requests are intercepted and trigger cluster operations All non-intercepted requests are forwarded to the IPFS daemon attached to the cluster peer  This endpoint is enabled by default, and listens by default on /ip4/127.0.0.1/tcp/9095 and is provided by the ipfshttp connector component. It can be disabled by removing its section from the service.json configuration file.\nThe requests that are intercepted are the following:\n /add: the proxy adds the content to the local ipfs daemon and pins the resulting hash[es] in cluster. /pin/add: the proxy pins the given CID in cluster. /pin/update: the proxy updates the given pin to a new one in cluster. /pin/rm: the proxy unpins the given CID from cluster. /pin/ls: the proxy lists the pinned items in cluster. /repo/stat: the proxy responds with aggregated /repo/stat from all connected IPFS daemons. /repo/gc: the proxy performs garbage collection on all IPFS daemons and responds with collected CIDs. /block/put: the proxy uploads the blocks to the local IPFS daemon but if ?pin=true is set, then it performs a cluster recursive pin for each of them. /dag/put: the proxy uploads the DAG nodes to the local IPFS daemon but if ?pin=true is set, then it performs a cluster recursive pin for each of them.  Responses from the proxy mimic the IPFS daemon responses, thus allowing to drop-in this endpoint in places where the IPFS API was used before. For example, you can use the go-ipfs CLI as follows:\n ipfs --api /ip4/127.0.0.1/tcp/9095 pin add \u0026lt;cid\u0026gt; ipfs --api /ip4/127.0.0.1/tcp/9095 add myfile.txt ipfs --api /ip4/127.0.0.1/tcp/9095 pin rm \u0026lt;cid\u0026gt; ipfs --api /ip4/127.0.0.1/tcp/9095 pin ls  The IPFS Proxy endpoint can be used with the IPFS companion extension. The responses would come from cluster, not from go-ipfs. The intercepted endpoints aim to mimic the format, headers and response codes from IPFS. If you have custom headers configured in IPFS, you will need to add their names them to the ipfsproxy.extract_headers_extra configuration option.\n"
},
{
	"uri": "https://ipfscluster.io/documentation/collaborative/joining/",
	"title": "Joining a collaborative cluster",
	"tags": [],
	"description": "",
	"content": " Joining a collaborative cluster Collaborative clusters allow people to join forces to backup and distribute interesting content on the IPFS-network.\nThese clusters are formed by a group of trusted peers, which add and modify the list of items in the cluster, and a group of followers, which subscribe to the cluster and pin things accordingly.\nJoining a cluster using ipfs-cluster-follow You should be able to join using a single oneliner:\nipfs-cluster-follow \u0026lt;clusterName\u0026gt; run --init \u0026lt;template-configuration-url\u0026gt;  Always run on the same or on a newer IPFS Cluster version than the version used by the trusted peers in the cluster. If you don\u0026rsquo;t have or don\u0026rsquo;t know a URL for your configuration, you can ask for it to whoever runs the trusted peers for that cluster. If you know the right pieces of information (see below), you can build your own configuration, add it to IPFS, and use the gateway URL for it http://127.0.0.1:8080/ipfs/Qmhash.... You can also run ipfs-cluster-follow \u0026lt;clusterName\u0026gt; init with a random url and then replace the generated service.json with your own.\nYou can customize and overwrite any configuration options set in the [remote] template using environment variables as explained in the configuration reference. Finding collaborative clusters to join Visit collab.ipfscluster.io for a list of collaborative clusters that you can join.\nManually configuring a follower peer The main requirement to join a collaborative cluster is to know the right pieces of information about it:\n The list of trusted_peers The multiaddresses on which to contact at least one trusted peer The value of the cluster secret The crdt/cluster_name (if other than default)  Placing that information in a service.json configuration file, as generated by ipfs-cluster-service init, should be enough to create a peer that joins an existing cluster either using ipfs-cluster-service or ipfs-cluster-follow.\nFor more information about ipfs-cluster-follow, check the reference.\n"
},
{
	"uri": "https://ipfscluster.io/_release/",
	"title": "Release process",
	"tags": [],
	"description": "",
	"content": " Release process Tasks that need to happen for a release:\nPreparation  Open release issue with the Release tag and mention all issues and tickets that will go on that release (https://github.com/ipfs-cluster/ipfs-cluster/issues/620). It helps backtrack when things happening from the issues. Write the changelog entry for the release (copy from previous) and commit to branch:  Summary of what\u0026rsquo;s happening in the release List of features, bugs Configuration changes and upgrades notices (note comment at the bottom of the file about how to write @issuelinks and replace them with sed)  PR to the website with all the necessary documentation changes  Special attention to documentation changes (/configuration section) Special attention to behaviour changes that are described somewhere in the docs Add an entry to news about the new release. Explain it for humans. Thank main contributors. Updaqte the Updated tp version x.x.x string   Make a release candidate  Merge all code that needs to be merged for the release. Double check on master:  It builds. gx deps dupes is empty. All the dependency tree is pinned (gx2cluster). ipfs-cluster-service starts with a configuration file from the last release just fine. Depending on what has changed in the release, it might be useful to run a test deployment of cluster and make sure things look fine. Make sure to start a peer at least and check that there is no weird log messages on boot.  Fully clean your repository workspace: there should not be anything in it that is not part of the repository (untracked files). Call the /release.sh 0.x.x-rc1 on master:  IPFS daemon must be started before. git must be able to sign commits. gx must be installed in the system. It seds and replaces the version numbers in the code, signs and commits the new Release, tags the new Release with an annotated signed tag which includes the full changelog since last stable release, and gx-releases the whole thing (which again, signs the gx-release commit). Triple-check all is well. If not, remove tags, reset to origin/master --hard. git push origin master --tags NO TURNING BACK FROM THIS POINT. Pin the new ipfs-cluster gx hash.  Publish the new release to dist.ipfs.io:  Keep your cluster repository in master and don\u0026rsquo;t touch anything in it. ipfs daemon must be running. Check out ipfs/distributions. Make a branch. Update the versions file and add the new RC (MUST HAVE A NEW LINE AT THE END). make clean -\u0026gt; make publish. This will take a while. ipfs object diff /ipns/dist.ipfs.io (tail -n1 versions ) should only show changes in cluster folders. Commit and push the branch, add the ipfs object diff output to the PR, wait for travis to be green and merge. Pin the new dist hash. Update DNSSimple entry for dist.ipfs.io to the new hash.  Deploy storage cluster with the new release:  ipfs-cluster-infra/ansible repo, update the configurations and run ansible. If configurations changed, you may need to update ansible-ipfs-cluster submodule.  Update the pinbot to the new cluster version:  ipfs/pinbot-irc and gx update ipfs-cluster github.com/ipfs-cluster/ipfs-cluster. Any client API changes might need to fix the pinbot code. Commit and push. Deploy the pinbot by updated the ref commit in ipfs-cluster-infra/ansible.  Announce the RC to the world via twitter.  Testing  This is the triple check that things should be working as they should. Most of this testing should have happened locally before making the RC. Storage cluster started just fine. Pinbot works with the new cluster and in the new pinbot version. Test that things that were introduced are actually doing what they should be doing (again). Test that bugfixes are actually fixed (again). Depending on the introduced changes, let the RC rest for a couple of days.  Make the final release  Set the right date in the changelog branch. Ensure all issue links are there. Merge. Close Release issue. Set the right date in the news post in the website. If there have been changes since the RC, double check the same things on master as with RC. Fully clean your repo space as with the RC. Call /release.sh 0.x.x. Same subtasks as with the RC. Publish the new release to dist.ipfs.io. Same procedure except:  Need to update the current files to the new version. THEY NEED TO HAVE A NEW LINE AT THE END.  Merge the documentation for the website.  Check that Jenkins actually built the website and published it. Pin the new hash.  Deploy storage cluster with the stable. Update the pinbots to the stable. Announce to the world.  "
},
{
	"uri": "https://ipfscluster.io/documentation/deployment/bootstrap/",
	"title": "Bootstrapping the Cluster",
	"tags": [],
	"description": "",
	"content": " Bootstrapping the Cluster This section explains how to start a Cluster for the first time depending on the consensus choice (crdt or raft) made during initialization.\nThe first start of the Cluster is the most critical step during the lifetime. We must ensure that peers are able to contact each other (connectivity) and discard common configuration errors (like using different value for secret).\nStarting a cluster peer is as easy as running:\nipfs-cluster-service daemon  BUT, unlike the IPFS daemon, which by default connects to the public IPFS network and can discover other peers in it by first connecting to a well known list of available bootstrappers, a Cluster peer runs on a private network and does not have any public peer to bootstrap to.\nMake sure the `ipfs` daemon is running before starting the Cluster. Although this is not an strict requirement, it avoids a few error messages. Thus, when starting IPFS Cluster peers for the first time, it is important to provide information so that they can discover the other peers and join the Cluster. Once a peer has successfully started once, it can be subsequently re-started with the command above. During shutdown, each peer\u0026rsquo;s peerstore file will be updated to remember known addresses for other peers.\nAs we will see below, the first start has slightly different requirements depending on whether you will be running a crdt-based or a raft-based Cluster. You can read more about the differences between the two in the CRDT vs Raft table.\nAll peers in a Cluster must run in the same mode, either CRDT or Raft. Bootstrapping the Cluster in CRDT mode This is the easiest option to start a cluster because the only requirement a crdt-based peer has to become part of a Cluster is to contact at least one other peer. This can be achieved in several ways:\n Adding addresses for other peers to the peer_addresses array in the configuration. Pre-filling the peerstore file with addresses for other peers (as we saw in the previous section). Running with the --bootstrap \u0026lt;peer-multiaddress1,peer-multiaddress2\u0026gt; flag. Note that using this flag will automatically trust the given peers. For more information about trust, read the CRDT section. In local networks with mDNS discovery support, peers will autodiscover each other and no additional measures are necessary.  Example 1. Starting the first peer in a CRDT-based Cluster:\nipfs-cluster-service daemon  Example 2. Starting more peers in a CRDT-based cluster by customizing the peerstore. The given multiaddress corresponds to the first peer:\necho \u0026quot;/dns4/cluster1.domain/tcp/9096/ipfs/QmcQ5XvrSQ4DouNkQyQtEoLczbMr6D9bSenGy6WQUCQUBt\u0026quot; \u0026gt;\u0026gt; ~/.ipfs-cluster/peerstore ipfs-cluster-service daemon  Example 3. Starting more peers in a CRDT-based cluster using the --bootstrap flag. The given multiaddress corresponds to the first peer:\nipfs-cluster-service daemon --bootstrap /dns4/cluster1.domain/tcp/9096/ipfs/QmcQ5XvrSQ4DouNkQyQtEoLczbMr6D9bSenGy6WQUCQUBt  Bootstrapping the Cluster in Raft mode In Raft Clusters, the first start of a peer must not only contact a different peer, but also complete the task of becoming a member of the Raft Cluster. Therefore the first start of a peer must always use the --bootstrap flag:\nExample 1. Starting the first peer in a Raft-based Cluster:\nipfs-cluster-service daemon  Example 2. Starting more peers in a Raft-based cluster. The given multiaddress corresponds to the first peer:\nipfs-cluster-service daemon --bootstrap /dns4/cluster1.domain/tcp/9096/ipfs/QmcQ5XvrSQ4DouNkQyQtEoLczbMr6D9bSenGy6WQUCQUBt  Example 3. Subsequent starts when the peer already successfully joined a Raft cluster before:\nipfs-cluster-service daemon  Verifying a successful bootstrap After starting your Cluster peers (especially the first time you are doing so), you should check that things are working correctly:\nCheck for errors in the logs. A successful peer start will print the \u0026ldquo;READY\u0026rdquo; message:\nINFO cluster: ** IPFS Cluster is READY **  Run ipfs-cluster-ctl id to verify the details of your local Cluster peer. You should be able to see information for the Cluster peer and for the IPFS daemon it is connected to:\n$ ipfs-cluster-ctl id QmYY1ggjoew5eFrvkenTR3F4uWqtkBkmgfJk8g9Qqcwy51 | peername | Sees 3 other peers \u0026gt; Addresses: - /ip4/127.0.0.1/tcp/9096/ipfs/QmYY1ggjoew5eFrvkenTR3F4uWqtkBkmgfJk8g9Qqcwy51 - /ip4/192.168.1.10/tcp/9096/ipfs/QmYY1ggjoew5eFrvkenTR3F4uWqtkBkmgfJk8g9Qqcwy51 \u0026gt; IPFS: QmPFJcZfhFCmz1rAoew214h9d7Nv4aseqtCg5sm4fMdeYq - /ip4/127.0.0.1/tcp/4001/ipfs/QmPFJcZfhFCmz1rAoew214h9d7Nv4aseqtCg5sm4fMdeYq - /ip4/127.0.0.1/tcp/4002/ws/ipfs/QmPFJcZfhFCmz1rAoew214h9d7Nv4aseqtCg5sm4fMdeYq - /ip6/::1/tcp/4001/ipfs/QmPFJcZfhFCmz1rAoew214h9d7Nv4aseqtCg5sm4fMdeYq - /ip6/::1/tcp/4002/ws/ipfs/QmPFJcZfhFCmz1rAoew214h9d7Nv4aseqtCg5sm4fMdeYq   CRDT Cluster peers may take a few minutes to discover additional peers in the Cluster (depending on how they were bootstrapped), even after the READY message. Raft Cluster peers will fail to start after a few seconds if they have not successfully joined or re-joined the Raft Cluster. The READY message indicates that the peer is fine, even though it may show errors when contacting other peers that are down.  Common issues Here are some things that usually go wrong during the first boot. For more instructions on how to debug a cluster peer, see the Troubleshooting guide.\nDifferent secret among Cluster peers Using different Cluster secrets makes Cluster peers unable to communicate while causing libp2p to throw unobvious errors on the logs:\ndial attempt failed: incoming message was too large  No connectivity between peers The following error messages indicate connectivity issues. Make sure that the necessary ports are open and that connectivity can be established between peers:\ndial attempt failed: context deadline exceeded dial backoff dial attempt failed: connection refused  "
},
{
	"uri": "https://ipfscluster.io/documentation/guides/",
	"title": "Guides",
	"tags": [],
	"description": "",
	"content": " IPFS Cluster Guides The following section contains in-depth guides about multiple topics relevant to IPFS Cluster usage:\n\n Adding and pinning   Consensus components   Datastore backends   Security and ports   Data, backups and recovery   Peerset management   Cluster pubsub metrics   Upgrades   Troubleshooting   Monitoring and tracing   Deployment on Kubernetes   "
},
{
	"uri": "https://ipfscluster.io/documentation/guides/security/",
	"title": "Security and ports",
	"tags": [],
	"description": "",
	"content": " Security and ports This section explores some security considerations when running IPFS Cluster.\nThere are four types of endpoints in IPFS Cluster to be taken into account when protecting access to the system. Exposing an unprotected endpoint might give anyone control of the cluster. Cluster configuration uses sensible defaults.\n The cluster secret The trusted_peers in CRDT mode Ports and endpoints overview  The cluster secret The 32-byte hex-encoded secret in the service.json file acts as libp2p network protector. This provides additional encryption for all communications between peers (libp2p) using a pre-shared key.\nThis makes it impossible to communicate with a peer\u0026rsquo;s swarm endpoint (see below) and thus, to send RPC commands to that peer, without knowing the secret in advance.\nThe secret is a security requirement for raft-based clusters which do not enforce any RPC authorization policy. CRDT-based clusters can run with an empty secret as long as trusted_peers is correctly set: only the peers in trusted_peers can modify the pinset and perform actions.\nHowever, we recommend to set the secret in all cases, as it provides network isolation: clusters running without a secret may discover and connect to the main IPFS network, which is mostly useless for the cluster peers (and for the IPFS network).\nThe trusted_peers in CRDT mode The trusted_peers option in the crdt section of the service.json file provides access control to the peer RPC endpoints and allows modifications of the pinset issued by the peers in that array (as identified by their peer IDs), but only apply to clusters running in crdt-mode.\nTrusted peers can:\n Modify the pinset: indirectly trigger pin/unpin operations Trigger status sync operations (resulting in ipfs pin ls) Add content to the peer (resulting in ipfs block put)  Non-trusted peers only have access to ID and Version endpoints (returning IPFS and Cluster Peer information).\ntrusted_peers can be set to [ \"*\" ] to trust every other peer. Ports overview  Cluster swarm: tcp:9096 is used by the Cluster swarm and protected by the shared secret. It is OK to expose this port (the cluster secret acts as password to interact with it). HTTP API: tcp:9094 can be exposed when enabling SSL and setting up basic authentication IPFS Pinning API endpoint: tcp:9097 similar to the HTTP endpoint above. libp2p-HTTP API: when using an alternative libp2p host, for the api, the libp2p_listen_multiaddress can be exposed when basic authentication is enabled. IPFS API: tcp:5001 is the API of the IPFS daemon and should not be exposed to other than localhost. IPFS Proxy endpoint: tcp:9095 should not be exposed without an authentication mechanism on top (nginx etc\u0026hellip;). By default it provides no authentication nor encryption (similar to IPFS\u0026rsquo;s tcp:5001) Prometheus: when metrics are enabled, they are by default exposed on tcp:8888. Jaeger: when tracing is enabled, it uses tcp:6831 by default to expose the Jaeger service.  Read the sections below to get a more detailed explanation.\nCluster swarm endpoints The endpoint is controlled by the cluster.listen_multiaddress configuration key, defaults to /ip4/0.0.0.0/tcp/9096 and represents the listening address to establish communication with other peers (via Remote RPC calls and consensus protocol).\nAs explained above, the shared secret controls authorization by locking this endpoint so that only the cluster peers holding the secret can communicate. We recommend to never run with an empty secret.\nHTTP API and IPFS Pinning Service API endpoints IPFS Cluster peers provide by default an HTTP API endpoint and an IPFS Pinning Service API endpoint which can be configured with SSL. It also provides a libp2p API endpoint for each, which re-uses either the Cluster libp2p host or a specifically configured libp2p host.\nThese endpoints are controlled by the *.http_listen_multiaddress (default /ip4/127.0.0.1/tcp/9094) and the *.libp2p_listen_multiaddress (if a specific private_key and id are configured in the restapi/pinsvcapi sections).\nNote that when no additional libp2p host is configured and running in Raft mode, the Cluster\u0026rsquo;s peer libp2p host (which listens on 0.0.0.0) is re-used to provide the libp2p API endpoint. As explained, this endpoint is only protected by the cluster secret. When running in CRDT mode, the libp2p endpoint is disabled, as the cluster secret may be shared and would otherwise expose an open endpoint to the world. In order to run a lib2p API endpoint when using CRDT mode, configure an additional, separate libp2p host in the restapi configuration.\nBoth endpoints support Basic Authentication but are unauthenticated by default.\nAccess to these endpoints allow to fully control an IPFS Cluster peer, so they should be adecuately protected when they are opened up to other than localhost. The secure channel provided by the configurable SSL or libp2p endpoint, along with Basic Authentication, allow to safely use these endpoints for remote administration.\nThe restapi/pinsvcapi configurations offer a number of variables to configure CORS headers. By default, Allow-Origin is set to * and Allow-Methods to GET. You should verify that this configuration is suitable for your needs, application and environment.\nThese APIs can be disabled altogether by removing the restapi/pinsvcapi sections from the configuration. If the REST API is disabled, ipfs-cluster-ctl will be unable to talk to the peer.\nIPFS and IPFS Proxy endpoints IPFS Cluster peers communicate with the IPFS daemon (usually running on localhost) via plain, unauthenticated HTTP, using the IPFS HTTP API (by default on /ip4/127.0.0.1/tcp/9095.\nIPFS Cluster peers also provide an unauthenticated HTTP IPFS Proxy endpoint, controlled by the ipfshttp.proxy_listen_multiaddress option which defaults to /ip4/127.0.0.1/tcp/9095.\nAccess to any of these two endpoints imply control of the IPFS daemon and of IPFS Cluster to a certain extent. Thus they run on localhost by default.\nThe IPFS Proxy will attempt to mimic CORS configuration from the IPFS daemon. If your application security depends on CORS, you should configure the IPFS daemon first, and then verify that the responses from hijacked endpoints in the proxy look as expected. OPTIONS requests are always proxied to IPFS.\nThe IPFS Proxy API can be disabled altogether by removing the ipfsproxy section from the configuration.\n"
},
{
	"uri": "https://ipfscluster.io/support/",
	"title": "Support",
	"tags": [],
	"description": "",
	"content": " Support We are happy to support you, answer questions, and help you use IPFS Cluster. You can reach out to the team and the community in several different ways:\n For general questions and usage guidance: we prefer that you open a topic at https://discuss.ipfs.io/. For bugs and feature requests: please open issues in the repository. For quick chat and questions: join the #ipfs-cluster channel on Matrix.  Interactions in all of these venues is governed by the IPFS Community Code of Conduct.\nIf you are looking for things to start with, filter for issues with easy and ready labels.\nIn general, anything marked with help wanted is ready to be taken on by external contributors.\nPlease let us know when you are going to work on something, or more clarifications are needed, so we can help you out!\nGetting oriented To check what\u0026rsquo;s going on in the project, check:\n the Changelog the News the Roadmap the upcoming release issues  Contribution Guidelines IPFS Cluster adopts the existing guidelines in the IPFS community:\n The Go contribution guidelines The IPFS community contributing notes The IPFS Community Code of Conduct  Code contribution guidelines In practice, these are our soft standards:\n IPFS Cluster uses the Apache2/MIT dual license. All contributions are via Pull Request, which needs a Code Review approval from one of the project collaborators. Tests must pass (although sometimes it takes several runs) Code coverage must be stable or increase We prefer meaningful branch names: feat/999-adding-this, fix/392-error-doing-this\u0026hellip; preferably with an issue number in them as well. We prefer commit messages which reference an issue fix #999: ...  These are just guidelines. We are friendly people and are happy to help :)\nDo you use IPFS Cluster? Let us know about your setup by contributing to the IPFS Cluster user registry.\n"
},
{
	"uri": "https://ipfscluster.io/documentation/reference/ctl/",
	"title": "ipfs-cluster-ctl",
	"tags": [],
	"description": "",
	"content": " ipfs-cluster-ctl The ipfs-cluster-ctl command line application is a user-friendly REST API client for IPFS Cluster. It allows to perform all the operations supported by a Cluster peer:\n Pinning Unpinning Adding Listing items in the pinset Checking the status of pins Listing cluster peers Removing peers  Usage Usage information can be obtained by running:\n$ ipfs-cluster-ctl --help  You can also obtain command-specific help with ipfs-cluster-ctl help [cmd]. The (--host) can be used to talk to any remote cluster peer (localhost is used by default). In summary, it works as follows:\n$ ipfs-cluster-ctl id # show cluster peer and ipfs daemon information $ ipfs-cluster-ctl peers ls # list cluster peers $ ipfs-cluster-ctl peers rm \u0026lt;peerid\u0026gt; # remove a cluster peer $ ipfs-cluster-ctl add myfile.txt http://domain.com/file.txt # adds content to the cluster $ ipfs-cluster-ctl pin add Qma4Lid2T1F68E3Xa3CpE6vVJDLwxXLD8RfiB9g1Tmqp58 # pins a CID in the cluster $ ipfs-cluster-ctl pin rm Qma4Lid2T1F68E3Xa3CpE6vVJDLwxXLD8RfiB9g1Tmqp58 # unpins a CID from the cluster $ ipfs-cluster-ctl pin ls [CID] # list tracked CIDs (shared state) $ ipfs-cluster-ctl status [CID] # list current status of tracked CIDs (local state) $ ipfs-cluster-ctl sync Qma4Lid2T1F68E3Xa3CpE6vVJDLwxXLD8RfiB9g1Tmqp58 # re-sync seen status against status reported by the IPFS daemon $ ipfs-cluster-ctl recover Qma4Lid2T1F68E3Xa3CpE6vVJDLwxXLD8RfiB9g1Tmqp58 # attempt to re-pin/unpin CIDs in error state  Authentication The IPFS Cluster API can be configured with Basic Authentication support.\nipfs-cluster-ctl --basic-auth \u0026lt;username:password\u0026gt; will use the given credentials to perform the request.\nNote that unless --force-http is passed, using basic-auth is only supported on HTTPS requests or using the libp2p API endpoint (which uses an encrypted channel).\nUsing the libp2p API endpoint Since 0.3.5, IPFS Cluster provides a libp2p endpoint for the HTTP API which provides channel security without the need to configure SSL certificates, by either re-using the peer\u0026rsquo;s libp2p host or by setting up a new one with the given parameters in the API configuration.\nIn order to have ipfs-cluster-ctl use a libp2p endpoint, provide the --host flag as follows:\nipfs-cluster-ctl --host /ip4/\u0026lt;ip\u0026gt;/ipfs/\u0026lt;peerID\u0026gt; ...\nor\nipfs-cluster-ctl --host /dnsaddr/mydomain.com ... (setting a _dnsaddr TXT dnsaddr=peer_multiaddress field in your dns).\nIf the libp2p peer you\u0026rsquo;re contacting is using a cluster secret (a private networks key), you will also need to provide --secret \u0026lt;32 byte-hex-encoded key\u0026gt; to the command.\nWe recommend that you alias the ipfs-cluster-ctl command in your shell to something shorter and with the right global options.\nExit codes ipfs-cluster-ctl will exit with:\n 0: the request/operation succeeded. The output contains the response data. 1: argument error, network error or any other error which prevented the application to perform a request and obtain a response from the IPFS Cluster API. In such case, the output contains the contents of the error and the HTTP code 0. 2: IPFS Cluster peer error. The request was performed correctly but the response is an error (HTTP status 4xx or 5xx). In such case, the output contains the contents of the error and the HTTP code associated to it.  Debugging ipfs-cluster-ctl takes a --debug flag which allows to inspect request paths and raw response bodies.\nDownload To download ipfs-cluster-ctl check the downloads page.\n"
},
{
	"uri": "https://ipfscluster.io/documentation/guides/backups/",
	"title": "Data, backups and recovery",
	"tags": [],
	"description": "",
	"content": " Data, backups, and recovery The configurations and data persisted by a running IPFS Cluster peer (with ipfs-cluster-service) is, by default, in the $HOME/.ipfs-cluster/ folder. A Cluster peer persists several types of information on disk:\n The list of known peer addresses for future use. Is stored in the peerstore file during shutdown. The cluster pinset (the list of objects that are pinned in the cluster along with all the options associated to them (like the name, the allocations or the replication factor) are stored depending on the consensus component chosen:  crdt stores everything in the chose key-value datastore backend (pebble, badger3, badger, leveldb folders). raft stores the-append-only log making up the pinset, along with the list of cluster peers in a BoltDB store frequently snapshotted. All is saved in the raft folder.  service.json and identity.json are also persistent data, but normally they are not modified.  Offline state: export and import Since the pinset information is persistend on disk, it can be exported from an offline peer with:\nipfs-cluster-service state export  This will produce a list of json objects that represent the current pinset (very similar to ipfs-cluster-ctl --enc=json pin ls on peers that are online). The resulting file can be re-imported with:\nipfs-cluster-service state import  Always re-import using the same ipfs-cluster-service version that you exported with. Consider https://github.com/ipfs-cluster/ipfs-cluster/issues/1547 when importing a state to multiple cluster peers. Note that the state dump just contains the pinset. It does not include any bookkeeping information, Raft peerset membership, Raft current term, CRDT Merkle-DAG nodes etc. Thus, when re-importing a pinset it is important to remember that:\n In raft, the given pinset will be used to create a new snapshot, newer than any existing ones, but including information like the current peerset when existing. In crdt, importing will clean the state completely and create a single batch Merkle-DAG node. This effectively compacts the state by replacing the Merkle-DAG, but to prevent this peer from re-downloading the old DAG, all other peers in the Cluster should have replaced or removed it too.  See Disaster recovery below for more information.\nraft state dumps can be imported as crdt pinsets and vice-versa. Resetting a peer: state cleanup Cleaning up the state results in a blank cluster peer. Such peer will need to re-bootstrap (raft) or reconnect (crdt) to a Cluster in order to re-download the state. The state can also be provided by importing it, as described above. The cleanup can be performed by:\nipfs-cluster-service state cleanup  Note that this does not remove or rewrite the configuration, the identity or the peerstore files. Removing the raft or crdt data folders is to all effects the equivalent of a state cleanup.\nWhen using Raft, the raft folder will be renamed as raft.old.X. Several copies will be kept depending on the backups_rotate configuration value. When using CRDT, the crdt-related data will be deleted from the datastore.\nDisaster recovery The only content that IPFS Cluster stores and which is unique to a cluster peer is the pinset. IPFS content is stored by IPFS. Usually, if you are running a cluster, there will be several peers replicating the content and the cluster pinset so that when one or several peers crash, are destroyed, disappear or simply fail, they can be reset to their clean form re-sync from other existing peers.\nA healthy cluster is one with at least 50% of healthy online peers (raft) or at least one trusted, healthy peer (crdt). Thus, any peer can be fully reset and re-join an otherwise healthy cluster with the same procedure that you would add a new peer. In raft, departed peers should be nevertheless manually removed with ipfs-cluster-ctl peer rm if they are never going to re-join again.\nUnhealthy clusters Things change for unhealthy clusters:\n In crdt, the lack of trusted peers will prevent the restored peer from re-syncing to the cluster state (although, as a workaround, it could temporally trust any other peer). In raft, the lack of quorum when more than 50% of peers are down, prevents adding new peers, removing broken peers or operating the cluster.  In such events, it may be easier to simply salvage the state and re-create your cluster following the next procedure:\n Locate a peer that still stores the state (raft or any of the datastore folders) Export the pinset with ipfs-cluster-service state export Reset your peer or setup a new peer from scratch Run ipfs-cluster-service state import to import the state copy from step 2 Start the peer as a single-peer-cluster Fully cleanup, upgrade and bootstrap the rest of the peers to the running one  State upgrades Since version 0.10.0, Cluster peers will not need manual state upgrades (the state upgrade command is gone).\n"
},
{
	"uri": "https://ipfscluster.io/documentation/deployment/automations/",
	"title": "Deployment automations",
	"tags": [],
	"description": "",
	"content": " Deployment automations We have a number of automations to facilitate configuring and deploying IPFS Clusters:\n Ansible roles Docker Kubernetes with Kustomize  Ansible roles Ansible roles for configuring and deploying ipfs-cluster-service, ipfs-cluster-ctl and go-ipfs (including templated configuration files) are available at https://github.com/hsanjuan/ansible-ipfs-cluster.\nDocker IPFS Cluster provides official dockerized releases at https://hub.docker.com/r/ipfs/ipfs-cluster/ along with an example template for docker-compose. If you want to run one of the /ipfs/ipfs-cluster Docker containers, it is important to know that:\n The container does not run go-ipfs and you should run the IPFS daemon separately, for example, using the ipfs/go-ipfs Docker container. The ipfs_connector/ipfshttp/node_multiaddress configuration value will need to be adjusted accordingly to be able to reach the IPFS API. This path supports DNS addresses (/dns4/ipfs1/tcp/5001) and is set from the CLUSTER_IPFSHTTP_NODEMULTIADDRESS environment variable when starting the container and no previous configuration exists. By default, we use the /data/ipfs-cluster as the IPFS Cluster configuration path. We recommend mounting this folder as means to provide custom configurations and/or data persistency for your peers. This is usually achieved by passing -v \u0026lt;your_local_path\u0026gt;:/data/ipfs-cluster to docker run.  The container (Dockerfile here runs an entrypoint.sh script which initializes IPFS Cluster when no configuration is present. The configuration values can be controlled by setting environment variables as explained in the configuration reference.\nBy default crdt consensus is used to initialize the configuration. This can be overridden by setting IPFS_CLUSTER_CONSENSUS=raft. Similarly, pebble is used as the default backend, and can be overridden setting IPFS_CLUSTER_DATASTORE=badger3 (for example).\nUnless you run docker with --net=host, you will need to set $CLUSTER_IPFSHTTP_NODEMULTIADDRESS or make sure the configuration has the correct node_multiaddress. Unless you run docker with --net=host, the REST API endpoint will only listen locally inside the container. You may need to set $CLUSTER_RESTAPI_HTTPLISTENMULTIADDRESS to /ip4/0.0.0.0/tcp/9096 if you wish to access this endpoint from outside the container, along with the necessary port forwarding (-p 9094:9094. Docker compose We also provide an example docker-compose.yml that is able to launch an IPFS Cluster with two Cluster peers and two IPFS daemons running.\nDuring the first launch, configurations are automatically generated and will be persisted for next launches in the ./compose folder, along with the ipfs ones. Delete this folder to reset the Docker Compose setup.\nOnly the IPFS swarm port (tcp 4001/4101) and the IPFS Cluster API ports (tcp 9094/9194) are exposed out of the containers.\nThis compose file is provided as an example on how to set up a multi-peer Cluster using Docker containers. You may need to adapt it for your own needs.\n"
},
{
	"uri": "https://ipfscluster.io/documentation/reference/follow/",
	"title": "ipfs-cluster-follow",
	"tags": [],
	"description": "",
	"content": " ipfs-cluster-follow The ipfs-cluster-follow command line application is a user-friendly way of running follower peers to join collaborative IPFS Clusters. You can obtain more information about collaborative clusters in the respective section.\nipfs-cluster-follow runs an optimized cluster peer for use with collaborative cluster. It focuses on simplicity and security for users running follower peers, removing most of the configuration hassles that running a peer has.\nConfiguration ipfs-cluster-follow normally uses configurations distributed through the local IPFS gateway as templates.\nIn this case, the service.json file for each configured cluster contains a single source key pointing to a URL, which is read when starting the peer.\nThis file can be replaced by a custom service.json file. Alternatively, every configuration value can be overridden with environment variables as explained in the configuration reference. The IPFS_GATEWAY environmental variable can be used to set the gateway location if it\u0026rsquo;s not the default (127.0.0.1:8080).\nIf you need to expose the HTTP API on a TCP port rather than the default unix socket, set CLUSTER_RESTAPI_HTTPLISTENMULTIADDRESS accordingly.\nUsing ipfs-cluster-ctl with ipfs-cluster-follow ipfs-cluster-follow exposes an API endpoint using, by default, a unix socket, rather than listening on a local TCP port.\nIf you wish to talk to the peer using ipfs-cluster-ctl, you can run:\nipfs-cluster-ctl --host /unix//\u0026lt;home_path\u0026gt;/.ipfs-cluster-follow/\u0026lt;clusterName\u0026gt;/api-socket ...  Usage Usage information can be obtained by running:\n$ ipfs-cluster-follow --help  Finding collaborative clusters to join Visit collab.ipfscluster.io for a list of collaborative clusters that you can join and more instructions.\nipfs-cluster-ctl --host /ip4/\u0026lt;ip\u0026gt;/ipfs/\u0026lt;peerID\u0026gt; ...\nor\nipfs-cluster-ctl --host /dnsaddr/mydomain.com ... (setting a _dnsaddr TXT dnsaddr=peer_multiaddress field in your dns).\nIf the libp2p peer you\u0026rsquo;re contacting is using a cluster secret (a private networks key), you will also need to provide --secret \u0026lt;32 byte-hex-encoded key\u0026gt; to the command.\nWe recommend that you alias the ipfs-cluster-ctl command in your shell to something shorter and with the right global options.\nDownload To download ipfs-cluster-follow check the downloads page.\n"
},
{
	"uri": "https://ipfscluster.io/documentation/reference/service/",
	"title": "ipfs-cluster-service",
	"tags": [],
	"description": "",
	"content": " ipfs-cluster-service The ipfs-cluster-service is a command line application that runs a full cluster peer:\n ipfs-cluster-service init initializes configuration and identity. ipfs-cluster-service daemon launches a cluster peer. ipfs-cluster-service state allows to export, import, and cleanup the persistent state.  The ipfs-cluster-service provides its own help by running ipfs-cluster-service --help or ipfs-cluster-service \u0026lt;command\u0026gt; --help.\nDebugging ipfs-cluster-service offers two debugging options:\n --debug enables debug logging from the ipfs-cluster, go-libp2p-raft and go-libp2p-rpc layers. This will be a very verbose log output, but at the same time it is the most informative. --loglevel sets the log level ([error, warning, info, debug]) for the ipfs-cluster only, allowing to get an overview of the what cluster is doing. The default log-level is info.  By default, logs are coloured. To disable log colours set the IPFS_LOGGING_FMT environment variable to nocolor.\nDownload To download ipfs-cluster-service check the downloads page.\n"
},
{
	"uri": "https://ipfscluster.io/documentation/guides/peerset/",
	"title": "Peerset management",
	"tags": [],
	"description": "",
	"content": " Peerset management Adding and removing peers from the Cluster might be a simpler or trickier operation depending on the \u0026ldquo;consensus\u0026rdquo; component used by the cluster (the consensus component is in charge on managing the peerset).\nListing peers ipfs-cluster-ctl peers ls  The peers ls command will produce the list of peers in the cluster will all their information. It is the equivalent of calling ipfs-cluster-ctl id on every cluster peer and building a list with the results, but for it to work it needs to contact all the current peers of the cluster, meaning it can be a slow operation. Instead, if you just want a list of the peer IDs in the cluster you can see it with the id command (the text output only shows the number of peers):\nipfs-cluster-ctl --enc=json id  Adding new peers Adding new peers to a cluster works exactly as described in the Bootstrapping the Cluster section. The works-for-all method is to use the ipfs-cluster-service daemon --bootstrap flag.\nRemoving peers CRDT mode In CRDT-mode, peers can be simply stopped. Other peers may consider them part of the peerset until their last metric expires. Thus, reducing the metric ttls will speed this up.\nRaft mode In Raft-mode, peers can be stopped, but then they will not be available to participate in cluster operations and will still be considered part of the peerset. This is perfectly fine if the peer will be re-started on the future and the majority of cluster peers will still be online. Otherwise, the departing peer needs to be manually removed with:\nipfs-cluster-ctl peers rm \u0026lt;pid\u0026gt;  Raft peers can only be removed when the Raft cluster has at least 50% of its members online. This can be called from the peer shutting down (self-removal) or from any other peer. In any case, it will cause the peer to shut itself down when it realizes it has been removed.\nAlternatively, the leave_on_shutdown configuration option can be set to true. With this option, a peer shutting down cleanly will try to remove itself from the Raft peerset in the process. Peers which have been removed from the Raft peerset automatically clean their state and will need to bootstrap again to it to re-join it.\n"
},
{
	"uri": "https://ipfscluster.io/documentation/reference/",
	"title": "Reference",
	"tags": [],
	"description": "",
	"content": " Cluster reference  Configuration   REST API   Pinning Service API   IPFS Proxy   ipfs-cluster-ctl   ipfs-cluster-follow   ipfs-cluster-service   "
},
{
	"uri": "https://ipfscluster.io/documentation/guides/metrics/",
	"title": "Cluster pubsub metrics",
	"tags": [],
	"description": "",
	"content": " Cluster pubsub metrics This section is about metrics broadcasted between cluster peers. For Prometheus metrics for monitoring see the monitoring guide. Cluster peers regularly broadcast (using gossipsub) metrics between each others. These metrics serve several purposes:\n They allow to detect when a peer has left the cluster (each metric has an expiration date and is expected to be renewed before it is reached). This can be used to trigger actions such as repinnings. In crdt-mode they serve to identify the current cluster peerset (list of peers with non expired metrics). They are used to communicate information such as peer names and free-space, which can be used, for example, to make pin allocation decisions.  The metrics are produced by \u0026ldquo;informer\u0026rdquo; components. There are currently several types of metrics:\n ping: the lack of pings from a given cluster peer signifies that the peer is down and is used to trigger re-pinnings when enabled. The ping metric includes information about each peer, like its peer name, IPFS daemon ID and addresses etc. which are then re-used to fill-in fields in the pin status objects when requested. freespace: this metric informs how much free space IPFS has in its repository and is used to decide whether to allocate new pins to this peer or others. tag:*: \u0026ldquo;tag\u0026rdquo; metric provide values coming from the tag informer. For example, peer may broadcast a metric tag:group with value server. The values are used by the balanced allocator to distribute pins across different values of a single tag. pinqueue: this metric carries the number of items queued to pin and can also be used to avoid pinning on peers with long pinning queues.  Administrators can inspect the latest metrics received by a peer with the following commands:\nipfs-cluster-ctl health metrics # lists available metrics ipfs-cluster-ctl health metrics ping ipfs-cluster-ctl health metrics freespace ...  Note that:\n The Time-To-Live associated to freespace and other informer-metrics is controlled with the metric_ttl options for the different informers (the disk informer is used by default). Increasing it reduces the number of time a peer sends metrics to the network. The Time-To-Live associated to ping metrics is controlled by the cluster.monitor_ping_interval option. The pubsubmon.check_interval option controls how often a peer checks for expired metrics from other peers. You can read all the details in the Configuration reference.  "
},
{
	"uri": "https://ipfscluster.io/documentation/guides/upgrades/",
	"title": "Upgrades",
	"tags": [],
	"description": "",
	"content": " Upgrades The IPFS Cluster project releases new versions regularly. This section describes the procedure to upgrade Clusters with minimal or no downtime.\nIt is very important to check the changelog before upgrading, in order to get familiar with changes since the last version. All information about potential incompatibilities and breaking changes are included there.\nThe other main consideration is that:\nStarting on v0.12.1, all the cluster peers need to run on the same RPC protocol version. The RPC Protocol version can be seen in the response of ipfs-cluster-ctl --enc=json id (rpc_protocol_version field), or in the source code. It should remain stable across multiple IPFS Cluster releases and it only changes when non-backwards compatible RPC changes happen.\nIf there is a mismatch between the RPC protocol versions of the peers, they will not be able to communicate.\nWhen the RPC protocol version is the same, the core functionality of peers will work even if the cluster is made of peers running different versions, unless the changelog states otherwise.\nRunning the upgrade The general approach to is to:\n Upgrade ipfs-cluster-ctl, ipfs-cluster-service or ipfs-cluster-follow to the new version. Restart all the peers (either sequentially or at once).  In the case of raft, the restart only works if:\n leave_on_shutdown is set to false. Otherwise, those peer will need to be bootstrapped on the next start. wait_for_leader_timeout is sufficiently high to account for the restart of the majority peers in the cluster (default should be ok in most cases)  "
},
{
	"uri": "https://ipfscluster.io/documentation/roadmap/",
	"title": "Roadmap",
	"tags": [],
	"description": "",
	"content": " Roadmap IPFS Cluster is an open source project in the IPFS ecosystem stewarded by Protocol Labs.\nUsers of the project can expect:\n Ongoing releases and bug fixes. New functionality, usually focused on unlocking additional scalability of the clusters. Support over the common channels and issue triage. Dependency upgrades, compatibility and features from the IPFS and libp2p downstreams.  The following represent \u0026ldquo;larger endaevors\u0026rdquo; that are in our radar:\n In progress: Full and officially-vetted Kubernetes support. Upcoming: Fully embedded IPFS peer in the IPFS Cluster daemon. Optimistic replication: allow cluster peers to decide what content they back rather than defining allocations. DAG Sharding support: distributing large DAGs across multiple peers. Ongoing effort but lacking go-ipfs support for depth-limited pins. Additional chunking/sharding/encoding strategies. FEC support. Cluster-controlled MFS. Done! Improve the metrics exporting system (i.e. Prometheus) with new metrics. Done! RPC streaming improvements (primarily affects speeding up adding content to many cluster nodes at once). Done! Exploring support for a more allocation strategies e.g. by geographic location or as a function of access patterns. Done! IPFS Pinning API support.  "
},
{
	"uri": "https://ipfscluster.io/documentation/guides/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "",
	"content": " Troubleshooting This sections contain a few tips to identify and correct problems when running IPFS Cluster.\nIPFS Cluster fails to build Please read the Download page. It has instructions on how to build the software (please follow them).\nDebug logging When discovering a problem, it is always useful to try to figure out the issue and potentially provide some relevant logs when asking for help.\nipfs-cluster-service By default, ipfs-cluster-service prints only INFO, WARNING and ERROR messages. You can increase the logging level in several ways:\n The --debug flag will set the DEBUG log level in ALL logging subsystems. This produces a lot of information and may even slow down your peer significantly. Do not use by default! The --loglevel allows fine-grain control of what components should log and what levels they should use. --loglevel debug makes all cluster-relevant components print DEBUG messages. This can be limited by component too: --loglevel error,restapi:debug,pintracker:debug will set the default log level to ERROr, while setting DEBUG on the restapi and the pintracker components.  Interpreting debug information can be tricky. Take this example:\n2020-05-17T00:51:52.953+0200 ERROR ipfshttp ipfshttp/ipfshttp.go:722 Post \u0026quot;http://127.0.0.1:5001/api/v0/repo/stat?size-only=true\u0026quot;: dial tcp 127.0.0.1:5001: connect: connection refused  The above line shows a message of ERROR severity, coming from the ipfshttp facility. It was logged in ipfshttp/ipfshttp.go:722 (filename and line number) . This facility corresponds to the ipfshttp module which implements the IPFS Connector component. This information helps narrowing the context from which the error comes from. The error message indicates that the component failed to perform a GET request to the IPFS API.\nGiven all this context, we can figure out that very probably the ipfs daemon is not running, or not reachable.\nWhen debugging, you can find out which component is producing the errors and then use --loglevel \u0026lt;component\u0026gt;:debug to get more information about what that component is doing.\nipfs-cluster-ctl ipfs-cluster-ctl offers a --debug flag which will print information about the API endpoints used by the tool. --enc json allows to print raw json responses from the API.\nipfs-cluster-follow ipfs-cluster-follow does not include a way to increase verbosity. You can however run ipfs-cluster-service -c \u0026lt;folder\u0026gt; daemon where the \u0026lt;folder\u0026gt; is the Config folder as shown by ipfs-cluster-follow \u0026lt;clusterName\u0026gt; info. You can then increase verbosity as shown above. Only do this for debugging when necessary!\nPeer not starting When your peer is not starting:\n Check the logs and look for errors Are all the listen addresses free or are they used by a different process? Are other peers of the cluster reachable? Is the cluster.secret the same for all peers? Double-check that the peerstore file has the right content and that you\u0026rsquo;ve followed one of the methods in the Bootstrapping the Cluster section. Double-check that the rest of the cluster is in a healthy state. In some cases, it may help to run ipfs-cluster-service state clean (specially if the reason for not starting is a mismatch between the raft state and the cluster peers). Assuming that the cluster is healthy, this will allow the non-starting peer to pull a clean state from the cluster Leader when bootstrapping.  Peer stopped unexpectedly When a peer stops unexpectedly:\n Make sure you simply haven\u0026rsquo;t removed the peer from the cluster or triggered a shutdown Check the logs for any clues that the process died because of an internal fault Check your system logs to find if anything external killed the process Report any application panics, as they should not happen, along with the logs  libp2p errors Since cluster is built on top of libp2p, many errors that new users face come from libp2p and have confusing messages which are not obvious at first sight. This list compiles some of them:\n dial attempt failed: misdial to \u0026lt;peer.ID XXXXXX\u0026gt; through ....: this means that the multiaddress you are contacting has a different peer in it than expected. dial attempt failed: connection refused: the peer is not running or not listening on the expected address/protocol/port. dial attempt failed: context deadline exceeded: this means that the address is not reachable or that the wrong secret is being used. dial backoff: same as above. dial attempt failed: incoming message was too large: this probably means that your cluster peers are not sharing the same secret. version not supported: this means that your nodes are running on incompatible versions.  "
},
{
	"uri": "https://ipfscluster.io/documentation/guides/monitoring/",
	"title": "Monitoring and tracing",
	"tags": [],
	"description": "",
	"content": " Monitoring and tracing IPFS Cluster can expose a Prometheus endpoint for metric-scraping and can also submit code tracing information to Jaeger.\nThese are configured in the observations section of the configuration and can be enabled from there, or by starting a cluster peer with:\nipfs-cluster-service daemon --stats --tracing  Apart from all go-specific metrics, cluster exports some metrics to track the current state of the cluster peer, these can be quickly inspected with curl 'http://127.0.0.1:8888/metrics | grep cluster, and include the total number of pins, the number of items queued etc.\nDevelopment setup for tracing and metrics The following section shows how to:\n Configure and run Jaeger and Prometheus services locally using Docker Configure IPFS Cluster to send traces to Jaeger and metrics to Prometheus  This section shows how to deploy and configure tracing and metrics on a local development environment. Production deployment of either Jaeger or Prometheus is beyond the scope of what is being covered here. Jaeger First, pull down the Jaeger all-in-one image:\n$ docker pull jaegertracing/all-in-one:1.9  Once the image has been downloaded, run the image with the following configuration:\n$ docker run -d --name jaeger \\ -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \\ -p 5775:5775/udp \\ -p 6831:6831/udp \\ -p 6832:6832/udp \\ -p 5778:5778 \\ -p 16686:16686 \\ -p 14268:14268 \\ -p 9411:9411 \\ jaegertracing/all-in-one:1.9  Of particular note are the following ports on the Jaeger container: - 6831 is default agent endpoint used by IPFS Cluster - 16686 exposes the web UI of the Jaeger service, where you can query and search collected traces\nPrometheus To configure Prometheus, we create a prometheus.yml file, such as the following:\nglobal: scrape_interval: 15s evaluation_interval: 15s scrape_configs: - job_name: ipfs-cluster-daemon scrape_interval: 2s static_configs: - targets: ['localhost:8888']  The target address specified matches the default address in the metrics configuration in IPFS Cluster, but feel to change it to something more suitable to your environment, just make sure to update your ~/.ipfs-cluster/service.json to match.\nIn order to run prometheus, pull the following Docker image:\n$ docker pull prom/prometheus  Then run the Prometheus container, making sure to mount the configuration file we just created:\n$ docker run --network host -v /tmp/prometheus.yml:/etc/prometheus/prometheus.yml --name promy prom/prometheus  Note that to have Prometheus reach the metrics endpoint exposed by IPFS Cluster, it requires that the container be run on the host\u0026rsquo;s network, this done via the --network host flag in the run command above.\nIPFS Cluster configuration Configure the observations section in the service.json file as follows:\n{ \u0026quot;metrics\u0026quot;: { \u0026quot;enable_stats\u0026quot;: true, \u0026quot;prometheus_endpoint\u0026quot;: \u0026quot;/ip4/0.0.0.0/tcp/8888\u0026quot;, \u0026quot;reporting_interval\u0026quot;: \u0026quot;2s\u0026quot; }, \u0026quot;tracing\u0026quot;: { \u0026quot;enable_tracing\u0026quot;: true, \u0026quot;jaeger_agent_endpoint\u0026quot;: \u0026quot;/ip4/0.0.0.0/udp/6831\u0026quot;, \u0026quot;sampling_prob\u0026quot;: 0.3, \u0026quot;service_name\u0026quot;: \u0026quot;cluster-daemon\u0026quot; } }  For local development tracing, it is advised to change the observations.tracing.sampling_prob to 1, so that every action in the system is recorded and sent to Jaeger.\nRunning the cluster peer with the configuration above should provide an endpoint for Prometheus to collect metrics and will push traces to Jaeger.\nOnce the cluster peer has started, go to http://localhost:9090/targets to confirm that Prometheus has been able to begin scraping metrics from IPFS Cluster.\nTo confirm that tracing is functioning correctly, we will add a file and pin to IPFS Cluster in one step by using the IPFS Cluster add command and then search for its trace in Jaeger.\n$ echo 'test tracing file' \u0026gt; test.file $ ipfs-cluster-ctl add test.file  Go to https://localhost:16686 and you should see a trace, it may be labelled \u0026lt;trace-without-root-span\u0026gt; due to an issue with how Jaeger creates/determines root spans, but all the information is still inside. If there is nothing there, give it sometime to flush the traces to the Jaeger Collector as it isn\u0026rsquo;t instantaneous.\nAfter having run a few commands to get some traces, it is a good time to go check out the graph page of Prometheus, which is prefilled with a histogram of the request latencies of the gorpc calls between IPFS Cluster components. There are plenty of other metrics configured for collection and they can be found in the drop-down next to the Execute button.\nHopefully, this tooling enables you to better understand how IPFS Cluster operates and performs.\n"
},
{
	"uri": "https://ipfscluster.io/documentation/guides/k8s/",
	"title": "Deployment on Kubernetes",
	"tags": [],
	"description": "",
	"content": " Running Cluster on Kubernetes Helm templates Monaparty maintains Helm templates for cluster deployment at https://github.com/monaparty/helm-ipfs-cluster\nKubernetes Operator A Kubernetes [Operator](\u0026ldquo;https://kubernetes.io/docs/concepts/extend-kubernetes/operator/\u0026quot;) written by a partnership between Protocol Labs and Red Hat. The project is still in active development and should not be used for production use cases. If this is something you would like to try, head over to the documentation or github\nKustomize Beware we have not updated the following instructions in a while. They show how to run a simple Cluster on Kubernetes using Kustomize.\nThis guide assumes you have a running Kubernetes cluster to deploy to and have properly configured `kubectl`. Prepare Configuration Values Configuring the Secret Resource In Kubernetes, Secret objects are used to hold values such as tokens, or private keys.\n# secret.yaml apiVersion: v1 kind: Secret metadata: name: secret-config type: Opaque data: cluster-secret: \u0026lt;INSERT_SECRET\u0026gt;  Cluster Secret To generate the cluster_secret value in secret.yaml, run the following and insert the output in the appropriate place in the secret.yaml file:\n$ od -vN 32 -An -tx1 /dev/urandom | tr -d ' \\n' | base64 -w 0 -  Bootstrap Peer ID and Private Key To generate the values for bootstrap_peer_id and bootstrap_peer_priv_key, install ipfs-key and then run the following:\n$ ipfs-key | base64 -w 0  Copy the id into the env-configmap.yaml file. I.e:\n# env-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: env-config data: bootstrap-peer-id: \u0026lt;INSERT_PEER_ID\u0026gt;  Then copy the private key value and run the following with it:\n$ echo \u0026quot;\u0026lt;INSERT_PRIV_KEY_VALUE_HERE\u0026gt;\u0026quot; | base64 -w 0 -  Copy the output to the secret.yaml file.\n# secret.yaml apiVersion: v1 kind: Secret metadata: name: secret-config type: Opaque data: cluster-secret: \u0026lt;INSERT_SECRET\u0026gt; bootstrap-peer-priv-key: \u0026lt;INSERT_KEY\u0026gt;  Defining a StatefulSet From the Kubernetes documentation on StatefulSets:\n Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods.\nLike a Deployment, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.\n This means for us, that any Kubernetes generated configuration, such as hostnames, i.e. ipfs-cluster-0, will be associated with the same Pod and VolumeClaim, which means for example, hostnames will always be associated with the same peer id that is stored in the ~/.ipfs-cluster/service.json file. And all this is important, because it allows us to bootstrap our ipfs-cluster without a spinning up a specific bootstrapping peer.\nBreaking the StatefulSet definition into chunks, the first is the preamble and start of the StatefulSet spec:\napiVersion: apps/v1 kind: StatefulSet metadata: name: ipfs-cluster spec: serviceName: ipfs-cluster replicas: 3 selector: matchLabels: app: ipfs-cluster  Following that is the definition of the go-ipfs container:\ntemplate: metadata: labels: app: ipfs-cluster spec: initContainers: - name: configure-ipfs image: \u0026quot;ipfs/go-ipfs:v0.4.18\u0026quot; command: [\u0026quot;sh\u0026quot;, \u0026quot;/custom/configure-ipfs.sh\u0026quot;] volumeMounts: - name: ipfs-storage mountPath: /data/ipfs - name: configure-script mountPath: /custom containers: - name: ipfs image: \u0026quot;ipfs/go-ipfs:v0.4.18\u0026quot; imagePullPolicy: IfNotPresent env: - name: IPFS_FD_MAX value: \u0026quot;4096\u0026quot; ports: - name: swarm protocol: TCP containerPort: 4001 - name: swarm-udp protocol: UDP containerPort: 4002 - name: api protocol: TCP containerPort: 5001 - name: ws protocol: TCP containerPort: 8081 - name: http protocol: TCP containerPort: 8080 livenessProbe: tcpSocket: port: swarm initialDelaySeconds: 30 timeoutSeconds: 5 periodSeconds: 15 volumeMounts: - name: ipfs-storage mountPath: /data/ipfs - name: configure-script mountPath: /custom resources: {}  Take note of the initContainers section, which is used to configure the ipfs node with production appropriate values, see Defining Configuration Scripts.\nNext we define the ipfs-cluster container:\n- name: ipfs-cluster image: \u0026quot;ipfs-cluster/ipfs-cluster:latest\u0026quot; imagePullPolicy: IfNotPresent command: [\u0026quot;sh\u0026quot;, \u0026quot;/custom/entrypoint.sh\u0026quot;] envFrom: - configMapRef: name: env-config env: - name: BOOTSTRAP_PEER_ID valueFrom: configMapRef: name: env-config key: bootstrap-peer-id - name: BOOTSTRAP_PEER_PRIV_KEY valueFrom: secretKeyRef: name: secret-config key: bootstrap-peer-priv-key - name: CLUSTER_SECRET valueFrom: secretKeyRef: name: secret-config key: cluster-secret - name: CLUSTER_MONITOR_PING_INTERVAL value: \u0026quot;3m\u0026quot; - name: SVC_NAME value: $(CLUSTER_SVC_NAME) ports: - name: api-http containerPort: 9094 protocol: TCP - name: proxy-http containerPort: 9095 protocol: TCP - name: cluster-swarm containerPort: 9096 protocol: TCP livenessProbe: tcpSocket: port: cluster-swarm initialDelaySeconds: 5 timeoutSeconds: 5 periodSeconds: 10 volumeMounts: - name: cluster-storage mountPath: /data/ipfs-cluster - name: configure-script mountPath: /custom resources: {}  Note that BOOTSTRAP_PEER_ID and BOOTSTRAP_PEER_PRIV_KEY were the values we defined earlier, they will be used only be the very first ipfs-cluster container ipfs-cluster-0 and then BOOTSTRAP_PEER_ID will be used to pass the bootstrapping multiaddress to the other ipfs-cluster containers.\nFinally, we define the volumes for the configuration scripts and also the data volumes for the ipfs and ipfs-cluster containers:\nvolumes: - name: configure-script configMap: name: ipfs-cluster-set-bootstrap-conf volumeClaimTemplates: - metadata: name: cluster-storage spec: storageClassName: standard accessModes: [\u0026quot;ReadWriteOnce\u0026quot;] persistentVolumeReclaimPolicy: Retain resources: requests: storage: 5Gi - metadata: name: ipfs-storage spec: storageClassName: standard accessModes: [\u0026quot;ReadWriteOnce\u0026quot;] persistentVolumeReclaimPolicy: Retain resources: requests: storage: 200Gi  Depending on your cloud provider, you will have to change the value of storageClassName to the appropriate value.\nThe StatefulSet definition as an entirety, can be found at github.com/lanzafame/ipfs-cluster-k8s.\nDefining Configuration Scripts The ConfigMap contains two scripts, entrypoint.sh which enables hands-free bootstrapping of the ipfs-cluster cluster and configure-ipfs.sh which configures the ipfs daemon with production values. For more information about configuring ipfs for production, see go-ipfs configuration tweaks.\napiVersion: v1 kind: ConfigMap metadata: name: ipfs-cluster-set-bootstrap-conf data: entrypoint.sh: | #!/bin/sh user=ipfs # This is a custom entrypoint for k8s designed to connect to the bootstrap # node running in the cluster. It has been set up using a configmap to # allow changes on the fly. if [ ! -f /data/ipfs-cluster/service.json ]; then ipfs-cluster-service init fi PEER_HOSTNAME=`cat /proc/sys/kernel/hostname` grep -q \u0026quot;.*ipfs-cluster-0.*\u0026quot; /proc/sys/kernel/hostname if [ $? -eq 0 ]; then CLUSTER_ID=${BOOTSTRAP_PEER_ID} \\ CLUSTER_PRIVATEKEY=${BOOTSTRAP_PEER_PRIV_KEY} \\ exec ipfs-cluster-service daemon --upgrade else BOOTSTRAP_ADDR=/dns4/${SVC_NAME}-0/tcp/9096/ipfs/${BOOTSTRAP_PEER_ID} if [ -z $BOOTSTRAP_ADDR ]; then exit 1 fi # Only ipfs user can get here exec ipfs-cluster-service daemon --upgrade --bootstrap $BOOTSTRAP_ADDR --leave fi configure-ipfs.sh: | #!/bin/sh set -e set -x user=ipfs # This is a custom entrypoint for k8s designed to run ipfs nodes in an appropriate # setup for production scenarios. mkdir -p /data/ipfs \u0026amp;\u0026amp; chown -R ipfs /data/ipfs if [ -f /data/ipfs/config ]; then if [ -f /data/ipfs/repo.lock ]; then rm /data/ipfs/repo.lock fi exit 0 fi ipfs init --profile=badgerds,server ipfs config Addresses.API /ip4/0.0.0.0/tcp/5001 ipfs config Addresses.Gateway /ip4/0.0.0.0/tcp/8080 ipfs config --json Swarm.ConnMgr.HighWater 2000 ipfs config --json Datastore.BloomFilterSize 1048576 ipfs config Datastore.StorageMax 100GB  Exposing IPFS Cluster Endpoints The final step for us is to define the Service which expose the IPFS Cluster endpoints to the outside the Pod.\napiVersion: v1 kind: Service metadata: name: ipfs-cluster annotations: external-dns.alpha.kubernetes.io/hostname: change.me.com labels: app: ipfs-cluster spec: type: LoadBalancer ports: - name: swarm targetPort: swarm port: 4001 - name: swarm-udp targetPort: swarm-udp port: 4002 - name: ws targetPort: ws port: 8081 - name: http targetPort: http port: 8080 - name: api-http targetPort: api-http port: 9094 - name: proxy-http targetPort: proxy-http port: 9095 - name: cluster-swarm targetPort: cluster-swarm port: 9096 selector: app: ipfs-cluster  Depending on where and how you have set up your Kubernetes cluster, you may be able to make use of the ExternalDNS annotation external-dns.alpha.kubernetes.io/hostname, which will automatically take the provided value, change.me.com, and create a DNS record in the configured DNS provider, i.e. AWS Route53 or Google CloudDNS.\nAlso note that the targetPort fields are using the named ports from the PodSpec defined in the StatefulSet resource.\n"
},
{
	"uri": "https://ipfscluster.io/news/state-of-the-clusters-march-2022/",
	"title": "State of the clusters: March 2022",
	"tags": [],
	"description": "",
	"content": " State of the clusters: March 2022 Two months have passed since our last update on the \u0026ldquo;state of the clusters\u0026rdquo;. In our previous post I mentioned we were tracking 25 million pins on a 9-peer cluster.\nToday that cluster (which stores content for NFT.storage) has grown to 18 peers and 50 million pins. Our average usage rate keeps at around 4 new pins per second.\nThe new peers were added and were able to sync the cluster pinset in about 24 hours. This is a cluster with a crdt-DAG-depth 500k, which, given the multiple branches, likely involved syncing millions of CRDT-dag blocks. Because the new peers are empty and have more space that the older ones, they started storing and taking the load, relieving others as intended (older ones have up to 70TB of data pinned).\nIn the last version (v0.14.5), which we rolled out everywhere, we included some changes to improve performance and CRDT-DAG syncing. We have also started rebuilding older nodes with LVM-striped, XFS and flatfs/next-to-last-3 datastore layout configuration for IPFS. In our experience, XFS performs better than Ext4 for folder with large number of files, which is essentially what flatfs does. Next-to-last-3 is a sharding strategy that shards blocks over folders with 3 letters (the default is 2). By having more shards, there are less items on every folder, which is better for very large nodes.\nThe main issue now preventing unbounded scalability is that the huge pinset causes RAM memory spikes whenever a cluster peer needs to check that the pins that are supposed to on ipfs are actually there. This is because every item on the pinset is loaded on memory to be able to iterate on them. At this point, the memory spikes are very noticeable and steal memory which IPFS would gladly use.\nThe next release of IPFS Cluster will address this and other issues through a major shift on how things work internally, which will not only fix the memory spikes, but also unlock lots of performance gains when adding content to cluster peers. With these changes, IPFS Cluster will graduate to version 1.0.0, having proven its reliability and scalability properties while serving production infrastructure.\n"
},
{
	"uri": "https://ipfscluster.io/news/state-of-the-clusters-jan-2022/",
	"title": "State of the clusters: January 2022",
	"tags": [],
	"description": "",
	"content": " State of the clusters: January 2022 Today, we would like to provide a few details and figures on where we are with regards to cluster scalability, particularly as ensuring IPFS storage allocation and replication behind the NFT.storage platform.\nWe have started 2022 with a new release (v0.14.4). A few months ago, we were happy to report that we were tracking around 2 million pins.\nToday, cluster is tracking over 25 million pins for NFT.storage in a single cluster, made of 9 peers with around 85TB of storage each running go-ipfs v0.12.0-rc1. On average, we are ingesting more than 4 items per second (normally add-requests that put the content directly on the cluster). We know we can take many hundreds of pins per second when needed.\nThese numbers are not overly impressive when compared with, for example a PostgreSQL instance for pinset tracking, but we understand cluster as a distributed application with seamless pinset syncing which also supports things like follower-clusters and scalability to hundreds of peers based on its pubsub+crdt pinset distribution mechanisms.\nIn terms of configuration, we have set the cluster peer to let IPFS pin up to 8 items in parallel. This is what we found was a well-performing value when going through pinning queues of several million items. Bitswap performance, disk usage, network bandwidth all affect the right values. The cluster-peers are configured using the crdt consensus mode, with replication factors set to 3. Each node is tagged with a datacenter tag, and the allocator is set to allocate per datacenter and free-space. Thus, we get global distribution of every pin, which are then allocated to the peers with most free space in each DC. We make use of the crdt-batching function, creating commits every 300 items or 10 seconds (although we tune them as we need, sometimes increasing the batch size or delay). For reference, one batch (crdt-delta) can fit almost 4000 pins with 3 allocations (actual number depends on the pin options and allocations).\nThe 20x pinset growth in the last few months has necessarily been accompanied by several releases to get IPFS Cluster up to the task of handling multi-million setups:\n The cluster-peer datastore can be setup with LevelDB and Badger, and the latter is GC\u0026rsquo;ed reguarlly so that it does not grow to take too much space per pin. We heavily sped up operations reading the full pinset (pin ls or status). For example, it is now very efficient to check all the pins in error or queued states because filtering has been improved. Listing all pins in the state has improved an order of magnitude. State export and import functions have also been improved to allow for cluster pinsets to be moved around (to new clusters), which facilitates maintenance, for example by setting new allocations for pins.  The next steps are to keep iterating towards supporting much larger pinsets. One of the improvements in the pipeline will be streaming-RPC support (cluster components communicate via RPC). This will allow us to speed up many operations, such as listing or adding to the cluster.\n"
},
{
	"uri": "https://ipfscluster.io/news/0.13.3_nft_storage/",
	"title": "NFT.storage - powered by IPFS Cluster v0.13.3",
	"tags": [],
	"description": "",
	"content": " 20210514 | NFT.storage - powered by IPFS Cluster v0.13.3 Filecoin recently announced the launch of NFT.storage, a pinning service to provide perpetual IPFS storage specifically catered to NFT creators and collectors.\nThe service is backed by storage provided by Pinata and Protocol Labs, with the service on the Protocol Labs side relying on IPFS Cluster for pin tracking and replication.\nThe service has been setup as a collaborative cluster with 3 main storage peers run by Protocol Labs. The Cluster currently tracks and pins 1.900.000+ items, including many existing NFTs from around the web, which are preserved for posterity.\nTo better support the requirements of the project, a couple of upgrades have been added to IPFS Cluster:\n First, we have enabled batch-pin ingest in CRDT-mode. This allowed us to easily ingest over 400.000 pins to the cluster in less than 1 hour, with a very low cost to the system. From that point, the cluster peers make sure that IPFS pins the items in an orderly fashion, restarting stuck pins as needed.\n Second, we have added the possibility of adding arbitrary DAGs to the cluster directly, by enabling CAR-file imports on the /add endpoint. This powers the storage of CBOR-encoded DAGs, that include metadata and links to the actual NFT-material.\n  These features have been included in IPFS Cluster 0.13.3, which we just released.\nHappy pinning (and now, at very high rates)!\n"
},
{
	"uri": "https://ipfscluster.io/news/0.13.1_release/",
	"title": "Release 0.13.1 and current state of the project",
	"tags": [],
	"description": "",
	"content": " 20210114 | Release 0.13.1 and current state of the project We just released IPFS Cluster 0.13.1, with some bugfixes, dependency upgrades and a couple of improvements.\nWhile development efforts have been moved to other parts of the ecosystem in the last few months, the IPFS Project continues being maintained, although without active development of large features. What users can expect is:\n Support over the common channels, responses to issues etc. Bugfixes and pull request reviews. Dependency upgrades and project maintenance, with a slow but stable release candence.  IPFS Cluster continues to be used in production to replicate and distribute important data hosted on IPFS. Happy pinning!\n"
},
{
	"uri": "https://ipfscluster.io/news/0.12.0_release/",
	"title": "Release 0.12.0",
	"tags": [],
	"description": "",
	"content": " 20201220 | Release 0.12.0 IPFS Cluster 0.12.0 is here! It comes with the new ipfs-cluster-follow application, a super-easy way of launching a \u0026ldquo;follower\u0026rdquo; peer.\nFollower cluster peers join clusters to participate in the replication and distribution of IPFS content, but do not have permissions to modify the Cluster peerset or perform actions on other peers of the Cluster. When running ipfs-cluster-follow, peers are automatically configured with a template configuration fetched through IPFS (or any HTTP url) and run with some follower-optimized parameters. Additionally, ipfs-cluster-follow can setup and run multiple peers in parallel, so users can subscribe to several clusters at the same time.\nMinor release 0.12.1 contains some minor fixes to ipfs-cluster-follow. Would you like to try it out? Grab ipfs-cluster-follow and run:\n./ipfs-cluster-follow ipfs-websites init ipfs-websites.collab.ipfscluster.io ./ipfs-cluster-follow ipfs-websites run   Your IPFS daemon will start pinning a list of IPFS-related websites (you will need about 600MB of available space). You can stop and re-start your followers any time and they will catch up to the latest state of things.\nWe have also added a bunch of new features. Pins can now have expiration times so that they are automatically unpinned at some point. And Cluster operators can now use the Cluster-GC command to trigger garbage collections on all the managed IPFS daemons. These should be very useful for IPFS storage providers.\nFinally, users running clusters behind NATs or in Dockerized environments will benefit from improvements in NAT traversal and connectivity. Cluster peers now support the new libp2p QUIC transport and TLS handshake.\nYou can read more information about all the new things for this release in the changelog.\nWe hope the possibilities opened by this new release will make IPFS Cluster a very useful tool for hosting and re-distributing IPFS data in a collaborative manner, building communities around archives based on user interests and strenghthening content distribution in the IPFS network by doing so.\n"
},
{
	"uri": "https://ipfscluster.io/news/0.11.0_release/",
	"title": "Release 0.11.0",
	"tags": [],
	"description": "",
	"content": " 20191001 | Release 0.11.0 A few days ago we shipped IPFS Cluster 0.11.0. This was a huge leap forward as it finally crystalizes the journey to replace Raft with a system that allows peers to come and go freely from a cluster while keeping consistency guarantees on the shared pinset. The effort to find a suitable replacement started almost a year ago and resulted in a new crdt component that is based on go-ds-crdt, a datastore implementation using Merkle-CRDTs.\nAs mentioned in the changelog, version 0.11.0 is the biggest release in the project\u0026rsquo;s history and it comes with many other features and improvements.\nWe have also started running IPFS Cluster workshops on several conferences. We keep an updated list of past and upcoming events at our workshops repository. In these workshops, participants install and run cluster peers with a IPFS-hosted configuration and they automatically discover each other, form a Cluster and try out all commands.\nDuring the upcoming months we will be shipping more features but also start taking advantage of IPFS Cluster\u0026rsquo;s new features by launching public collaborative clusters: we will publish instructions for anyone to join specific clusters to backup pieces of important IPFS data such as the distributions page, wikipedia mirrors or community websites.\nWe wish you a lot of success using the latest version of IPFS Cluster.\n"
},
{
	"uri": "https://ipfscluster.io/news/0.10.0_release/",
	"title": "Release 0.10.0",
	"tags": [],
	"description": "",
	"content": " 20190307 | Release 0.10.0 Today we release 0.10.0, a release with major changes under the hood that will make IPFS Cluster perform significantly faster with large pinsets and less memory demanding.\nFor those upgrading, this release is a mandatory step before any future upgrades, as it will upgrade the internal state to a new format which prepares the floor for the upcoming addition of an alternative CRDT-based \u0026ldquo;consensus\u0026rdquo; component. The new component will increase IPFS Cluster scalability orders of magnitude and unlock collaborative Cluster where random invididuals can collaborate in replicating content.\nWe also have a few new features:\n Path resolving before pinning and unpinning Ability to manually specify pin allocations Environment variable override to all configuration options Added the possibility to store custom metadata with all pins  Finally, the Cluster team would like to thank @alekswn and @roignpar for their awesome contributions!\nBe sure to check the changelog for a detailed overview of changes and upgrade notices.\nHappy pinning!\n"
},
{
	"uri": "https://ipfscluster.io/news/0.9.0_release/",
	"title": "Release 0.9.0",
	"tags": [],
	"description": "",
	"content": " 20190218 | Release 0.9.0 IPFS Cluster version 0.9.0 comes with one big new feature, OpenCensus support! This allows for the collection of distributed traces and metrics from the IPFS Cluster application as well as supporting libraries. Currently, we support the use of Jaeger as the tracing backend and Prometheus as the metrics backend. Support for other OpenCensus backends will be added as requested by the community. Please file an issue if you would like to see a particular backend supported. We are looking forward to digging deeper into how IPFS Cluster peers operate and communicate with each other and accurately measuring how they are performing in real world deployments.\nThe one other significant change that comes with the 0.9.0 release is the removal of the Snap distribution of IPFS Cluster. Due to difficulties in getting Snap builds to work reliably without a disproportionate amount of time spent debugging them, we decided to deprecate the distribution mechanism.\nHappy Measured Pinning!\n"
},
{
	"uri": "https://ipfscluster.io/news/0.8.0_release/",
	"title": "Release 0.8.0",
	"tags": [],
	"description": "",
	"content": " 20190116 | Release 0.8.0 Since the beginning of IPFS Cluster, one of our ideas was that it should be easily dropped in place of the IPFS daemon in any integration. This was achieved by adding an IPFS Proxy endpoint which essentially provides an IPFS-compatible API for Cluster. Those endpoints and operations which does not make sense to be handled by Cluster are simply forwarded to the underlying daemon. Add, pin and unpin operations become, however, Cluster action.\nIPFS Cluster 0.8.0 comes out today and includes a revamp of the IPFS Proxy endpoint. We have promoted it to be its own API-type component, extracting it from the IPFS Connector (which is just the client to IPFS). We have additionally made improvements so that it truly mimics IPFS, by dynamically extracting headers from the real daemon that can be re-used in the responses handled by Cluster. Thus, there will be no CORS-related breakage when swapping out IPFS for Cluster, and custom IPFS headers (i.e. X-Ipfs-Gateway) can be configured and forwarded by the proxy.\nThe increasing importance of browser integrations prompted us to fully support Cross-Origin Resource Sharing (CORS) in the REST API as well. It will now handle CORS pre-flight requests (OPTIONS) and the configuration allows the user to set up all the CORS-related headers as needed.\nThis is the first release with @kishansagathiya as full-time member of the team. Apart from extracting the IPFS proxy component, Kishan is behind the new --filter flag for ipfs-cluster-ctl status. You can now list all the items that are pinning or in error without the need of a complex grep line. A few more useful features will be coming up in the future.\nFor the full list of changes and update notices for this release, check out the changelog.\nHappy pinning!\n"
},
{
	"uri": "https://ipfscluster.io/news/0.7.0_release/",
	"title": "Release 0.7.0",
	"tags": [],
	"description": "",
	"content": " 20181031 | Release 0.7.0 We are proud to introduce the 0.7.0 release today. It comes with a few small improvements and bugfixes.\nWe have slightly changed the /add endpoint response format in a non-compatible way, to return more adequate objects than the ones mimic-ing the IPFS API. It\u0026rsquo;s not the best but, better now than later.\nWe have also fixed the proxy /add endpoint to work correctly with the IPFS Companion extension and js-ipfs-api. Thanks to @lidel for helping figuring out the problem!\nRegarding features, @kishansagathiya has been making a few contributions lately and now, among other features, we have new commands like ipfs-cluster-ctl health metrics freespace which show the list of last received freespace metrics and their validity.\nFinally, we have included a default docker-compose.yml template, which launches a stack with 2 ipfs daemons and 2 cluster peers.\nAs usual, for the full list of changes and update notices, check out the changelog.\n"
},
{
	"uri": "https://ipfscluster.io/news/20181003_0.6.0_release/",
	"title": "Release 0.6.0",
	"tags": [],
	"description": "",
	"content": " 20181003 | Release 0.6.0 | @hsanjuan We are publishing the first of IPFS Cluster 0.6.0 series today. After the large amount of code in 0.5.0, we expect to regain a bit of cadency and push out changes and improvements more often.\nAll peers from the 0.6.x series will be able to interact among each others. That essentially means that we will keep the internal RPC API compatible and that you will not need to upgrade all your peers at the same time. This change comes as Cluster APIs become more mature.\nOn the engineering front, this quarter we will be mostly working towards:\n prototyping a replacement for Cluster\u0026rsquo;s consensus component (currently uses Raft) gaining insights into the system with metrics and tracing performing the last steps to get cluster sharding working  For the moment, check out the changelog for a full list of the changes in this release. And big thanks to all the contributors: @lanzafame, @meiqimichelle, @kishansagathiya, @cannium, @jglukasik and @mike-ngu and others.\n"
},
{
	"uri": "https://ipfscluster.io/news/20180824_0.5.0_release/",
	"title": "Release 0.5.0",
	"tags": [],
	"description": "",
	"content": " 20180823 | Release 0.5.0: adding content with IPFS Cluster | @hsanjuan The new version of IPFS Cluster comes with thousands of lines of new code which implement content adding and replication to IPFS using IPFS Cluster.\nThat means that we know have an ipfs-cluster-ctl add command that, just like its ipfs add causing, can chunk and turn files and folders into Direct-Acyclic-Grapgs (DAGs), identified by a Content ID (CID), which is returned to the user when the adding process is completed.\nIPFS Cluster performs the DAG building in an equivalent way to ipfs, but stores the results different. While ipfs sends the resulting blocks directly to disk, IPFS Cluster allocates Cluster peers for the content and forwards the blocks to them (as they are generated). When the content has been added, it is then pinned in those allocations. Since the content is available locally in the allocations, pinning is a quicker step.\nThus adding to Cluster allows replicating and pushing content to multiple IPFS daemons. Additionally, you can take advantage of the Cluster API features when adding content to IPFS: basic authentication and secure channels (either via HTTPS or via libp2p-http). For example, if you run a cluster and want to add something to it, you can do it all in a single command:\nipfs-cluster-ctl --host /dnsaddr/my.cluster.domain --basic-auth user:pw add --rmin 2 --rmax 2 --name \u0026quot;my stuff\u0026quot; --recursive myFolder/\nBut add is not the only feature in the 0.5.0. We have also added a new experimental PinTracker stateless implementation which uses less memory (see last post by @lanzafame) and enabled DHT-routing so that Cluster peers can auto-discover other peers.\nCheck out the changelog for a full list of changes and important notices if you are upgrading.\nFinally, big thanks to all the contributors to this release: @lanzafame, @zenground0, @laevos and @whilei.\n"
},
{
	"uri": "https://ipfscluster.io/news/cluster_rpc_components/",
	"title": "RPC Components in IPFS Cluster",
	"tags": [],
	"description": "",
	"content": " RPC Components in IPFS Cluster | @hsanjuan In this post, I would like to perform a deep dive into one of the architectural features of IPFS Cluster which has turned out to be extremely useful for building a distributed application on top of libp2p: Components.\nCluster Components are modules that implement different parts of Cluster. For example, the restapi module implements the HTTP server that provides the API to interact with a Cluster peer. The ipfshttp module implements functionality to interact with the IPFS daemon. In the following diagram, we can see the multiple components that make up an IPFS Cluster peer:\n\nWhen I started developing IPFS Cluster, some things were very clear:\n We would need modules with fixed interfaces for the very distinct functionality areas The implementations of these modules would likely need not only to be improved over time, but probably be fully replaced by alternative implementations. For example, perhaps we would have to write a different API module in addition to the original one. Unlike a traditional application, where all your objects/modules/classes are at hand as part of the running process, the modules in a distributed application like Cluster would likely need to easily interact with other peers, running in a different computer.  The approach to the first two items in that list is rather obvious and consists in dividing splitting the functionality in modules and giving them the right interfaces, something which the Go language supports very well. The last item however, prompted me to put RPC at the center of the design for every Cluster peer and every module playing a part in it. This meant:\n Creating an internal RPC API, that every Cluster peer offers, exposing all of the functionality provided by the modules. Defining a base Component interface that every module would need to implement and which makes them RPC-powered (with an RPC client), as I\u0026rsquo;ll explain below.  These RPC-enabled modules became what we call Components and their particularlity is being RPC-first: any functionality not belonging to the component itself is accessed via RPC, even if it belongs to the same peer.\nThe main Component (called the Cluster component), ties all components together. It is in charge of running the RPC server for the Cluster peer and making an RPC client available to all components.\nThus, whenever a component needs to make use of functionality offered by its peer, or by another peer, the approach is the same (pseudocode):\nrpcClient.Call(\u0026lt;peerID\u0026gt;, \u0026lt;method\u0026gt;, \u0026lt;argument\u0026gt;, \u0026lt;response\u0026gt;)  This may seem counter intuitive at first. For example, when the restapi component receives a POST /pins/\u0026lt;cid\u0026gt;, it doesn\u0026rsquo;t have access to the Cluster.Pin() method offered by the main component directly. Instead, it needs to rpcClient.Call(\u0026quot;\u0026lt;localPeerID\u0026gt;\u0026quot;, \u0026quot;Pin\u0026quot;, cid, nil).\nHowever, it soon became clear that this becomes really convenient when needing to orchestrate actions on several Cluster peers, as shown by several examples from the code base:\n Broadcasting an action to the whole suddenly becomes extremely natural. For example, the Peers() method (peers ls) action is just a parallel call to the ID() exposed by every peer via RPC. The response is an array of the individual answers provided by each peer. Redirecting an action to the right actor becomes totally transparent. For example, our raft layer (Consensus component), needs to redirect all write actions to the Raft Leader. When a peer is performing one of these actions and sees the leader is a different peer, it just uses RPC to trigger the same method in the right peer. There is no cumbersome code overhead when it comes to performing actions anywhere in the Cluster vs. performing them locally. For example, when adding content through Cluster, the peer receiving the upload will chunk the content and then will send the resulting blocks directly to the IPFSConnector.BlockPut method in the peers allocated to receive that content.  RPC-first means that any submodule has full access to all of the Cluster functionality anywhere in the Cluster, for free (or almost, as we\u0026rsquo;ll see below). As such, doing things anywhere in the Cluster is as natural as doing them in the local peer.\nThe RPC library: go-libp2p-gorpc Every IPFS Cluster peer runs on top of it\u0026rsquo;s own libp2p Host, and exposes different services on it. Because libp2p multiplexes several streams onto the same connection, we can run dozens of different services and expose them on the same socket. libp2p facilitates things a lot: we do not need to do connection management, nor worry about closing and opening [tcp] connections (a very expensive operation), nor even knowing in which IP or port our desntination peer is, and all our communications occur encrypted and under a private network.\nThe RPC server, which receives RPC requests and sends the call to the appropriate Component and Method, is one of those services.\nWe use go-libp2p-gorpc as our RPC library. It started as a libp2p-powered clone of Go\u0026rsquo;s original net/rpc and, while it has evolved a little bit, it still remains a very simple and easy to use module. It cannot compete with gRPC in terms of functionality, but it certainly represents a reduced overhead (no protobuffers generation) along with some helpers that come very handy.\nThe most useful of these helpers is the local-server-shortcut. Making remote RPC calls is not cheap as the request and response need to travel on the wire. Local RPC calls are better, but libp2p Hosts cannot open streams to themselves, and if they could it would still add some lag. In order to have super fast local calls, go-libp2p-rpc allows initializing the RPC Client with a Server. When the destination peer ID is empty (\u0026quot;\u0026quot;), or matches the local libp2p host, the server method for that call is invoked directly. This saves us the need to serialize the arguments, write the request, read the response from the connection and deserialize it (even if it was a local call). Cluster components calling RPC methods on the local peer benefit from the RPC layer without suffering a large performance penalty.\nAnother trick is that Client also offers a MultiCall method, which facilitates making the same request to several destinations in parallel. All calls take contexts which, upon cancelled, automatically cancel the context used to call the RPC methods on the server side.\nComponent testing One of the advantages of using RPC for all inter-component communication is that we can very easily isolate the components for testing, by simply creating partial RPC server mocks which implement just the functionality required by the component (or the test).\nFor example, our maptracker module, an implementation of the PinTracker component, uses a custom RPC server implementation which provides the IPFSPin method. Depending on what is pinned, this server will simulate that IPFS takes a very long time to pin something, or that the request has been cancelled, or simply that the item gets pinned very quickly. Thus we can correctly test things like cancelling ongoing IPFS Pin requests when, for example, an Unpin request is received.\nMocks are always an option when testing, specially when Go code makes correct use of interfaces, but in the case of the RPC we don\u0026rsquo;t need to mock every method of the RPC API (as we would have to do if we were creating a mock implementation to satisfy an interface), but just those which we are going to use. Even so, many tests benefit from a common dummy RPC server implementation.\nRe-implementing components The Component architecture has made it easy to provide alternative or replacement implementations for components. One of the examples is the PinTracker component, which triggers, cancels and tracks errors from requests to the IPFSConnector component as new pins are tracked in the system.\nThis component had a maptracker implementation, which stores all the information in memory. We recently added an stateless implementation which relies on the IPFS daemon pinset as well as the shared state to piece together the PinTracker state, keeping track only of errors, and thus reducing memory usage with really big pinsets.\nWe also re-implemented the PeerMonitor component, using Pubsub instead of RPC calls to broadcast peer metrics.\nBecause components share the same RPC API and must implement the same interfaces, different peers in a Cluster may potentially run with different implementations of the same component.\nA final note and future path for improvement The Cluster Component architecture has resulted very handy in building IPFS Cluster and currently saves a lot of effort when approaching the implementation of new functionality that requires coordinated actions among several peers. However, using this RPC-first approach also comes with some downsides.\nThe first downside is the need to be able to serialize objects and responses used in RPC methods. This means, that every argument and every response provided in RPC methods needs to be translatable to something that can be unmarshaled into the same object type on the other side. This is a common requirement for all RPC protocols, but since we need to use the RPC API for local calls too (even with the local-call-shortcut explained above), we suffer a penalty translating objects to their serializable versions. I.e. a *cid.Cid type cannot be sent as such, so we make it a string first. This also implies designing APIs types with this limitations in mind. Depending on the type and the size of data, making a type serializable might take way longer time that it would take to just pass a pointer to a local method call. [UPDATE: As of v0.10.0 Cluster all internal types are serializable directly, so we do not need to make any copies anymore].\nThe go-libp2p-gorpc library puts some hard constraints on how the RPC server methods look like (1 input and 1 output argument). It does not require definitions or code pre-generate code like gRPC, but on the other side, it becomes easier to introduce bad runtime errors, for example, calling a method with different types than expected.\nThe use of RPC-for-all also needs to be accompanied by considerations on security. Currently, we rely on libp2p private network\u0026rsquo;s pre-shared-key to protect all internal libp2p services. But in the future we will have to explore additional authorization methods, probably associated at each request type. The good thing about libp2p is that the requests are directly authenticated by the peer ID originating them, so that side of things is solved for us. [UPDATE: go-libp2p-gorpc now offers per-method authorization support].\nFinally, if you\u0026rsquo;re planning to transmit large amounts of data over RPC you will need an RPC layer that supports streaming (hopefully in a duplex fashion for request and responses). This is something that we\u0026rsquo;ll likely end up adding to go-libp2p-gorpc.\nIf you read down here, I hope you now have a better idea of how different parts of your Cluster peers interact. I will be making regular posts on other parts of the codebase. In the meantime, if you have any questions, feel free to ask in our Discourse forum.\n"
},
{
	"uri": "https://ipfscluster.io/news/20180615_pintracker_revamp/",
	"title": "PinTracker revamp",
	"tags": [],
	"description": "",
	"content": " 20180615 | PinTracker Revamp | @lanzafame In this post, I am going to cover how the IPFS Cluster\u0026rsquo;s PinTracker component used to work, what some of the issues with that implementation were, how we fixed them, and where to go next.\nHow the PinTracker worked First, the purpose of the pintracker: the pintracker serves the role of ferrying the appropriate state from IPFS Cluster\u0026rsquo;s shared state to a peer\u0026rsquo;s ipfs daemon state.\nHow this occurs is as follows: - IPFS Cluster receives a request to Pin a particular Cid - this request is routed to the consensus component, where it is stored in the distributed log and then an RPC call to Track the Cid is made to the pintracker - the pintracker then creates and stores the PinInfo in an internal map, before making a Pin request to the IPFS node via an RPC call to the IPFSConnector component - the IPFSConnector component is what finally requests the ipfs daemon to pin the Cid\nIssues that we faced The issues are separated into those which were due to how we initially implemented the MapPinTracker and then those that were/are inherent in any implementation of the pintracker that uses a map internally to store the status of the pins.\nIssues with the implementation:\n the local state would get stale and required repeated syncing race conditions inability to cancel ongoing operations  resulted in unnecessary requests to \u0026lsquo;undo\u0026rsquo; requests that couldn\u0026rsquo;t be cancelled  no way of knowing how many requests were currently in flight/queued for a single Cid  Issues with a MapPinTracker:\n the local state (the map inside the pintracker) is potentially too large to keep for large clusters unnecessary duplication of PinInfo in the pintracker component  How we fixed it To tackle the issues with current implementation of the MapPinTracker, we did the following things.\nWe moved to a model where we track in-flight operations, so instead of a map that stored the status of a Cid, i.e. map[Cid]PinInfo, we now store an operation for a Cid, i.e. map[Cid]Operation.\nNow, an Operation contains not only the type of operation that is being performed for a Cid, (Pin or Unpin), and the phase of the operation, (Queued, In Progress, Done, or Error), but a context.Context.\nWith the addition of context propagation through RPC calls to the IPFSConnector component, having a context available in every Operation gives us the ability to cancel an operation at any point.\nAlso upon receiving opposing operations for the same Cid we can cancel the in-flight operation automatically, maybe even before that operation had started to be processed depending on the timing.\nWith the increased visibility into the queue of operations that have been requested and the ability to cancel operations, the potential of the local state getting out of sync has greatly decreased. This means that cluster.StateSync doesn\u0026rsquo;t need to be called every 60 seconds anymore to guarantee consistency. Also, Recover is now async as the queue of operations is no longer a blackbox.\nWhere to go next Currently in PR, there is a stateless implementation of the pintracker interface. This implementation removes the duplication of state and potential for stale PinInfos in the pintracker itself. The stateless pintracker relies directly on the shared state provided by the consensus component and the state provided by the ipfs node. The main benefit is for clusters with a very large number of pins, as the status of all those pins will not be held in memory.\n"
},
{
	"uri": "https://ipfscluster.io/news/20180604_0.4.0_release/",
	"title": "Release 0.4.0",
	"tags": [],
	"description": "",
	"content": " 20180604 | Release 0.4.0 | @hsanjuan Last week we released IPFS Cluster v0.4.0.\nWe bumped the minor version number to make explicit not only that we brought on a lot of new things, but that this release also included a number of breaking changes, mostly in regards to the configuration file.\nThe changelog gives an overview of what\u0026rsquo;s new (a lot of things!), but we will also take time to explore and explain the changes in more detail right in this news section, in separate upcoming entries.\nFor the moment, this release is a solid milestone and provides essential features and fixes for production deployments. It is so far running very smoothly in our storage cluster. This website and the documentation pages, on which we have put significant efforts, should help users install, configure, deploy and tune their Clusters. If you encounter a problem or need help, just reach out!\n"
},
{
	"uri": "https://ipfscluster.io/news/captains_log/",
	"title": "Old captain&#39;s log",
	"tags": [],
	"description": "",
	"content": " 20180329 | @hsanjuan In the last two months many things have happened in the IPFS Cluster project.\nFirst, we have welcomed a new team member: @lanzafame has already started contributing and has resolved a few issues already included in the last release.\nSecondly, we have been working very hard on implementing the \u0026ldquo;sharding RFC\u0026rdquo; that I mentioned in my last update. @zenground0 has made very significant progress on this front. Sharding will be a unique feature of IPFS Cluster and will help to drive the adoption of ipfs by being able tu support huge datasets distributed among different nodes. We hope that the first \u0026ldquo;sharding\u0026rdquo; prototype is ready in the upcoming weeks.\nThirdly, we have made 3 releases (the latest being 0.3.5) which bring a diverse set of features and some bugfixes. Some of the major ones are these:\n ipfs-cluster-ctl health graph generates a .dot file which allows to quickly have an overview of connectivity among the peers in the cluster. The refs pinning method allows to download dags in parallel and pin only when they content is already on the disk. The Rest API now exposes the HTTP endpoints through libp2p. By using a libp2p host to communicate with it, users get an encrypted connection without having to setup SSL certificates.  We have also started working on the IPFS Cluster website, which we will use to provide a central and well organized place for documentation, roadmaps and other information related to the project.\nHappy pinning!\n20180125 | @hsanjuan We are about to tag the 0.3.2 release and it comes with two nice features.\nOn one side, @zenground0 has been focused in implementing state offline export and import capabilities, a complement to the state upgrades added in the last release. They allow taking the shared from an offline cluster (and in a human readable format), and place it somewhere else, or in the same place. This feature might save the day in situations when the quorum of a cluster is completely lost and peers cannot be started anymore due to the lack of master.\nAdditionally, I have been putting some time into a new approach to replication factors. Instead of forcing cluster to store a pin an specific number of times, we now support a lower and upper bounds to the the replication factor (in the form of replication_factor_min and replication_factor_max). This feature (and great idea) was originally proposed by @segator.\nHaving this margin means that cluster will attempt its best when pinning an item (reach the max factor), but it won\u0026rsquo;t error if it cannot find enough available peers, as long as it finds more than the minimum replication factor.\nIn the same way, a peer going offline, will not trigger a re-allocation of the CID as it did before, if the replication factor is still within the margin. This allows, for example, taking a peer offline for maintenance, without having cluster vacate all the pins associated to it (and then coming up empty).\nOf course, the previous behaviour can still be obtained by setting both the max and the min to the same values.\nFinally, it is very important to remark that we recently finished the Sharding RFC draft. This document outlines how we are going to approach the implementation of one of the most difficult but important features upcoming in cluster: the ability to distribute a single CID (tree) among several nodes. This will allow to use cluster to store files or archives too big for a single ipfs node. Input from the community on this draft can be provided at https://github.com/ipfs/notes/issues/278.\n20171211 | @hsanjuan During the last weeks we\u0026rsquo;ve been working hard on making the first \u0026ldquo;live\u0026rdquo; deployment of IPFS Cluster. I am happy to announce that a 10-peer cluster runs on ipfs-gateway nodes, maintaining a \u0026gt;2000-length pinset.\nThe nodes are distributed, run a vanilla IPFS Cluster docker container mounting a volume with a customized cluster configuration, which uses higher-than-default timeouts and intervals. The injection of the pin-set took a while, but enventually every pin in every node became PINNED. In one occasion, a single IPFS node hanged while pinning. After re-starting the IPFS node in question, all pins in the queue became PIN_ERRORs, but they could easily be fixed with a recover operation.\nAdditionally, the IPFS IRC Pinbot now supports cluster-pinning, by using the IPFS Cluster proxy to ipfs, which intercepts pin requests and performs them in cluster. This allowed us to re-use the go-ipfs-api library to interact with cluster.\nThe first live setup has shown nevertheless that some things were missing. For example, we added --local flags to Sync, Status and Recover operations (and allowed a local RecoverAll). They are handy when a single node is at fault and you want to fix the pins on that specific node. We will also work on a go-ipfs-cluster-api library which provides a REST API client which allows to programmatically interact with cluster more easily.\nParallel to all this, @zenground0 has been working on state migrations. The cluster\u0026rsquo;s consensus state is stored on disk via snapshots in certain format. This format might evolve in the future and we need a way to migrate between versions without losing all the state data. In the new approach, we are able to extract the state from Raft snapshots, migrate it, and create a new snapshot with the new format so that the next time cluster starts everything works. This has been a complex feature but a very important step to providing a production grade release of IPFS Cluster.\nLast but not least, the next release will include useful things like pin-names (a string associated to every pin) and peer names. This will allow to easily identify pins and peers by other than their multihash. They have been contributed by @te0d, who is working on https://github.com/te0d/js-ipfs-cluster-api, a JS Rest API client for our REST API, and https://github.com/te0d/bunker, a web interface to manage IPFS Cluster.\n20171115 | @hsanjuan This update comes as our 0.3.0 release is about to be published. This release includes quite a few bug fixes, but the main change is the upgrade of the underlying Raft libraries to a recently published version.\nRaft 1.0.0 hardens the management of peersets and makes it more difficult to arrive to situations in which cluster peers have different, inconsistent states. These issues are usually very confusing for new users, as they manifest themselves with lots of error messages with apparently cryptic meanings, coming from Raft and libp2p. We have embraced the new safeguards and made documentation and code changes to stress the workflows that should be followed when altering the cluster peerset. These can be summarized with:\n --bootstrap is the method to add a peer to a running cluster as it ensures that no diverging state exists during first boot. The ipfs-cluster-data folder is renamed whenever a peer leaves the cluster, resulting on a clean state for the next start. Peers with a dirty state will not be able to join a cluster. Whenever ipfs-cluster-data has been initialized, cluster.peers should match the internal peerset from the data, or the node will not start.  In the documentation, we have stressed the importance of the consensus data and described the workflows for starting peers and leaving the cluster in more detail.\nI\u0026rsquo;m also happy to announce that we now build and publish \u0026ldquo;snaps\u0026rdquo;. Snaps are \u0026ldquo;universal Linux packages designed to be secure, sandboxed, containerised applications isolated from the underlying system and from other applications\u0026rdquo;. We are still testing them. For the moment we publish a new snap on every master build.\nYou are welcome to check the changelog for a detailed list of other new features and bugfixes.\nOur upcoming work will be focused on setting up a live IPFS Cluster and run it in a \u0026ldquo;production\u0026rdquo; fashion, as well as adding more capabilities to manage the internal cluster state while offline (migrate, export, import) etc.\n20171023 | @hsanjuan We have now started the final quarter of 2017 with renewed energy and plans for IPFS Cluster. The team has grown and come up with a set of priorities for the next weeks and months. The gist of these is:\n To make cluster stable and run it on production ourselves To start looking into the handling of \u0026ldquo;big datasets\u0026rdquo;, including IPLD integration To provide users with a delightful experience with a focus in documentation and support  The v0.2.0 marks the start of this cycle and includes. Check the changelog for a list of features and bugfixes. Among them, the new configuration options in the consensus component options will allow our users to experiment in environments with larger latencies than usual.\nFinally, coming up in the pipeline we have:\n the upgrade of Raft library to v1.0.0, which is likely to provide a much better experience with dynamic-membership clusters. Swagger documentation for the Rest API. Work on connectivity graphs, allowing to easily spot any connectivity problem among cluster peers.  20170726 | @hsanjuan Unfortunately, I have not thought of updating the Captain\u0026rsquo;s log for some months. The Coinlist effort has had me very busy, which means that my time and mind were not fully focused on cluster as before. That said, there has been significant progress during this period. Much of that progress has happened thanks to @Zenground0 and @dgrisham, who have been working on cluster for most of Q2 making valuable contributions (many of them on the testing front).\nAs a summary, since my last update, we have:\n A guide to running IPFS Cluster, with detailed information on how cluster works, what behaviours to expect and why. It should answer many questions which are not covered by the getting-started-quickly guides. Added sharness tests, which make sure that Ã¬pfs-cluster-ctl and ipfs-cluster-service are tested and not broken in obvious ways at least and complement our testing pipeline. Pushed the kubernetes-ipfs project great lengths, adding a lot of features to its DSL and a bunch of highly advanced IPFS Cluster tests. The goal is to be able to test deployments layouts which are closer to reality, including escalability tests. The extra tests uncovered and allowed us to fix a number of nasty bugs, usually around the IPFS Cluster behaviour when peers go down or stop responding. Added CID re-allocation on peer removal. Added \u0026ldquo;Private Networks\u0026rdquo; support to IPFS Cluster. Private Networks is a libp2p feature which allows to secure a libp2p connection with a key. This means that inter-peer communication is now protected and isolated with a cluster_secret. This brings a significant reduction on the security pitfalls of running IPFS Cluster: default setup does not allow anymore remote control of a cluster peer. More information on security can be read on the guide. Added HTTPs support for the REST API endpoint. This facilitates exposing the API endpoints directly and is a necessary preamble to supporting basic authentication (in the works).  All the above changes are about to crystallize in the v0.1.0 release, which we\u0026rsquo;ll publish in the next days.\n20170328 | @hsanjuan The last weeks were spent on improving go-ipfs/libp2p/multiformats documentation as part of the documentation sprint mentioned earlier.\nThat said, a few changes have made it to IPFS Cluster:\n All components have now been converted into submodules. This clarifies the project layout and actually makes the component borders explicit. Increase pin performance. By using type=recursive in IPFS API queries they return way faster. Connect different ipfs nodes in the cluster: we now trigger swarm connect operations for each ipfs node associated to a cluster peer, both at start up and upon operations like peer add. This should ensure that ipfs nodes in the cluster know each others. Add disk informer. The default allocation strategy now is based on how big the IPFS repository is. Pins will be allocated to peers with lower repository sizes.  I will be releasing new builds/release for IPFS Cluster in the following days.\n20170310 | @hsanjuan This week has been mostly spent on making IPFS Cluster easy to install, writing end-to-end tests as part of the Test Lab Sprint and bugfixing:\n There is now an IPFS Cluster docker container, which should be part of the ipfs docker hub very soon IPFS Cluster builds are about to appear in dist.ipfs.io I shall be publishing some Ansible roles to deploy ipfs+IPFS Cluster There are now some tests using kubernetes-ipfswith new docker container. These tests are the first automated tests that are truly end-to-end, using a real IPFS-daemon under the hood. I have added replication-factor-per-pin support. Which means that for every pinned item, it can be specified what it\u0026rsquo;s replication factor should be, and this factor can be updated. This allows to override the global configuration option for replication factor. Bugfixes: one affecting re-pinning+replication and some others in ipfs-cluster-ctl output.  Next week will probably focus on the Delightful documentation sprint. I\u0026rsquo;ll try to throw in some more tests for ipfs-cluster-ctl and will send the call for early testers that I was talking about in the last update, now that we have new multiple install options.\n20170302 | @hsanjuan IPFS cluster now has basic peer monitoring and re-pinning support when a cluster peer goes down.\nThis is done by broadcasting a \u0026ldquo;ping\u0026rdquo; from each peer to the monitor component. When it detects no pings are arriving from a current cluster member, it triggers an alert, which makes cluster trigger re-pins for all the CIDs associated to that peer.\nThe next days will be spent fixing small things and figuring out how to get better tests as part of the Test Lab Sprint. I also plan to make a call for early testers, to see if we can get some people on board to try IPFS Cluster out.\n20170215 | @hsanjuan A global replication factor is now supported! A new configuration file option replication_factor allows to specify how many peers should be allocated to pin a CID. -1 means \u0026ldquo;Pin everywhere\u0026rdquo;, and maintains compatibility with the previous behaviour. A replication factor \u0026gt;= 1 pin request is subject to a number of requirements:\n It needs to not be allocated already. If it is the pin will return with an error saying so. It needs to find enough peers to pin.  How the peers are allocated content has been most of the work in this feature. We have two three components for doing so:\n An Informer component. Informer is used to fetch some metric (agnostic to Cluster). The metric has a Time-to-Live and it is pushed in TTL/2 intervals to the Cluster leader. An Allocator component. The allocator is used to provide an Allocate() method which, given current allocations, candidate peers and the last valid metrics pushed from the Informers, can decide which peers should perform the pinning. For example, a metric could be the used disk space in a cluster peer, and the allocation algorithm would be to sort candidate peers according to that metrics. The first in the list are the ones with less disk used, and will then be chosen to perform the pin. An Allocator could also work by receiving a location metric and making sure that the most preferential location is different from the already existing ones etc. A PeerMonitor component, which is in charge of logging metrics and providing the last valid ones. It will be extended in the future to detect peer failures and trigger alerts.  The current allocation strategy is a simple one called numpin, which just distributes the pins according to the number of CIDs peers are already pinning. More useful strategies should come in the future (help wanted!).\nThe next steps in Cluster will be wrapping up this milestone with failure detection and re-balancing.\n20170208 | @hsanjuan So much for commitments\u0026hellip; I missed last friday\u0026rsquo;s log entry. The reason is that I was busy with the implementation of dynamic membership for IPFS Cluster.\nWhat seemed a rather simple task turned into a not so simple endeavour because modifying the peer set of Raft has a lot of pitfalls. This is specially if it is during boot (in order to bootstrap). A peer add operation implies making everyone aware of a new peer. In Raft this is achieved by committing a special log entry. However there is no way to notify of such event on a receiver, and such entry only has the peer ID, not the full multiaddress of the new peer (needed so that other nodes can talk to it).\nTherefore whoever adds the node must additionally broadcast the new node and also send back the full list of cluster peers to it. After three implementation attempts (all working but all improving on top of the previous), we perform this broadcasting by logging our own PeerAdd operation in Raft, with the multiaddress. This proved nicer and simpler than broadcasting to all the nodes (mostly on dealing with failures and errors - what do when a node has missed out). If the operation makes it to the log then everyone should get it, and if not, failure does not involve un-doing the operation in every node with another broadcast. The whole thing is still tricky when joining peers which have disjoint Raft states, so it is best to use it with clean, just started peers.\nSame as peer add, there is a join operation which facilitates bootstrapping a node and have it directly join a cluster. On shut down, each node will save the current cluster peers in the configuration for future use. A join operation can be triggered with the --bootstrap flag in ipfs-cluster-service or with the bootstrap option in the configuration and works best with clean nodes.\nThe next days will be spent on implementing replication factors, which implies the addition of new components to the mix.\n20170127 | @hsanjuan Friday is from now on the Captain Log entry day.\nLast week, was the first week out of three the current IPFS Cluster sprintino (https://github.com/ipfs/pm/issues/353). The work has focused on addressing \u0026ldquo;rough edges\u0026rdquo;, most of which came from @jbenet\u0026rsquo;s feedback (#14). The result has been significant changes and improvements to IPFS Cluster:\n I finally nailed down the use of multicodecs in go-libp2p-raft and go-libp2p-gorpc and the whole dependency tree is now Gx\u0026rsquo;ed. It seems for the moment we have settled for ipfs-cluster-service and ipfs-cluster-ctl as names for the cluster tools. Configuration file has been overhauled. It now has explicit configuration key names and a stronger parser which will be more specific on the causes of error. CLI tools have been rewritten to use urfave/cli, which means better help, clearer commands and more consistency. The Sync() operations, which update the Cluster pin states from the IPFS state have been rewritten. Recover() has been promoted to its own endpoint. I have added ID() endpoint which provides information about the Cluster peer (ID, Addresses) and about the IPFS daemon it\u0026rsquo;s connected to. The Peers() endpoint retrieves this information from all Peers so it is easy to have a general overview of the Cluster. The IPFS proxy is now intercepting pin add, pin rm and pin ls and doing Cluster pinning operations instead. This not only allows replacing an IPFS daemon by a Cluster peer, but also enables compositing cluster peers with other clusters (pointing ipfs_node_multiaddress to a different Cluster proxy endpoint).  The changes above include a large number of API renamings, re-writings and re-organization of the code, but IPFS Cluster has grown more solid as a result.\nNext week, the work will focus on making it easy to add and remove peers from a running cluster.\n20170123 | @hsanjuan I have just merged the initial cluster version into master. There are many rough edges to address, and significant changes to namings/APIs will happen during the next few days and weeks.\nThe rest of the quarter will be focused on 4 main issues:\n Simplify the process of adding and removing cluster peers Implement a replication-factor-based pinning strategy Generate real end to end tests Make IPFS Cluster stable  These endaevours will be reflected on the ROADMAP.\n"
},
{
	"uri": "https://ipfscluster.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://ipfscluster.io/",
	"title": "IPFS Cluster",
	"tags": [],
	"description": "",
	"content": "IPFS has given the users the power of content-addressed storage. The permanent web requires, however, a data redundancy and availability solution that does not compromise on the distributed nature of the IPFS Network.\nIPFS Cluster is a distributed application that works as a sidecar to IPFS peers, maintaining a global cluster pinset and intelligently allocating its items to the IPFS peers. IPFS Cluster powers large IPFS storage services like nft.storage and web3.storage:\n An easy to run application: ipfs-cluster-service runs as an independent daemon, independent from IPFS and interacting with the IPFS daemon\u0026rsquo;s API. Handle replication of millions of pins to hundreds of IPFS daemons in a \u0026ldquo;fire \u0026amp; forget\u0026rdquo; fashion: pin lifetime tracked asynchronously, the Cluster peers take care of asking IPFS to pin things at a sustainable rate and retry pinning in case of failures. Ingest pins at scale: Pins can be added at a very high rate (hundreds per second at least) into the cluster. From that moment they are tracked and managed by the cluster peers. Clever prioritization: New pins are prioritized over pin requests that are old or have repeatedly failed to pin. Balanced allocation: distribute pins evenly among peers in different groups and subgroups (i.e region, availability zone)\u0026hellip; ultimately choosing those with most free storage space available. Fully featured API and CLI: ipfs-cluster-ctl provides a command-line client to the fully featured Cluster HTTP REST API. No central server: cluster peers form a distributed network and maintain a global, replicated and conflict-free list of pins. Baked-in permissions: an embedded permission model supports standard peers (with permissions to change the cluster pinset) and follower peers (which store content as instructed but cannot modify the pinset). Name your pins: every pin supports custom replication factors, name and any other custom metadata. Multi-peer add: Ingest IPFS content to multiple daemons directly. CAR import support: import CAR-archived content with custom DAGs directly to the Cluster. A drop-in to any IPFS integration: each cluster peer provides an additional IPFS proxy API which performs cluster actions but behaves exactly like the IPFS daemon\u0026rsquo;s API does. Integration-ready: Written in Go, Cluster peers can be programmatically launched and controlled. The IPFS Cluster additionally provides Go and Javascript clients for its API. libp2p powered: IPFS Cluster is built on libp2p, the battle-tested next generation p2p networking library powering IPFS, Filecoin and Ethereum V2.  "
},
{
	"uri": "https://ipfscluster.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]