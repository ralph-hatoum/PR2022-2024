<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>IPFS Cluster on </title>
    <link>https://ipfscluster.io/</link>
    <description>Recent content in IPFS Cluster on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Mar 2022 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ipfscluster.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Architecture overview</title>
      <link>https://ipfscluster.io/documentation/deployment/architecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/deployment/architecture/</guid>
      <description>Architecture overview Before jumping in to install and run IPFS Cluster, it is important to clarify some basic concepts.
The IPFS Cluster software consists of three binary files:
 ipfs-cluster-service runs a Cluster peer (similar to ipfs daemon) using a configuration file and by storing some information on disk. ipfs-cluster-ctl is used to communicate with a Cluster peer and perform actions such as pinning IPFS CIDs to the Cluster. ipfs-cluster-follow runs a follower Cluster peer.</description>
    </item>
    
    <item>
      <title>Configuration</title>
      <link>https://ipfscluster.io/documentation/reference/configuration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/reference/configuration/</guid>
      <description>Configuration reference All IPFS Cluster configurations and persistent data can be found, by default, at the ~/.ipfs-cluster folder. For more information about the persistent data in this folder, see the Data, backups and recovery section.
ipfs-cluster-service -c &amp;lt;path&amp;gt; sets the location of the configuration folder. This is also controlled by the IPFS_CLUSTER_PATH environment variable. The ipfs-cluster-service program uses two main configuration files:
 service.json, containing the cluster peer configuration, usually identical in all cluster peers.</description>
    </item>
    
    <item>
      <title>Adding and pinning</title>
      <link>https://ipfscluster.io/documentation/guides/pinning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/guides/pinning/</guid>
      <description>Adding and pinning Cluster usage mostly consists of adding and removing pins. This is usually performed by using the ipfs-cluster-ctl utility or talking to one of the Cluster APIs.
You can get help and usage information for all ipfs-cluster-ctl commands with ipfs-cluster-ctl --help and ipfs-cluster-ctl &amp;lt;command&amp;gt; --help When working with a large number of pins, it is important to keep an eye on the state of the pinset, whether every pin is getting correctly pinned an allocated.</description>
    </item>
    
    <item>
      <title>Hosting a collaborative cluster</title>
      <link>https://ipfscluster.io/documentation/collaborative/setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/collaborative/setup/</guid>
      <description>Collaborative clusters setup In order to create your own collaborative cluster that other people can subscribe to you will need to:
 Setup your regular production deployment of IPFS Cluster in CRDT mode (these will be your &amp;ldquo;trusted peers&amp;rdquo;). Distribute a configuration template so that follower peers can easily join the cluster. Let users take advantage of ipfs-cluster-follow.  Trusted peers setup Collaborative clusters must use CRDT mode. The first step in setting a collaborative cluster is to deploy a regular CRDT cluster with one or more peers.</description>
    </item>
    
    <item>
      <title>REST API</title>
      <link>https://ipfscluster.io/documentation/reference/api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/reference/api/</guid>
      <description>REST API Reference IPFS Cluster peers include an API component which provides HTTP-based access to the peer&amp;rsquo;s functionality. The API attempts to be REST-ful in form and behaviour. It is enabled by default, but it can be disabled by removing its section from the service.json configuration file.
We do not maintain ad-hoc API documentation, as it gets easily out of date or, at worst, is innaccurate or buggy. Instead, we provide an easy way to find how to do what you need to do by using the ipfs-cluster-ctl command.</description>
    </item>
    
    <item>
      <title>Pinning Service API</title>
      <link>https://ipfscluster.io/documentation/reference/pinsvc_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/reference/pinsvc_api/</guid>
      <description>IPFS Pinning Service API IPFS Cluster peers include an API component which implements the IPFS Pinning Service API.
This API component is experimental and it attempts to have full compliance with the Spec, but it is important to keep in mind a number of caveats:
 IPFS Cluster does not support tracking the same CID multiple times (i.e. once per user). That means that pinning the same CID multiple times will always result in the same &amp;ldquo;request ID&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>Consensus components</title>
      <link>https://ipfscluster.io/documentation/guides/consensus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/guides/consensus/</guid>
      <description>Consensus components IPFS Cluster peers can be started using different choices for the implementations of some components. The most important one is the &amp;ldquo;consensus&amp;rdquo; one. The &amp;ldquo;consensus component&amp;rdquo; is in charge of:
 Managing the global cluster pinset by receiving modifications from other peers and publishing them. Managing the persistent storage of pinset-related data on disk. Achieving strong eventual consistency between all peers: all peers should converge to the same pinset.</description>
    </item>
    
    <item>
      <title>Datastore backends</title>
      <link>https://ipfscluster.io/documentation/guides/datastore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/guides/datastore/</guid>
      <description>Datastore backends IPFS Cluster supports several datastore backend options. The backend is a key-value store for all the persistent storage needed by a cluster peer. This includes the pinset information and also the CRDT-DAG blockstore etc.
Datastores are highly configurable, and configuration has an impact on performance, memory footprint and disk footprint. Different datastores will likely behave differently based on whether the underlying medium are SSDs or spinning disks. Depending on constraints, hardware used etc.</description>
    </item>
    
    <item>
      <title>Download and setup</title>
      <link>https://ipfscluster.io/documentation/deployment/setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/deployment/setup/</guid>
      <description>Download and installation In order to run an IPFS Cluster peer and perform actions on the Cluster, you will need to obtain the ipfs-cluster-service and ipfs-cluster-ctl binaries. The former runs the Cluster peer. The latter allows to interact with it:
 Visit the download page for instructions on the different ways to obtain ipfs-cluster-service and ipfs-cluster-ctl. Place the binaries in a place where they can be run unattended by an ipfs system user (usually /usr/local/bin).</description>
    </item>
    
    <item>
      <title>IPFS Proxy</title>
      <link>https://ipfscluster.io/documentation/reference/proxy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/reference/proxy/</guid>
      <description>IPFS Proxy The IPFS Proxy is an endpoint which presents the IPFS HTTP API in the following way:
 Some requests are intercepted and trigger cluster operations All non-intercepted requests are forwarded to the IPFS daemon attached to the cluster peer  This endpoint is enabled by default, and listens by default on /ip4/127.0.0.1/tcp/9095 and is provided by the ipfshttp connector component. It can be disabled by removing its section from the service.</description>
    </item>
    
    <item>
      <title>Joining a collaborative cluster</title>
      <link>https://ipfscluster.io/documentation/collaborative/joining/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/collaborative/joining/</guid>
      <description>Joining a collaborative cluster Collaborative clusters allow people to join forces to backup and distribute interesting content on the IPFS-network.
These clusters are formed by a group of trusted peers, which add and modify the list of items in the cluster, and a group of followers, which subscribe to the cluster and pin things accordingly.
Joining a cluster using ipfs-cluster-follow You should be able to join using a single oneliner:</description>
    </item>
    
    <item>
      <title>Release process</title>
      <link>https://ipfscluster.io/_release/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/_release/</guid>
      <description>Release process Tasks that need to happen for a release:
Preparation  Open release issue with the Release tag and mention all issues and tickets that will go on that release (https://github.com/ipfs-cluster/ipfs-cluster/issues/620). It helps backtrack when things happening from the issues. Write the changelog entry for the release (copy from previous) and commit to branch:  Summary of what&amp;rsquo;s happening in the release List of features, bugs Configuration changes and upgrades notices (note comment at the bottom of the file about how to write @issuelinks and replace them with sed)  PR to the website with all the necessary documentation changes  Special attention to documentation changes (/configuration section) Special attention to behaviour changes that are described somewhere in the docs Add an entry to news about the new release.</description>
    </item>
    
    <item>
      <title>Bootstrapping the Cluster</title>
      <link>https://ipfscluster.io/documentation/deployment/bootstrap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/deployment/bootstrap/</guid>
      <description>Bootstrapping the Cluster This section explains how to start a Cluster for the first time depending on the consensus choice (crdt or raft) made during initialization.
The first start of the Cluster is the most critical step during the lifetime. We must ensure that peers are able to contact each other (connectivity) and discard common configuration errors (like using different value for secret).
Starting a cluster peer is as easy as running:</description>
    </item>
    
    <item>
      <title>Security and ports</title>
      <link>https://ipfscluster.io/documentation/guides/security/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/guides/security/</guid>
      <description>Security and ports This section explores some security considerations when running IPFS Cluster.
There are four types of endpoints in IPFS Cluster to be taken into account when protecting access to the system. Exposing an unprotected endpoint might give anyone control of the cluster. Cluster configuration uses sensible defaults.
 The cluster secret The trusted_peers in CRDT mode Ports and endpoints overview  The cluster secret The 32-byte hex-encoded secret in the service.</description>
    </item>
    
    <item>
      <title>ipfs-cluster-ctl</title>
      <link>https://ipfscluster.io/documentation/reference/ctl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/reference/ctl/</guid>
      <description>ipfs-cluster-ctl The ipfs-cluster-ctl command line application is a user-friendly REST API client for IPFS Cluster. It allows to perform all the operations supported by a Cluster peer:
 Pinning Unpinning Adding Listing items in the pinset Checking the status of pins Listing cluster peers Removing peers  Usage Usage information can be obtained by running:
$ ipfs-cluster-ctl --help  You can also obtain command-specific help with ipfs-cluster-ctl help [cmd]. The (--host) can be used to talk to any remote cluster peer (localhost is used by default).</description>
    </item>
    
    <item>
      <title>Data, backups and recovery</title>
      <link>https://ipfscluster.io/documentation/guides/backups/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/guides/backups/</guid>
      <description>Data, backups, and recovery The configurations and data persisted by a running IPFS Cluster peer (with ipfs-cluster-service) is, by default, in the $HOME/.ipfs-cluster/ folder. A Cluster peer persists several types of information on disk:
 The list of known peer addresses for future use. Is stored in the peerstore file during shutdown. The cluster pinset (the list of objects that are pinned in the cluster along with all the options associated to them (like the name, the allocations or the replication factor) are stored depending on the consensus component chosen:  crdt stores everything in the chose key-value datastore backend (pebble, badger3, badger, leveldb folders).</description>
    </item>
    
    <item>
      <title>Deployment automations</title>
      <link>https://ipfscluster.io/documentation/deployment/automations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/deployment/automations/</guid>
      <description>Deployment automations We have a number of automations to facilitate configuring and deploying IPFS Clusters:
 Ansible roles Docker Kubernetes with Kustomize  Ansible roles Ansible roles for configuring and deploying ipfs-cluster-service, ipfs-cluster-ctl and go-ipfs (including templated configuration files) are available at https://github.com/hsanjuan/ansible-ipfs-cluster.
Docker IPFS Cluster provides official dockerized releases at https://hub.docker.com/r/ipfs/ipfs-cluster/ along with an example template for docker-compose. If you want to run one of the /ipfs/ipfs-cluster Docker containers, it is important to know that:</description>
    </item>
    
    <item>
      <title>ipfs-cluster-follow</title>
      <link>https://ipfscluster.io/documentation/reference/follow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/reference/follow/</guid>
      <description>ipfs-cluster-follow The ipfs-cluster-follow command line application is a user-friendly way of running follower peers to join collaborative IPFS Clusters. You can obtain more information about collaborative clusters in the respective section.
ipfs-cluster-follow runs an optimized cluster peer for use with collaborative cluster. It focuses on simplicity and security for users running follower peers, removing most of the configuration hassles that running a peer has.
Configuration ipfs-cluster-follow normally uses configurations distributed through the local IPFS gateway as templates.</description>
    </item>
    
    <item>
      <title>ipfs-cluster-service</title>
      <link>https://ipfscluster.io/documentation/reference/service/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/reference/service/</guid>
      <description>ipfs-cluster-service The ipfs-cluster-service is a command line application that runs a full cluster peer:
 ipfs-cluster-service init initializes configuration and identity. ipfs-cluster-service daemon launches a cluster peer. ipfs-cluster-service state allows to export, import, and cleanup the persistent state.  The ipfs-cluster-service provides its own help by running ipfs-cluster-service --help or ipfs-cluster-service &amp;lt;command&amp;gt; --help.
Debugging ipfs-cluster-service offers two debugging options:
 --debug enables debug logging from the ipfs-cluster, go-libp2p-raft and go-libp2p-rpc layers.</description>
    </item>
    
    <item>
      <title>Peerset management</title>
      <link>https://ipfscluster.io/documentation/guides/peerset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/guides/peerset/</guid>
      <description>Peerset management Adding and removing peers from the Cluster might be a simpler or trickier operation depending on the &amp;ldquo;consensus&amp;rdquo; component used by the cluster (the consensus component is in charge on managing the peerset).
Listing peers ipfs-cluster-ctl peers ls  The peers ls command will produce the list of peers in the cluster will all their information. It is the equivalent of calling ipfs-cluster-ctl id on every cluster peer and building a list with the results, but for it to work it needs to contact all the current peers of the cluster, meaning it can be a slow operation.</description>
    </item>
    
    <item>
      <title>Cluster pubsub metrics</title>
      <link>https://ipfscluster.io/documentation/guides/metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/guides/metrics/</guid>
      <description>Cluster pubsub metrics This section is about metrics broadcasted between cluster peers. For Prometheus metrics for monitoring see the monitoring guide. Cluster peers regularly broadcast (using gossipsub) metrics between each others. These metrics serve several purposes:
 They allow to detect when a peer has left the cluster (each metric has an expiration date and is expected to be renewed before it is reached). This can be used to trigger actions such as repinnings.</description>
    </item>
    
    <item>
      <title>Upgrades</title>
      <link>https://ipfscluster.io/documentation/guides/upgrades/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/guides/upgrades/</guid>
      <description>Upgrades The IPFS Cluster project releases new versions regularly. This section describes the procedure to upgrade Clusters with minimal or no downtime.
It is very important to check the changelog before upgrading, in order to get familiar with changes since the last version. All information about potential incompatibilities and breaking changes are included there.
The other main consideration is that:
Starting on v0.12.1, all the cluster peers need to run on the same RPC protocol version.</description>
    </item>
    
    <item>
      <title>Troubleshooting</title>
      <link>https://ipfscluster.io/documentation/guides/troubleshooting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/guides/troubleshooting/</guid>
      <description>Troubleshooting This sections contain a few tips to identify and correct problems when running IPFS Cluster.
IPFS Cluster fails to build Please read the Download page. It has instructions on how to build the software (please follow them).
Debug logging When discovering a problem, it is always useful to try to figure out the issue and potentially provide some relevant logs when asking for help.
ipfs-cluster-service By default, ipfs-cluster-service prints only INFO, WARNING and ERROR messages.</description>
    </item>
    
    <item>
      <title>Monitoring and tracing</title>
      <link>https://ipfscluster.io/documentation/guides/monitoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/guides/monitoring/</guid>
      <description>Monitoring and tracing IPFS Cluster can expose a Prometheus endpoint for metric-scraping and can also submit code tracing information to Jaeger.
These are configured in the observations section of the configuration and can be enabled from there, or by starting a cluster peer with:
ipfs-cluster-service daemon --stats --tracing  Apart from all go-specific metrics, cluster exports some metrics to track the current state of the cluster peer, these can be quickly inspected with curl &#39;http://127.</description>
    </item>
    
    <item>
      <title>Deployment on Kubernetes</title>
      <link>https://ipfscluster.io/documentation/guides/k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/documentation/guides/k8s/</guid>
      <description>Running Cluster on Kubernetes Helm templates Monaparty maintains Helm templates for cluster deployment at https://github.com/monaparty/helm-ipfs-cluster
Kubernetes Operator A Kubernetes [Operator](&amp;ldquo;https://kubernetes.io/docs/concepts/extend-kubernetes/operator/&amp;quot;) written by a partnership between Protocol Labs and Red Hat. The project is still in active development and should not be used for production use cases. If this is something you would like to try, head over to the documentation or github
Kustomize Beware we have not updated the following instructions in a while.</description>
    </item>
    
    <item>
      <title>State of the clusters: March 2022</title>
      <link>https://ipfscluster.io/news/state-of-the-clusters-march-2022/</link>
      <pubDate>Tue, 22 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/news/state-of-the-clusters-march-2022/</guid>
      <description>State of the clusters: March 2022 Two months have passed since our last update on the &amp;ldquo;state of the clusters&amp;rdquo;. In our previous post I mentioned we were tracking 25 million pins on a 9-peer cluster.
Today that cluster (which stores content for NFT.storage) has grown to 18 peers and 50 million pins. Our average usage rate keeps at around 4 new pins per second.
The new peers were added and were able to sync the cluster pinset in about 24 hours.</description>
    </item>
    
    <item>
      <title>State of the clusters: January 2022</title>
      <link>https://ipfscluster.io/news/state-of-the-clusters-jan-2022/</link>
      <pubDate>Mon, 17 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/news/state-of-the-clusters-jan-2022/</guid>
      <description>State of the clusters: January 2022 Today, we would like to provide a few details and figures on where we are with regards to cluster scalability, particularly as ensuring IPFS storage allocation and replication behind the NFT.storage platform.
We have started 2022 with a new release (v0.14.4). A few months ago, we were happy to report that we were tracking around 2 million pins.
Today, cluster is tracking over 25 million pins for NFT.</description>
    </item>
    
    <item>
      <title>NFT.storage - powered by IPFS Cluster v0.13.3</title>
      <link>https://ipfscluster.io/news/0.13.3_nft_storage/</link>
      <pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/news/0.13.3_nft_storage/</guid>
      <description>20210514 | NFT.storage - powered by IPFS Cluster v0.13.3 Filecoin recently announced the launch of NFT.storage, a pinning service to provide perpetual IPFS storage specifically catered to NFT creators and collectors.
The service is backed by storage provided by Pinata and Protocol Labs, with the service on the Protocol Labs side relying on IPFS Cluster for pin tracking and replication.
The service has been setup as a collaborative cluster with 3 main storage peers run by Protocol Labs.</description>
    </item>
    
    <item>
      <title>Release 0.13.1 and current state of the project</title>
      <link>https://ipfscluster.io/news/0.13.1_release/</link>
      <pubDate>Thu, 14 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/news/0.13.1_release/</guid>
      <description>20210114 | Release 0.13.1 and current state of the project We just released IPFS Cluster 0.13.1, with some bugfixes, dependency upgrades and a couple of improvements.
While development efforts have been moved to other parts of the ecosystem in the last few months, the IPFS Project continues being maintained, although without active development of large features. What users can expect is:
 Support over the common channels, responses to issues etc.</description>
    </item>
    
    <item>
      <title>Release 0.12.0</title>
      <link>https://ipfscluster.io/news/0.12.0_release/</link>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/news/0.12.0_release/</guid>
      <description>20201220 | Release 0.12.0 IPFS Cluster 0.12.0 is here! It comes with the new ipfs-cluster-follow application, a super-easy way of launching a &amp;ldquo;follower&amp;rdquo; peer.
Follower cluster peers join clusters to participate in the replication and distribution of IPFS content, but do not have permissions to modify the Cluster peerset or perform actions on other peers of the Cluster. When running ipfs-cluster-follow, peers are automatically configured with a template configuration fetched through IPFS (or any HTTP url) and run with some follower-optimized parameters.</description>
    </item>
    
    <item>
      <title>Release 0.11.0</title>
      <link>https://ipfscluster.io/news/0.11.0_release/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/news/0.11.0_release/</guid>
      <description>20191001 | Release 0.11.0 A few days ago we shipped IPFS Cluster 0.11.0. This was a huge leap forward as it finally crystalizes the journey to replace Raft with a system that allows peers to come and go freely from a cluster while keeping consistency guarantees on the shared pinset. The effort to find a suitable replacement started almost a year ago and resulted in a new crdt component that is based on go-ds-crdt, a datastore implementation using Merkle-CRDTs.</description>
    </item>
    
    <item>
      <title>Release 0.10.0</title>
      <link>https://ipfscluster.io/news/0.10.0_release/</link>
      <pubDate>Thu, 07 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/news/0.10.0_release/</guid>
      <description>20190307 | Release 0.10.0 Today we release 0.10.0, a release with major changes under the hood that will make IPFS Cluster perform significantly faster with large pinsets and less memory demanding.
For those upgrading, this release is a mandatory step before any future upgrades, as it will upgrade the internal state to a new format which prepares the floor for the upcoming addition of an alternative CRDT-based &amp;ldquo;consensus&amp;rdquo; component. The new component will increase IPFS Cluster scalability orders of magnitude and unlock collaborative Cluster where random invididuals can collaborate in replicating content.</description>
    </item>
    
    <item>
      <title>Release 0.9.0</title>
      <link>https://ipfscluster.io/news/0.9.0_release/</link>
      <pubDate>Mon, 18 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/news/0.9.0_release/</guid>
      <description>20190218 | Release 0.9.0 IPFS Cluster version 0.9.0 comes with one big new feature, OpenCensus support! This allows for the collection of distributed traces and metrics from the IPFS Cluster application as well as supporting libraries. Currently, we support the use of Jaeger as the tracing backend and Prometheus as the metrics backend. Support for other OpenCensus backends will be added as requested by the community. Please file an issue if you would like to see a particular backend supported.</description>
    </item>
    
    <item>
      <title>Release 0.8.0</title>
      <link>https://ipfscluster.io/news/0.8.0_release/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/news/0.8.0_release/</guid>
      <description>20190116 | Release 0.8.0 Since the beginning of IPFS Cluster, one of our ideas was that it should be easily dropped in place of the IPFS daemon in any integration. This was achieved by adding an IPFS Proxy endpoint which essentially provides an IPFS-compatible API for Cluster. Those endpoints and operations which does not make sense to be handled by Cluster are simply forwarded to the underlying daemon. Add, pin and unpin operations become, however, Cluster action.</description>
    </item>
    
    <item>
      <title>Release 0.7.0</title>
      <link>https://ipfscluster.io/news/0.7.0_release/</link>
      <pubDate>Wed, 31 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/news/0.7.0_release/</guid>
      <description>20181031 | Release 0.7.0 We are proud to introduce the 0.7.0 release today. It comes with a few small improvements and bugfixes.
We have slightly changed the /add endpoint response format in a non-compatible way, to return more adequate objects than the ones mimic-ing the IPFS API. It&amp;rsquo;s not the best but, better now than later.
We have also fixed the proxy /add endpoint to work correctly with the IPFS Companion extension and js-ipfs-api.</description>
    </item>
    
    <item>
      <title>Release 0.6.0</title>
      <link>https://ipfscluster.io/news/20181003_0.6.0_release/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/news/20181003_0.6.0_release/</guid>
      <description>20181003 | Release 0.6.0 | @hsanjuan We are publishing the first of IPFS Cluster 0.6.0 series today. After the large amount of code in 0.5.0, we expect to regain a bit of cadency and push out changes and improvements more often.
All peers from the 0.6.x series will be able to interact among each others. That essentially means that we will keep the internal RPC API compatible and that you will not need to upgrade all your peers at the same time.</description>
    </item>
    
    <item>
      <title>Release 0.5.0</title>
      <link>https://ipfscluster.io/news/20180824_0.5.0_release/</link>
      <pubDate>Thu, 23 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/news/20180824_0.5.0_release/</guid>
      <description>20180823 | Release 0.5.0: adding content with IPFS Cluster | @hsanjuan The new version of IPFS Cluster comes with thousands of lines of new code which implement content adding and replication to IPFS using IPFS Cluster.
That means that we know have an ipfs-cluster-ctl add command that, just like its ipfs add causing, can chunk and turn files and folders into Direct-Acyclic-Grapgs (DAGs), identified by a Content ID (CID), which is returned to the user when the adding process is completed.</description>
    </item>
    
    <item>
      <title>RPC Components in IPFS Cluster</title>
      <link>https://ipfscluster.io/news/cluster_rpc_components/</link>
      <pubDate>Wed, 22 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/news/cluster_rpc_components/</guid>
      <description>RPC Components in IPFS Cluster | @hsanjuan In this post, I would like to perform a deep dive into one of the architectural features of IPFS Cluster which has turned out to be extremely useful for building a distributed application on top of libp2p: Components.
Cluster Components are modules that implement different parts of Cluster. For example, the restapi module implements the HTTP server that provides the API to interact with a Cluster peer.</description>
    </item>
    
    <item>
      <title>PinTracker revamp</title>
      <link>https://ipfscluster.io/news/20180615_pintracker_revamp/</link>
      <pubDate>Fri, 15 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/news/20180615_pintracker_revamp/</guid>
      <description>20180615 | PinTracker Revamp | @lanzafame In this post, I am going to cover how the IPFS Cluster&amp;rsquo;s PinTracker component used to work, what some of the issues with that implementation were, how we fixed them, and where to go next.
How the PinTracker worked First, the purpose of the pintracker: the pintracker serves the role of ferrying the appropriate state from IPFS Cluster&amp;rsquo;s shared state to a peer&amp;rsquo;s ipfs daemon state.</description>
    </item>
    
    <item>
      <title>Release 0.4.0</title>
      <link>https://ipfscluster.io/news/20180604_0.4.0_release/</link>
      <pubDate>Mon, 04 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/news/20180604_0.4.0_release/</guid>
      <description>20180604 | Release 0.4.0 | @hsanjuan Last week we released IPFS Cluster v0.4.0.
We bumped the minor version number to make explicit not only that we brought on a lot of new things, but that this release also included a number of breaking changes, mostly in regards to the configuration file.
The changelog gives an overview of what&amp;rsquo;s new (a lot of things!), but we will also take time to explore and explain the changes in more detail right in this news section, in separate upcoming entries.</description>
    </item>
    
    <item>
      <title>Old captain&#39;s log</title>
      <link>https://ipfscluster.io/news/captains_log/</link>
      <pubDate>Mon, 23 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ipfscluster.io/news/captains_log/</guid>
      <description>20180329 | @hsanjuan In the last two months many things have happened in the IPFS Cluster project.
First, we have welcomed a new team member: @lanzafame has already started contributing and has resolved a few issues already included in the last release.
Secondly, we have been working very hard on implementing the &amp;ldquo;sharding RFC&amp;rdquo; that I mentioned in my last update. @zenground0 has made very significant progress on this front.</description>
    </item>
    
  </channel>
</rss>